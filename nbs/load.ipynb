{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data from the longwall\n",
    "\n",
    "> Methods for loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore import test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fastcore.all import *\n",
    "import wandb\n",
    "from datetime import datetime, timedelta\n",
    "from timecluster_extension.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from HMB longwall\n",
    "\n",
    "We will take data from one day of the shearer. the data is hosted at https://aida.ii.uam.es/2018-01-15.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: wget: not found\n"
     ]
    }
   ],
   "source": [
    "!wget -O /data/input_data.csv https://aida.ii.uam.es/2018-01-06.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File /home/jovyan/data/input_data.csv does not exist: '/home/jovyan/data/input_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-754b57576ac2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/jovyan/data/input_data.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskiprows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File /home/jovyan/data/input_data.csv does not exist: '/home/jovyan/data/input_data.csv'"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('/home/jovyan/data/input_data.csv', sep=';', skiprows=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The timestamp is given in the column `description`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['timestamp'] = pd.to_datetime(data['description'])\n",
    "data = data.drop('description', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = data.select_dtypes(exclude='object')\n",
    "df2 = data.select_dtypes(include='object').astype('bool')\n",
    "data = pd.concat([df2.reset_index(drop = True), df1], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the dimensionality reduction we might be interested only in the numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_numeric = data.select_dtypes(include=['float', 'datetime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As detailed in the TimeCluster paper, the data will be normalized into the range $[0, 1]$. Also, NaN columsn will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = data_numeric.select_dtypes(include='float')\n",
    "#data_numeric[data_numeric.select_dtypes(include='float')] = (tmp - tmp.min())/(tmp.max()-tmp.min())\n",
    "data_numeric[data_numeric.select_dtypes(include='float').columns] = (tmp - tmp.min())/(tmp.max()-tmp.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_numeric = data_numeric.dropna(axis=1, how='all').fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define a function that gathers all this operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def fpreprocess_numeric_vars(data, cname_ts=None, normalize=True, nan_replacement=0):\n",
    "    \"Preprocess a dataframe `data` containing the monitoring data from a mining longwall. \\\n",
    "    Non-numeric variables will be removed. Each column \\\n",
    "    is expected to have values of a variable in form of a time series, whose index will be described in the \\\n",
    "    column named `cname_ts`. If `cname_ts` is None (default), the index of the dataframe is assumed to contain the \\\n",
    "    timestamps. .NaN values will be \\\n",
    "    replaced by a constant value `nan_replacement`\"\n",
    "    if cname_ts is not None:\n",
    "        data.index = pd.to_datetime(data[cname_ts])\n",
    "        data = data.drop(cname_ts, axis=1)\n",
    "    df1 = data.select_dtypes(exclude='object')\n",
    "    df2 = data.select_dtypes(include='object').astype('bool')\n",
    "    data = pd.concat([df2, df1], axis = 1)\n",
    "    data_numeric = data.select_dtypes(include=['float', 'datetime'])\n",
    "    tmp = data_numeric.select_dtypes(include='float')\n",
    "    if normalize: data_numeric[data_numeric.select_dtypes(include='float').columns] = (tmp - tmp.min())/(tmp.max()-tmp.min())\n",
    "    data_numeric = data_numeric.dropna(axis=1, how='all').fillna(nan_replacement)\n",
    "    return data_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"/data/PACMEL-2019/JNK/jnk_before_handling_missing.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = None\n",
    "with open(path, 'rb') as f:\n",
    "    bin_data = f.read()\n",
    "    df = pickle.loads(bin_data)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df = fpreprocess_numeric_vars(data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.SM_ShearerLocation.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df.SM_ShearerLocation.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read multiple monitoring files, given as daily CSVs\n",
    "\n",
    "Since the mining monitoring data is given a set of CSV files, one per day, it is usefl to have a function to load multiple files in order to analyse data from multiple days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def fread_and_concat(paths, **read_args):\n",
    "    \"Read, from `paths`, a list of mining dataframes and concat them. All dataframes \\\n",
    "    must have the same columns. \"\n",
    "    return pd.concat([pd.read_csv(x, **read_args) for x in paths],\n",
    "                     ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = ['/data/PACMEL-2019/343_HMB/2018-01-14.csv', '/data/PACMEL-2019/343_HMB/2018-01-15.csv']\n",
    "df1 = pd.read_csv(paths[0], sep=';', skiprows=2, nrows=3)\n",
    "df2 = pd.read_csv(paths[1], sep=';', skiprows=2, nrows=3)\n",
    "df = fread_and_concat(paths, sep=';', skiprows=2, nrows=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.equals(df1.shape[0] + df2.shape[0], df.shape[0])\n",
    "test.all_equal([df1.shape[1], df2.shape[1], df.shape[1]], np.repeat(df1.shape[1], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def fread_mining_monitoring_files(paths, **kwargs):\n",
    "    \"Read monitoring files from the PACMEL mining use case.\"\n",
    "    df = fread_and_concat(paths,\n",
    "                          sep=';',\n",
    "                          low_memory=False,\n",
    "                          skiprows=2,\n",
    "                          **kwargs)\n",
    "    # Convert the timestamp column into a proper datetime object\n",
    "    df['description'] = pd.to_datetime(df['description'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = ['/data/PACMEL-2019/343_HMB/2018-01-14.csv', '/data/PACMEL-2019/343_HMB/2018-01-15.csv']\n",
    "df = fread_mining_monitoring_files(paths, nrows=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(df, pd.core.frame.DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series artifacts (to be used with weights and biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class is meant to extend `wandb.Artifact` for logging/using files with time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TSArtifact(wandb.Artifact):\n",
    "    default_storage_path = Path('/home/user/data/PACMEL-2019/wandb_artifacts/')\n",
    "    date_format = '%Y-%m-%d %H:%M:%S' # TODO add milliseconds\n",
    "\n",
    "    \"Class that represents a wandb artifact containing time series data. sd stands for start_date \\\n",
    "    and ed for end_date. Both should be pd.Timestamps\"\n",
    "    @delegates(wandb.Artifact.__init__)\n",
    "    def __init__(self, name, sd:pd.Timestamp=None, ed:pd.Timestamp=None, **kwargs):\n",
    "        super().__init__(type='dataset', name=name, **kwargs)\n",
    "        self.sd = sd\n",
    "        self.ed = ed\n",
    "        if self.metadata is None:\n",
    "            self.metadata = dict()\n",
    "        self.metadata['TS'] = dict(sd = self.sd.strftime(self.date_format),\n",
    "                                   ed = self.ed.strftime(self.date_format))\n",
    "\n",
    "    @classmethod\n",
    "    def from_daily_csv_files(cls, root_path, fread=pd.read_csv, start_date=None, end_date=None, metadata=None, **kwargs):\n",
    "        \"Create a wandb artifact of type `dataset`, containing the CSV files from `start_date` \\\n",
    "        to `end_date`. Dates must be pased as `datetime.datetime` objects. If a `wandb_run` is \\\n",
    "        defined, the created artifact will be logged to that run, using the longwall name as \\\n",
    "        artifact name, and the date range as version.\"\n",
    "        return None\n",
    "\n",
    "    @classmethod\n",
    "    @delegates(__init__)\n",
    "    def from_df(cls, df, name, path=None, sd=None, ed=None, normalize=False, **kwargs):\n",
    "        \"Stores the dataframe `df` as a pickle file in the pat `path` and adds its reference \\\n",
    "        to the entries of the artifact.\"\n",
    "        sd = df.index[0] if sd is None else sd\n",
    "        ed = df.index[-1] if ed is None else ed\n",
    "        obj = cls(name, sd=sd, ed=ed, **kwargs)\n",
    "        df = df.query('index >= @obj.sd') if obj.sd is not None else df\n",
    "        df = df.query('index <= @obj.ed') if obj.ed is not None else df\n",
    "\n",
    "        obj.metadata['TS']['created'] = 'from-df'\n",
    "        obj.metadata['TS']['freq'] = str(df.index.freq)\n",
    "        obj.metadata['TS']['n_vars'] = df.columns.__len__()\n",
    "        obj.metadata['TS']['n_samples'] = len(df)\n",
    "        obj.metadata['TS']['has_missing_values'] = np.any(df.isna().values).__str__()\n",
    "        obj.metadata['TS']['vars'] = list(df.columns)\n",
    "        # Normalization - Save the previous means and stds\n",
    "        if normalize:\n",
    "            obj.metadata['TS']['normalization'] = dict(\n",
    "                means = df.describe().loc['mean'].to_dict(),\n",
    "                stds = df.describe().loc['std'].to_dict()\n",
    "            )\n",
    "            df = normalize_columns(df)\n",
    "        # Hash and save\n",
    "        hash_code = str(hash(df.values.tobytes()))\n",
    "        path = obj.default_storage_path/f'{hash_code}' if path is None else Path(path)/f'{hash_code}'\n",
    "        df.to_pickle(path)\n",
    "        obj.metadata['TS']['hash'] = hash_code\n",
    "        obj.add_file(str(path))\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end, we are interested in working with time series as a dataframe. So we need a function to download the files contained in a `wandb.apis.public.Artifact` object and process them into a TS dataframe. The process of passing from files to dataframe must be different depending on what type of creation method was used to generate the original `TSArtifact`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def to_df(self:wandb.apis.public.Artifact):\n",
    "    \"Download the files of a saved wandb artifact and process them as a single dataframe. The artifact must \\\n",
    "    come from a call to `run.use_artifact` with a proper wandb run.\"\n",
    "    # The way we have to ensure that the argument comes from a TS arfitact is the metadata\n",
    "    if self.metadata.get('TS') is None:\n",
    "        print(f'ERROR:{self} does not come from a logged TSArtifact')\n",
    "        return None\n",
    "    dir = Path(self.download())\n",
    "    if self.metadata['TS']['created'] == 'from-df':\n",
    "        # Call read_pickle with the single file from dir\n",
    "        return pd.read_pickle(dir.ls()[0])\n",
    "    else:\n",
    "        print(\"ERROR: Only from_df method is allowed yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we can write a method to cast a downloaded wandb artifact (instance from `wandb.apis.public,Artifact`) to a TSArtifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def to_tsartifact(self:wandb.apis.public.Artifact):\n",
    "    \"Cast an artifact as a TS artifact. The artifact must have been created from one of the \\\n",
    "    class creation methods of the class `TSArtifact`. This is useful to go back to a TSArtifact \\\n",
    "    after downloading an artifact through the wand API\"\n",
    "    return TSArtifact(name=self.digest, #TODO change this\n",
    "                      sd=pd.to_datetime(self.metadata['TS']['sd'], format=TSArtifact.date_format),\n",
    "                      ed=pd.to_datetime(self.metadata['TS']['sd'], format=TSArtifact.date_format),\n",
    "                      description=self.description,\n",
    "                      metadata=self.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_longwall_data_artifact(root_path, start_date, end_date, longwall_name='Unnamed_longwall', wandb_run=None):\n",
    "    \"Create a wandb artifact of type `dataset`, containing the CSV files from `start_date` \\\n",
    "    to `end_date`. Dates must be pased as `datetime.datetime` objects. If a `wandb_run` is \\\n",
    "    defined, the created artifact will be logged to that run, using the longwall name as \\\n",
    "    artifact name, and the date range as version.\"\n",
    "    # Compute the number of variables for the metadata (total and numeric)\n",
    "    root_path = Path(root_path)\n",
    "    date_diff = end_date - start_date\n",
    "    sd_str = start_date.strftime(\"%Y-%m-%d\")\n",
    "    ed_str = end_date.strftime(\"%Y-%m-%d\")\n",
    "    mock_data = fread_mining_monitoring_files([f'{root_path/start_date.strftime(\"%Y-%m-%d\")}.csv'],\n",
    "                                             nrows=1)\n",
    "    artifact_name = longwall_name if longwall_name else root_path\n",
    "    artifact = wandb.Artifact(type='dataset',\n",
    "                              name=artifact_name,\n",
    "                              description='Dataset from the PACMEL mining use case. It contains \\\n",
    "                              monitoring data from a longwall shearer',\n",
    "                              metadata={\n",
    "                              'longwall': longwall_name,\n",
    "                              'start_time': datetime.strftime(start_date, format='%Y-%m-%d %H:%M:%S'),\n",
    "                              'end_time': datetime.strftime(end_date, format='%Y-%m-%d %H:%M:%S'),\n",
    "                              'n_variables': len(mock_data.columns)-1 # Exclude timestamp\n",
    "                              })\n",
    "    # ADd files as references (we do not upload files for confidential reasons)\n",
    "    [artifact.add_reference(f'file://{root_path/x.strftime(\"%Y-%m-%d\")}.csv')\n",
    "     for x in (start_date + timedelta(days=n) for n in range(date_diff.days + 1))]\n",
    "\n",
    "    if wandb_run:\n",
    "        artifact_version = f'{sd_str}_{ed_str}'\n",
    "        wandb_run.log_artifact(artifact,\n",
    "                               aliases=['latest', artifact_version])\n",
    "    return artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/vrodriguezf/timecluster-extension\" target=\"_blank\">https://app.wandb.ai/vrodriguezf/timecluster-extension</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/vrodriguezf/timecluster-extension/runs/1qaeu0v9\" target=\"_blank\">https://app.wandb.ai/vrodriguezf/timecluster-extension/runs/1qaeu0v9</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(job_type='create_dataset', resume=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_longwall_data_artifact' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2abda2236751>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"2018-01-01\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%Y-%m-%d\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0med\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msd\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhours\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m ar = create_longwall_data_artifact(root_path='/data/PACMEL-2019/343_HMB', \n\u001b[0m\u001b[1;32m      4\u001b[0m                                    \u001b[0mstart_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                    \u001b[0mend_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0med\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_longwall_data_artifact' is not defined"
     ]
    }
   ],
   "source": [
    "sd = datetime.strptime(\"2018-01-01\", \"%Y-%m-%d\")\n",
    "ed = sd + timedelta(hours=3)\n",
    "ar = create_longwall_data_artifact(root_path='/data/PACMEL-2019/343_HMB', \n",
    "                                   start_date=sd, \n",
    "                                   end_date=ed,\n",
    "                                   longwall_name='HMB', \n",
    "                                   wandb_run=None)\n",
    "ar.metadata, ar.manifest.entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to use the logged artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_recovered = run.use_artifact(name='HMB:2018-01-01_2018-01-04', type='dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = Path(ar_recovered.download())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_data = fread_mining_monitoring_files(dir.ls(), nrows=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load longwall data artifact\n",
    "\n",
    "This function is quite handy to turn the contents of a longwall artifact, created with the function `create_longwall_data_artifact`. This is specially useful in the case where the monitoring files are given in a daily basis, but you are only interested in analysing a couple of hours of data. In that case, the artifact will link the whole day file, but using the metadata, this function will only read the corresponding  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_longwall_data_artifact(a:wandb.Artifact):\n",
    "    \"Returns a dataframe with the longwall data, subsetted by the artifact metadata\"\n",
    "    a_refs = [x.ref for x in a.manifest.entries.values()]\n",
    "    data = fread_mining_monitoring_files(a_refs)\n",
    "    sd = datetime.strptime(a.metadata['start_time'], '%Y-%m-%d %H:%M:%S')\n",
    "    ed = datetime.strptime(a.metadata['end_time'], '%Y-%m-%d %H:%M:%S')\n",
    "    data = data.query('description >= @sd and description <= @ed')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Tiene que haber un error si el start date pasado es menor que el inicio del primer fichero, y lo mismo con el end date final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = ['/data/PACMEL-2019/343_HMB/2018-01-01.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fread_mining_monitoring_files(paths, nrows=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = df['description'][0]\n",
    "end_date = start_date + timedelta(minutes=15)\n",
    "start_date, end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = create_longwall_data_artifact(root_path='/data/PACMEL-2019/343_HMB/', \n",
    "                                  start_date=start_date, \n",
    "                                  end_date=start_date + timedelta(minutes=15), \n",
    "                                  wandb_run=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset = load_longwall_data_artifact(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.equals(df.columns, df_subset.columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.equals(df_subset['description'][0], start_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.equals(df_subset['description'][len(df_subset['description']) -1], end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JNK data\n",
    "\n",
    "The data from the JNK longwall comes in two formats:\n",
    "1. Queryable database\n",
    "2. Preprocessed pickle files\n",
    "3. Preprocessed CSV files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JNK pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path_JNK = Path('/data/PACMEL-2019/JNK/') # *\n",
    "[path for path in base_path_JNK.ls()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the pickle files and show the contents of each of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "jnk_files = []\n",
    "filepaths_pickle = base_path_JNK.ls(file_exts='.pickle')\n",
    "for i, path in enumerate(filepaths_pickle):\n",
    "    f = open(path, 'rb')    \n",
    "    bin_data = f.read()\n",
    "    print(f'Loading file {i}...')\n",
    "    jnk_files.append(pickle.loads(bin_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first file is a dataframe with information about the *boolean variables*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filepaths_pickle[0].name)\n",
    "df_bool_dict = jnk_files[0]\n",
    "df_bool_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second file is a dataframe with information about the *categorical variables*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filepaths_pickle[1].name)\n",
    "df_categorical_dict = jnk_files[1]\n",
    "df_categorical_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third file is a dataframe with the raw data of one month of the longwall (June 2019). It contains missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filepaths_pickle[2].name)\n",
    "df_jnk_base = jnk_files[2]\n",
    "df_jnk_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataframe contains all the 95 bool variables described in the first pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_jnk_base.columns) - len(set(df_jnk_base.columns) - set(df_bool_dict.bool_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, not all the variables listed in the dataframe for the categorical variables are present in the base data. (TODO: as why!). We can check which categorical variables are in the base data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jnk_base.filter(items=df_categorical_dict.categorical_variables).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 4th file is called `jnk_before_handling_missing`, and it is a list f 4 elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filepaths_pickle[3].name)\n",
    "jnk_before_handling_missing = jnk_files[3]\n",
    "len(jnk_before_handling_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first element is a dataframe with a subset of 16 columns from the base data (the number of rows is the same)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnk_before_handling_missing[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second and third elements of the list mark which of the variables of this dataframe are boolean, numerical, and categorical respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jnk_before_handling_missing[1])\n",
    "set(jnk_before_handling_missing[1]).issubset(df_bool_dict.bool_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jnk_before_handling_missing[2])\n",
    "set(jnk_before_handling_missing[2]).issubset(df_bool_dict.bool_variables), \\\n",
    "set(jnk_before_handling_missing[2]).issubset(df_categorical_dict.categorical_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnk_before_handling_missing[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This subset of data may represent the most interesting variables of analysis from an expert perspective (TODO: confirm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 5th and 6th pickle files look like an output instead of an input. More specifically, both of them are lists with two items:\n",
    "1. A 11 $\\times$ 11 array, containing the description of 11 clusters.\n",
    "2. An array of 1363601 elements with 11 different values (from 0 to 10), containing the assignation of each data point to each of the 11 clusters described in the first element of the file.\n",
    "\n",
    "The input dataset to achieve this result must be a preprocessed input, due to the size of the assignment array is much lower than the size of the base dataframe. Also,the number of columns of this input dataset should be 11, in case the first element of the list represents a multidimensional description of a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths_pickle[4].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnk_files[4][0][0].shape, jnk_files[4][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(jnk_files[4][1][0]), set(jnk_files[4][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filepaths_pickle[6].name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 7th file, `jnk_filled_removelong_imputeinterpolate_nothing.pickle`, is a list with two elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filepaths_pickle[6].name)\n",
    "jnk_filled_removelong_imputeinterpolate_nothing = jnk_files[6]\n",
    "len(jnk_filled_removelong_imputeinterpolate_nothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first item in the list contains a dataframe with the same number of rows that the clustering results mentioned above. Therefore, this is the preppreprocessed data used for those clustering computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jnk_filled_removelong_imputeinterpolate_nothing[0].__len__(), \n",
    "     jnk_files[4][1][0].shape)\n",
    "jnk_filled_removelong_imputeinterpolate_nothing[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns of this datafrae are classified in the second element of the list. It¡s the same set of variables than the dataset seen in the file `jnk_before_handling_missing`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnk_files[6][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(jnk_files[3][0].columns) == set(jnk_files[6][0].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check te effect of the removed periods if we compare the evolution of the timestamps between this dataset and the one without preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(jnk_files[3][0].index).plot(), pd.Series(jnk_files[6][0].index).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This must be taken into account when making use of this datasets. If one wants to consider timestamps as a countinuos source of information, even if there is no data, this last dataset, and none of them with removed periods, is an option for that analysis. For example, a forecastig task should take this very carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files 8th to 13th are the results of processing the data with the library `ts_learn`, and the resulting cluasters found in the data when using that preprocessing. Each file corresponds to the use of a different feature extracted from the package (mean, first element...) but I do not know more details about it. For know, these files can be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the last file is a list the type of variables of all the 146 variables found in the base dataframe.\n",
    "1. The first element lists the boolean variables\n",
    "2. The second element lists the numeric variables\n",
    "3. The third element lists the categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filepaths_pickle[13].name)\n",
    "types_variables = jnk_files[13]\n",
    "types_variables[0], types_variables[1], types_variables[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
