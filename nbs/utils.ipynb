{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils\n",
    "\n",
    "> Utilities used in the rest of the notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from dvats.imports import *\n",
    "from fastcore.all import *\n",
    "import wandb\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import tensorflow as tf\n",
    "import torch.nn as nn\n",
    "from fastai.basics import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import sys\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def print_flush(\n",
    "    mssg            : str,\n",
    "    print_to_path   : bool  = False,\n",
    "    print_path      : str   = \"~/data/logs/logs.txt\",\n",
    "    print_mode      : str   = 'a',\n",
    "    verbose         : int   = None,\n",
    "    print_time      : bool  = False,\n",
    "    print_both      : bool  = False,\n",
    "    **kwargs        # print args\n",
    "):\n",
    "    mssg_ = \"\"\n",
    "    if verbose is not None:\n",
    "        mssg_ += f\"[{verbose}] \"\n",
    "    if print_time: \n",
    "        now = datetime.datetime.now()\n",
    "        mssg_ += now.strftime('%Y-%m-%d %H:%M:%S') + f\".{now.microsecond // 1000:03d}\"\n",
    "        mssg_ += \" | \"\n",
    "    mssg_ += mssg\n",
    "    if print_to_path:\n",
    "        print_path = os.path.expanduser(print_path)\n",
    "        with open(print_path, print_mode) as f:\n",
    "            print(mssg_, file=f, **kwargs)\n",
    "    if (not print_to_path) or (print_to_path and print_both):\n",
    "        if print_both: mssg_ += \" | \" + print_path\n",
    "        print(mssg_, **kwargs)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate random time series dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def generate_TS_df(rows, cols):\n",
    "    \"Generates a dataframe containing a multivariate time series, where each column \\\n",
    "    represents a variable and each row a time point (sample). The timestamp is in the \\\n",
    "    index of the dataframe, and it is created with a even space of 1 second between samples\"\n",
    "    index = np.arange(pd.Timestamp.now(),\n",
    "                      pd.Timestamp.now() + pd.Timedelta(rows-1, 'seconds'),\n",
    "                      pd.Timedelta(1, 'seconds'))\n",
    "    data = np.random.randn(len(index), cols)\n",
    "    return pd.DataFrame(data, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "df = generate_TS_df(3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_eq(df.shape, (3, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  pandas Dataframe utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def normalize_columns(df:pd.DataFrame):\n",
    "    \"Normalize columns from `df` to have 0 mean and 1 standard deviation\"\n",
    "    mean = df.mean()\n",
    "    std = df.std() + 1e-7\n",
    "    return (df-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.153647</td>\n",
       "      <td>0.053706</td>\n",
       "      <td>-0.232063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.790580</td>\n",
       "      <td>1.939948</td>\n",
       "      <td>2.088105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.747919</td>\n",
       "      <td>-1.703933</td>\n",
       "      <td>-2.610741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.133795</td>\n",
       "      <td>-0.987042</td>\n",
       "      <td>-0.997468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.480328</td>\n",
       "      <td>-0.270151</td>\n",
       "      <td>0.615804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.604430</td>\n",
       "      <td>0.932525</td>\n",
       "      <td>0.957275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.728532</td>\n",
       "      <td>2.135201</td>\n",
       "      <td>1.298747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2\n",
       "count  3.000000  3.000000  3.000000\n",
       "mean   0.153647  0.053706 -0.232063\n",
       "std    0.790580  1.939948  2.088105\n",
       "min   -0.747919 -1.703933 -2.610741\n",
       "25%   -0.133795 -0.987042 -0.997468\n",
       "50%    0.480328 -0.270151  0.615804\n",
       "75%    0.604430  0.932525  0.957275\n",
       "max    0.728532  2.135201  1.298747"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "foo = generate_TS_df(3, 3)\n",
    "foo.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.850372e-17</td>\n",
       "      <td>-1.850372e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.140385</td>\n",
       "      <td>-9.060235e-01</td>\n",
       "      <td>-1.139156e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.363584</td>\n",
       "      <td>-5.364821e-01</td>\n",
       "      <td>-3.665548e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.413217</td>\n",
       "      <td>-1.669407e-01</td>\n",
       "      <td>4.060464e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.570192</td>\n",
       "      <td>4.530117e-01</td>\n",
       "      <td>5.695780e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.727168</td>\n",
       "      <td>1.072964e+00</td>\n",
       "      <td>7.331096e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0             1             2\n",
       "count  3.000000  3.000000e+00  3.000000e+00\n",
       "mean   0.000000  1.850372e-17 -1.850372e-17\n",
       "std    1.000000  9.999999e-01  1.000000e+00\n",
       "min   -1.140385 -9.060235e-01 -1.139156e+00\n",
       "25%   -0.363584 -5.364821e-01 -3.665548e-01\n",
       "50%    0.413217 -1.669407e-01  4.060464e-01\n",
       "75%    0.570192  4.530117e-01  5.695780e-01\n",
       "max    0.727168  1.072964e+00  7.331096e-01"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "bar = normalize_columns(foo)\n",
    "bar.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_close(bar.describe().loc['mean'].values, np.repeat(0.0, len(bar.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_close(bar.describe().loc['std'].values, np.repeat(1.0, len(bar.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove constant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def remove_constant_columns(df:pd.DataFrame):\n",
    "    return df.loc[:, (df != df.iloc[0]).any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>constant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-11-22 10:22:42.225099</th>\n",
       "      <td>-0.672069</td>\n",
       "      <td>-0.662154</td>\n",
       "      <td>-0.734045</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-22 10:22:43.225099</th>\n",
       "      <td>-0.702293</td>\n",
       "      <td>-0.344449</td>\n",
       "      <td>0.477274</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-22 10:22:44.225099</th>\n",
       "      <td>0.562166</td>\n",
       "      <td>0.796140</td>\n",
       "      <td>-0.337126</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   0         1         2  constant\n",
       "2024-11-22 10:22:42.225099 -0.672069 -0.662154 -0.734045       0.0\n",
       "2024-11-22 10:22:43.225099 -0.702293 -0.344449  0.477274       0.0\n",
       "2024-11-22 10:22:44.225099  0.562166  0.796140 -0.337126       0.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "foo = generate_TS_df(3, 3)\n",
    "foo['constant'] = [0.0]*len(foo)\n",
    "foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-11-22 10:22:42.225099</th>\n",
       "      <td>-0.672069</td>\n",
       "      <td>-0.662154</td>\n",
       "      <td>-0.734045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-22 10:22:43.225099</th>\n",
       "      <td>-0.702293</td>\n",
       "      <td>-0.344449</td>\n",
       "      <td>0.477274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-22 10:22:44.225099</th>\n",
       "      <td>0.562166</td>\n",
       "      <td>0.796140</td>\n",
       "      <td>-0.337126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   0         1         2\n",
       "2024-11-22 10:22:42.225099 -0.672069 -0.662154 -0.734045\n",
       "2024-11-22 10:22:43.225099 -0.702293 -0.344449  0.477274\n",
       "2024-11-22 10:22:44.225099  0.562166  0.796140 -0.337126"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "bar = remove_constant_columns(foo)\n",
    "bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "column_diff = set(foo.columns) - set(bar.columns)\n",
    "test_eq_type(column_diff, set(['constant']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create wandb artifact containing just the reference to an object pass as argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ReferenceArtifact(wandb.Artifact):\n",
    "    default_storage_path = Path('data/wandb_artifacts/') # * this path is relative to Path.home()\n",
    "    \"This class is meant to create an artifact with a single reference to an object \\\n",
    "    passed as argument in the contructor. The object will be pickled, hashed and stored \\\n",
    "    in a specified folder.\"\n",
    "    @delegates(wandb.Artifact.__init__)\n",
    "    def __init__(self, obj, name, type='object', folder=None, **kwargs):\n",
    "        super().__init__(type=type, name=name, **kwargs)\n",
    "        # pickle dumps the object and then hash it\n",
    "        hash_code = str(hash(pickle.dumps(obj)))\n",
    "        folder = Path(ifnone(folder, Path.home()/self.default_storage_path))\n",
    "        with open(f'{folder}/{hash_code}', 'wb') as f:\n",
    "            pickle.dump(obj, f)\n",
    "        self.add_reference(f'file://{folder}/{hash_code}')\n",
    "        if self.metadata is None:\n",
    "            self.metadata = dict()\n",
    "        self.metadata['ref'] = dict()\n",
    "        self.metadata['ref']['hash'] = hash_code\n",
    "        self.metadata['ref']['type'] = str(obj.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Path \"file://./-6264641716327866930\" must be a valid file or directory path",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#| hide\u001b[39;00m\n\u001b[1;32m      2\u001b[0m foo \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m bar \u001b[38;5;241m=\u001b[39m \u001b[43mReferenceArtifact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfoo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfoo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m bar_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbar\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mref\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m test_eq(bar_path\u001b[38;5;241m.\u001b[39mexists(), \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[18], line 15\u001b[0m, in \u001b[0;36mReferenceArtifact.__init__\u001b[0;34m(self, obj, name, type, folder, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhash_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     14\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(obj, f)\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_reference\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfile://\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfolder\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mhash_code\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/share/miniconda3/envs/env/lib/python3.10/site-packages/wandb/sdk/wandb_artifacts.py:479\u001b[0m, in \u001b[0;36mArtifact.add_reference\u001b[0;34m(self, uri, name, checksum, max_objects)\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m url\u001b[38;5;241m.\u001b[39mscheme:\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    476\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReferences must be URIs. To reference a local file, use file://\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    477\u001b[0m     )\n\u001b[0;32m--> 479\u001b[0m manifest_entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_storage_policy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore_reference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mURIStr\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri_str\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchecksum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchecksum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m manifest_entries:\n\u001b[1;32m    487\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manifest\u001b[38;5;241m.\u001b[39madd_entry(entry)\n",
      "File \u001b[0;32m/usr/local/share/miniconda3/envs/env/lib/python3.10/site-packages/wandb/sdk/wandb_artifacts.py:901\u001b[0m, in \u001b[0;36mWandbStoragePolicy.store_reference\u001b[0;34m(self, artifact, path, name, checksum, max_objects)\u001b[0m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstore_reference\u001b[39m(\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    895\u001b[0m     artifact: ArtifactInterface,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    899\u001b[0m     max_objects: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    900\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Sequence[ArtifactManifestEntry]:\n\u001b[0;32m--> 901\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[43martifact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchecksum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchecksum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_objects\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/share/miniconda3/envs/env/lib/python3.10/site-packages/wandb/sdk/wandb_artifacts.py:1140\u001b[0m, in \u001b[0;36mMultiHandler.store_path\u001b[0;34m(self, artifact, path, name, checksum, max_objects)\u001b[0m\n\u001b[1;32m   1138\u001b[0m handler: StorageHandler\n\u001b[1;32m   1139\u001b[0m handler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handlers[url\u001b[38;5;241m.\u001b[39mscheme]\n\u001b[0;32m-> 1140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m    \u001b[49m\u001b[43martifact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchecksum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchecksum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_objects\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/share/miniconda3/envs/env/lib/python3.10/site-packages/wandb/sdk/wandb_artifacts.py:1318\u001b[0m, in \u001b[0;36mLocalFileHandler.store_path\u001b[0;34m(self, artifact, path, name, checksum, max_objects)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     entries\u001b[38;5;241m.\u001b[39mappend(entry)\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;66;03m# TODO: update error message if we don't allow directories.\u001b[39;00m\n\u001b[0;32m-> 1318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPath \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m must be a valid file or directory path\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m path)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m entries\n",
      "\u001b[0;31mValueError\u001b[0m: Path \"file://./-6264641716327866930\" must be a valid file or directory path"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "foo = np.arange(10)\n",
    "bar = ReferenceArtifact(obj=foo, name='foo', folder='.')\n",
    "bar_path = Path(f'./{bar.metadata[\"ref\"][\"hash\"]}')\n",
    "test_eq(bar_path.exists(), True)\n",
    "test_eq(bar.metadata['ref']['type'], \"<class 'numpy.ndarray'>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a reference artifact is used by one wandb run, we should have a method to get the original object from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def to_obj(self:wandb.apis.public.Artifact):\n",
    "    \"\"\"Download the files of a saved ReferenceArtifact and get the referenced object. The artifact must \\\n",
    "    come from a call to `run.use_artifact` with a proper wandb run.\"\"\"\n",
    "    if self.metadata.get('ref') is None:\n",
    "        print_flush(f'ERROR:{self} does not come from a saved ReferenceArtifact')\n",
    "        return None\n",
    "    original_path = ReferenceArtifact.default_storage_path/self.metadata['ref']['hash']\n",
    "    path = original_path if original_path.exists() else Path(self.download()).ls()[0]\n",
    "    with open(path, 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with Reference artifact from a df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "foo = generate_TS_df(3, 3)\n",
    "bar = ReferenceArtifact(obj=foo, name='test_reference_artifact')\n",
    "bar.manifest.entries.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_eq(bar.name, 'test_reference_artifact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_eq(bar.metadata['ref']['type'], str(type(foo)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Test method `to_obj`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReferenceArtifact with a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "foo = np.random.randn(5)\n",
    "bar = ReferenceArtifact(obj=foo, name='test_reference_artifact')\n",
    "bar.manifest.entries.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_eq(bar.metadata['ref']['type'], str(type(foo)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch.nn as nn\n",
    "class PrintLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PrintLayer, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Do your print / debug stuff here\n",
    "        print_flush(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def export_and_get(self:Learner, keep_exported_file=False):\n",
    "    \"\"\"\n",
    "        Export the learner into an auxiliary file, load it and return it back.\n",
    "    \"\"\"\n",
    "    aux_path = Path('aux.pkl')\n",
    "    self.export(fname='aux.pkl')\n",
    "    aux_learn = load_learner('aux.pkl')\n",
    "    if not keep_exported_file: aux_path.unlink()\n",
    "    return aux_learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_wandb_artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_wandb_artifacts(project_path, type=None, name=None, last_version=True):\n",
    "    \"\"\"\n",
    "        Get the artifacts logged in a wandb project.\n",
    "        Input:\n",
    "        - `project_path` (str): entity/project_name\n",
    "        - `type` (str): whether to return only one type of artifacts\n",
    "        - `name` (str): Leave none to have all artifact names\n",
    "        - `last_version`: whether to return only the last version of each artifact or not\n",
    "\n",
    "        Output: List of artifacts\n",
    "    \"\"\"\n",
    "    public_api = wandb.Api()\n",
    "    if type is not None:\n",
    "        types = [public_api.artifact_type(type, project_path)]\n",
    "    else:\n",
    "        types = public_api.artifact_types(project_path)\n",
    "\n",
    "    res = L()\n",
    "    for kind in types:\n",
    "        for collection in kind.collections():\n",
    "            if name is None or name == collection.name:\n",
    "                versions = public_api.artifact_versions(\n",
    "                    kind.type,\n",
    "                    \"/\".join([kind.entity, kind.project, collection.name]),\n",
    "                    per_page=1,\n",
    "                )\n",
    "                if last_version: res += next(versions)\n",
    "                else: res += L(versions)\n",
    "    return list(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "foo = get_wandb_artifacts('wandb/artifacts-example', type='model')\n",
    "test_eq(len(foo), 2)\n",
    "foo = get_wandb_artifacts('wandb/artifacts-example', type='model', name='convnet')\n",
    "test_eq(len(foo), 1)\n",
    "foo = get_wandb_artifacts('wandb/artifacts-example', type='model', name='convnet', last_version=False)\n",
    "test_eq(len(foo), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_pickle_artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_pickle_artifact(filename):\n",
    "\n",
    "    with open(filename, \"rb\") as f:\n",
    "        df = pickle.load(f)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exec from feather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pyarrow.feather as ft\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def exec_with_feather(function, path = None, verbose = 0, *args, **kwargs):\n",
    "    result = None\n",
    "    if not (path is None):\n",
    "        if verbose > 0: print_flush(f\"--> Exec with feather | reading input from {path}\")\n",
    "        input = ft.read_feather(path)\n",
    "        if verbose > 0: print_flush(f\"--> Exec with feather | Apply function {path}\")\n",
    "        result = function(input, *args, **kwargs)\n",
    "        if verbose > 0: print_flush(f\"Exec with feather --> \", {path})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def py_function(module_name, function_name, verbose = 0):\n",
    "    try:\n",
    "        function = getattr(__import__('__main__'), function_name)\n",
    "    except:\n",
    "        module = __import__(module_name, fromlist=[''])\n",
    "        function = getattr(module, function_name)\n",
    "    print_flush(f\"py function: {function_name}: {function}\")\n",
    "    return function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def suma(a,b,c): return a+b+c\n",
    "foo = py_function(\"main\", \"suma\", True)\n",
    "print_flush(f\"foo: {foo(1,1,1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "function_name = \"prepare_forecasting_data\"\n",
    "module_name = \"tsai.data.preparation\"\n",
    "foo = py_function(module_name, function_name, True)\n",
    "foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def exec_with_feather_k_output(\n",
    "    function_name   : str, \n",
    "    module_name     : str   = \"main\", \n",
    "    path            : str   = None, \n",
    "    k_output        : int   = 0, \n",
    "    verbose         : int   = 0, \n",
    "    time_flag       : bool  = False, \n",
    "    *args, \n",
    "    **kwargs\n",
    "):\n",
    "    result = None\n",
    "    function = py_function(module_name, function_name,verbose)\n",
    "    if time_flag: t_start = time.time()\n",
    "    if not (path is None):\n",
    "        if verbose > 0: \n",
    "            print_flush(f\"--> Exec with feather | reading input from {path}\")\n",
    "        input = ft.read_feather(path)\n",
    "        if verbose > 0: print_flush(f\"--> Exec with feather | Apply function {path}\")\n",
    "        result = function(input, *args, **kwargs)[k_output]\n",
    "    if time_flag:\n",
    "        t_end = time.time()\n",
    "        print_flush(f\"Exec with feather | time: {t_end-t_start}\")\n",
    "    if verbose > 0: print_flush(f\"Exec with feather --> {path}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "enc_input = exec_with_feather_k_output(\n",
    "            function_name = \"prepare_forecasting_data\",\n",
    "            module_name   = \"tsai.data.preparation\",\n",
    "            path = \"/home/macu/data/wandb_artifacts/3967977247651105648\",\n",
    "            k_output        = 0,\n",
    "            verbose         = 1,\n",
    "            time_flag       = True,\n",
    "            fcst_history    = 450\n",
    "        )\n",
    "enc_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def exec_with_and_feather_k_output(function_name, module_name = \"main\", path_input = None, path_output = None, k_output = 0, verbose = 0, time_flag = False, *args, **kwargs):\n",
    "    result = None\n",
    "    function = py_function(module_name, function_name, verbose-1)\n",
    "    if time_flag: t_start = time.time()\n",
    "    if not (path_input is None):\n",
    "        if verbose > 0: print_flush(f\"--> Exec with feather | reading input from {path_input}\")\n",
    "        input = ft.read_feather(path_input)\n",
    "        if verbose > 0: \n",
    "            print_flush(f\"--> Exec with feather | Apply function {function_name} input type: {type(input)}\")\n",
    "        \n",
    "        result = function(input, *args, **kwargs)[k_output]\n",
    "        ft.write_feather(df, path, compression = 'lz4')\n",
    "    if time_flag:\n",
    "        t_end = time.time()\n",
    "        print_flush(f\"Exec with feather | time: {t_end-t_start}\")\n",
    "    if verbose > 0: print_flush(f\"Exec with feather --> {path_output}\")\n",
    "    return path_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "import time\n",
    "from dataclasses import dataclass, field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class Time:\n",
    "    time_start  : float =  None\n",
    "    time_end    : float =  None\n",
    "    time_total  : float =  0.0\n",
    "    function    : str   =  ''\n",
    "\n",
    "    def start(self, verbose = 0): \n",
    "        if verbose > 0: print_flush(f\"--> Start: {self.function}\")\n",
    "        self.time_start = time.time()\n",
    "        return self.time_start\n",
    "\n",
    "    def end(self, verbose= 0):\n",
    "        self.time_end = time.time()\n",
    "        self.time_total = self.duration()\n",
    "        if verbose > 0: print_flush(f\"End: {self.function} -->\")\n",
    "        return self.time_end\n",
    "        \n",
    "    def duration(self):\n",
    "        self.time_total=self.time_end - self.time_start\n",
    "        return self.time_total\n",
    "    def show(self, \n",
    "        verbose         : int = None,\n",
    "        print_to_path   : bool = False,\n",
    "        print_path      : str  = \"~/data/logs/logs.txt\",\n",
    "        print_mode      : str  = 'a'\n",
    "    ):\n",
    "        if self.time_start is None: \n",
    "            print_flush(f\"[{self.function}] Not started\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose )\n",
    "        elif self.time_end is None:\n",
    "            print_flush(f\"[{self.function}] Not ended | Start: \", self.time_start, print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose )\n",
    "        else:\n",
    "            print_flush(f\"[{self.function}] Start: {self.time_start} | End: {self.time_end} | Duration: {self.time_total} seconds\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose )\n",
    "        return self.time_total     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def funcname():\n",
    "    \"\"\"Get calling function name\"\"\"\n",
    "    return inspect.stack()[1][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Timer basic example\n",
    "foo = Time()\n",
    "foo.start()\n",
    "time.sleep(2) \n",
    "foo.end()\n",
    "foo.show()\n",
    "def foo(): return funcname()\n",
    "foo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VSCode update path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#Function for making notebooks clearer\n",
    "from IPython.display import clear_output, DisplayHandle\n",
    "def update_patch(self, obj):\n",
    "    clear_output(wait=True)\n",
    "    self.display(obj)\n",
    "    print_flush(f\"... Enabling Vs Code execution ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#from nbdev.export import notebook2script\n",
    "#notebook2script()\n",
    "beep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Styled printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def styled_print(text, color='black', size='16px', weight='normal'):\n",
    "    html_text = f\"<span style='color: {color}; font-size: {size}; font-weight: {weight};'>{text}</span>\"\n",
    "    display(HTML(html_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def show_sequence(\n",
    "    data         : List[ List [ float ] ] = None, \n",
    "    hide_rows    : bool = False, \n",
    "    hide_columns : bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Show the sequence in a nice format similar to stumpy tutorials\n",
    "    \"\"\"\n",
    "    df          = pd.DataFrame(data)\n",
    "    styled_df   = df.style\n",
    "    if hide_rows: \n",
    "        styled_df = styled_df.hide(axis='index')\n",
    "    if hide_columns: \n",
    "        styled_df = styled_df.hide(axis='columns')\n",
    "    styled_df = styled_df.set_table_styles([\n",
    "        {'selector': '',\n",
    "         'props': [('border', '2px solid black'),\n",
    "                   ('text-align', 'center'),\n",
    "                   ('font-family', 'Arial'),\n",
    "                   ('border-collapse', 'collapse')]},\n",
    "        {'selector': 'td',\n",
    "         'props': [('border', '1px solid black'),\n",
    "                   ('padding', '5px')]}\n",
    "    ])\n",
    "    display(styled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def plot_with_dots(\n",
    "    time_series             : List[float]    = None,\n",
    "    xlabel                  : str            = 'Index (time)',\n",
    "    ylabel                  : str            = 'Value',\n",
    "    title                   : str            = 'Time series',\n",
    "    sequence_flag           : bool           = True,\n",
    "    show_sequence_before    : bool           = True, \n",
    "    hide_rows               : bool           = True,\n",
    "    hide_columns            : bool           = False,\n",
    "    show_title              : bool           = True,\n",
    "    fontsize                : int            = 10,\n",
    "    save_plot               : bool           = False,\n",
    "    dots                    : bool           = True,\n",
    "    figsize                 : Tuple[int, int]= (10, 6),\n",
    "    plot_path               : str            = \"./\",\n",
    "    plot_name               : str            = \"\",\n",
    "    plot_format             : str            = \"svg\",\n",
    "    plot_resolution         : int            = 1\n",
    "  ) -> None:\n",
    "    if sequence_flag and show_sequence_before: \n",
    "        show_sequence([time_series], hide_rows, hide_columns)\n",
    "    n = len(time_series)\n",
    "    x_coords = range(n)\n",
    "    \n",
    "    plt.figure(figsize=figsize)  # Crear la figura con el tamaño especificado\n",
    "    \n",
    "    if dots: \n",
    "        plt.plot(x_coords, time_series)\n",
    "        plt.scatter(x_coords, time_series, color='red')\n",
    "    else:\n",
    "        plt.plot(x_coords, time_series, linestyle='-')\n",
    "        \n",
    "    if show_title: \n",
    "        plt.title(title, fontsize=fontsize)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    if save_plot:\n",
    "        plot_path = os.path.expanduser(plot_path)\n",
    "        if plot_name == \"\":\n",
    "            plot_name = title\n",
    "        plot_path = os.path.join(plot_path, f'{plot_name}.{plot_format}')\n",
    "        plt.savefig(plot_path, format = plot_format)\n",
    "    plt.show()\n",
    "    if sequence_flag and not show_sequence_before:\n",
    "        show_sequence([time_series], hide_rows, hide_columns)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Example following Stumpy's 13-length case\n",
    "plt.close('all')\n",
    "foo_data = np.array([0, 1, 3, 2, 9, 1, 14, 15, 1, 2, 2, 10, 7])\n",
    "foo_title = title = \"Example 1: Time series of length 13\"\n",
    "show_sequence([foo_data], hide_rows = True, hide_columns = False)\n",
    "plot_with_dots(\n",
    "    time_series             = foo_data,\n",
    "    title                   = foo_title,\n",
    "    sequence_flag           = False,\n",
    "    fontsize                = 20,\n",
    "    figsize                 = (10,3)\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Piecewise Aggregate Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "## -- Classes & types\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional, Tuple, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class Interpolator(BaseEstimator, TransformerMixin):\n",
    "    method            : str  ='linear'\n",
    "    n_segments        : int  = 1\n",
    "    plot_original_data: bool = False\n",
    "    plot_interpolated : bool = False\n",
    "    verbose           : int  = 0\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "                \n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(1, -1)\n",
    "        \n",
    "        if self.plot_original_data:\n",
    "            if self.verbose > 0: print_flush(f\"Interpolator | Plot original data\")\n",
    "            for dim in range (X.ndim-1):\n",
    "                if self.verbose > 1: print_flush(f\"Interpolator | Plot original data dimension {dim}\")\n",
    "                plot_with_dots(\n",
    "                    X[dim], \n",
    "                    sequence_flag = False, \n",
    "                    title = f'Original data | dim {dim}'\n",
    "                )\n",
    "                \n",
    "        n_samples, n_features = X.shape\n",
    "        if n_features % self.n_segments != 0 or n_features == self.n_segments:\n",
    "            raise ValueError(\n",
    "                f\"The number of segments {self.n_segments} must divide (and be different of) the number of features {n_features} | Reminder: {n_features // self.n_segments}\"\n",
    "            )\n",
    "\n",
    "        segment_size = n_features // self.n_segments\n",
    "        interpolated_result = np.full_like(X, np.nan)\n",
    "\n",
    "        if self.verbose > 0: print_flush(f\"NFeatures: {n_features} | NSegments: {self.n_segments} | segment_size: {segment_size} | interpolated result ~ {interpolated_result.shape}\")\n",
    "        \n",
    "        for i in np.arange(self.n_segments):\n",
    "            start = i * segment_size \n",
    "            end = start + segment_size\n",
    "            segment_mean = np.nanmean(X[:, start:end], axis=1)\n",
    "            for j in np.arange(n_samples):\n",
    "                nan_mask = np.isnan(X[j, start:end])\n",
    "                interpolated_result[j, start:end][nan_mask] = segment_mean[j]\n",
    "        res = np.where(np.isnan(X), interpolated_result, X)\n",
    "        if self.plot_interpolated:\n",
    "            for dim in range (X.ndim-1):\n",
    "                plot_with_dots(\n",
    "                    res[dim], \n",
    "                    sequence_flag = False, \n",
    "                    title = f'Interpolated data | dim {dim}'\n",
    "                )\n",
    "            \n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo_data = np.array([1.0, 2.0, np.nan, 4.0, 5.0, np.nan, 7.0, 8.0])\n",
    "\n",
    "foo_inter = Interpolator(\n",
    "            method='polynomial', \n",
    "            n_segments         = 4,\n",
    "            plot_interpolated  = True,\n",
    "            plot_original_data = False,\n",
    "            verbose            = 2\n",
    "        )\n",
    "\n",
    "foo = foo_inter.fit_transform(foo_data)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class PAATransformer(BaseEstimator, TransformerMixin):\n",
    "    n_segments       : int  = 1\n",
    "    plot_aggregated  : bool = True\n",
    "    verbose          : int  = 0\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        n_samples, n_features = X.shape\n",
    "        if n_features <= self.n_segments:\n",
    "            raise ValueError(f\"The number of segments ({self.n_segments}) must be lower than the number of points ({n_features})\")\n",
    "\n",
    "        segment_size = n_features // ( self.n_segments + 1)\n",
    "        remainder = n_features % ( self.n_segments + 1)\n",
    "\n",
    "        if self.verbose > 0: \n",
    "            print_flush(f\"NFeatures: {n_features} | NSegments: {self.n_segments} | Segment size: {segment_size} | Reminder: {remainder}\")\n",
    "\n",
    "        # Crear un array para los resultados\n",
    "        result = np.zeros((n_samples, self.n_segments + 1))\n",
    "\n",
    "        if self.verbose > 1: print_flush(f\"Result ~ {result.shape}\")\n",
    "\n",
    "        # Procesar cada segmento\n",
    "        for i in range(self.n_segments+1):\n",
    "            start = i * segment_size + min(i, remainder)\n",
    "            end = start + segment_size + (1 if i < remainder else 0)\n",
    "            result[:, i] = np.mean(X[:, start:end], axis=1)\n",
    "\n",
    "        if self.plot_aggregated:\n",
    "            for dim in range (X.ndim-1):\n",
    "                if self.verbose > 1:\n",
    "                    print_flush(f\"Plos res | Dim\", {dim}, verbose = verbose)\n",
    "                plot_with_dots(\n",
    "                    result[dim], \n",
    "                    sequence_flag = False, \n",
    "                    title = f'Aggregated data | dim {dim}',\n",
    "                    fontsize = 20,\n",
    "                    save_plot = True\n",
    "                )\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "foo_data = np.array([1.0, 2.0, np.nan, 4.0, 5.0, np.nan, 7.0, 8.0])\n",
    "foo_paa_pipeline = Pipeline([\n",
    "    (\n",
    "        # Step for interpolating NaNs in the original data\n",
    "        'interpolator', \n",
    "        Interpolator(\n",
    "            method='polynomial', \n",
    "            n_segments         = 4, \n",
    "            plot_interpolated  = True,\n",
    "            plot_original_data = False,\n",
    "            verbose            = 2\n",
    "        )\n",
    "    ),\n",
    "    (\n",
    "        # Step for applying Peicewise Aggregated Approximation\n",
    "        'paa', PAATransformer(\n",
    "            n_segments      = 3, \n",
    "            plot_aggregated = True, \n",
    "            verbose         = 2\n",
    "        )\n",
    "    )\n",
    "])\n",
    "\n",
    "\n",
    "foo = foo_paa_pipeline.fit_transform(foo_data)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Errors definitions\n",
    "class DownsampleError(Exception):\n",
    "    \"\"\"Exception raised for errors in the downsample process.\"\"\"\n",
    "    def __init__(self, message=\"Invalid number of min/max points for the proposed time series. You must allow cropping and check the final length\"):\n",
    "        self.message = message\n",
    "        super().__init__(self.message)\n",
    "class DivisorsError(Exception):\n",
    "    def __init__(self, message = \"Invalid parameters\"):\n",
    "        self.message = message\n",
    "        super().__init__(self.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def divisors(\n",
    "    N : int, \n",
    "    min_val:int, \n",
    "    max_val:int, \n",
    "    verbose = 0\n",
    ") -> List [ int ] : \n",
    "    if verbose > 0: print_flush(f\"--> divisors | verbose: {verbose}\", verbose = verbose)\n",
    "    if verbose > 0: \n",
    "        print_flush(f\"Looking for the divisors of {N} between {min_val} and {max_val}\")\n",
    "    if (N < 0 or min_val < 0):\n",
    "        mssg = f\"N, min_val, max_val {N}, {min_val}, {max_val} must be a positive integer (>0)\"\n",
    "        raise DivisorsError(mssg)\n",
    "    elif ( min_val > max_val):\n",
    "        mssg = f\"min_val > max_val ({min_val} > {max_val}). Please take a look\"\n",
    "        raise DivisorsError(mssg)\n",
    "    arr = np.arange(min_val,max_val+1)\n",
    "    arr = arr[ N % arr == 0]\n",
    "    if verbose > 0: print_flush(f\"Found {len(arr)} divisors of {N} between {min_val} and {max_val}\")\n",
    "    return arr\n",
    "\n",
    "def downsample_propose_crop_(\n",
    "    N            : int, \n",
    "    min_points   : int, \n",
    "    max_points   : int, \n",
    "    verbose      : int  = 0,\n",
    "    allow_crop   : bool = True,\n",
    "    nearest_val  : bool = False,\n",
    "    potential_val: int = 1\n",
    ") -> int:\n",
    "    if verbose > 0: \n",
    "        print_flush(f\"Verbose: {verbose}\")\n",
    "        print_flush(f\"Downsample Propose Crop | Prev N: {N}\")\n",
    "    all_divisors = divisors(\n",
    "        N       = N, \n",
    "        min_val = min_points, \n",
    "        max_val = max_points,\n",
    "        verbose = verbose-1\n",
    "    )\n",
    "    val = 0\n",
    "    if len(all_divisors) == 0:\n",
    "        if ( not nearest_val or potential_val < 1):\n",
    "            raise ValueError(\"No valid divisors found for the given N within the min and max points range.\")\n",
    "    else:\n",
    "        if ( nearest_val and potential_val > 0):\n",
    "                val = min(all_divisors, key=lambda x: abs(x - potential_val))\n",
    "        elif (divisors_flag):\n",
    "            val = divisors(\n",
    "                N       = N, \n",
    "                min_val = min_points, \n",
    "                max_val = max_points, \n",
    "                verbose = verbose-1\n",
    "            )[-1]\n",
    "    \n",
    "    if (allow_crop):\n",
    "        while (val < min_points and N > min_points): \n",
    "            N = N-1\n",
    "            all_divisors = divisors(\n",
    "                N       = N, \n",
    "                min_val = min_points, \n",
    "                max_val = max_points, \n",
    "                verbose = verbose-1\n",
    "            )\n",
    "            if len(all_divisors) > 0:\n",
    "                if ( nearest_val and potential_val > 0):\n",
    "                    val = min(all_divisors, key=lambda x: abs(x - potential_val))\n",
    "    else: \n",
    "        raise DownsampleError()\n",
    "        return -1\n",
    "    if verbose > 0: print_flush(f\"Downsample Propose Crop | Post N: {N} | Largest Divisor: {val}\")\n",
    "    return (val, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#print_flush(downsample_propose_crop_(7397222, 10000, 20000, 1, False))\n",
    "print_flush(downsample_propose_crop_(\n",
    "    N = 7397222, \n",
    "    min_points = 10000, \n",
    "    max_points = 20000, \n",
    "    verbose = 2, \n",
    "    allow_crop = True, \n",
    "    nearest_val = True, \n",
    "    potential_val = 14500\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def downsample(\n",
    "    data  : List [ float ] = None,\n",
    "    min_position : int  = 0,\n",
    "    max_position : int  = -1, \n",
    "    min_points   : int  = 1,\n",
    "    max_points   : int  = 10000,\n",
    "    verbose      : int  = 1,\n",
    "    show_plots   : bool = False,\n",
    "    allow_crop   : bool = True\n",
    ") -> Tuple [ List [ float ], float ]:  \n",
    "    if max_points >= data.shape[0]: return data, 1\n",
    "    if verbose > 1: print_flush(f\"[ Downsample | Position ] Before | Pos ({min_position}, {max_position})\")\n",
    "    min_position = min_position if min_position > 0 else 0\n",
    "    max_position = max_position if ( max_position > -1 and max_position < data.shape[0]) else data.shape[0]\n",
    "    if verbose > 1: print_flush(f\"[ Downsample | Position ] After | Pos ({min_position}, {max_position})\")\n",
    "    \n",
    "    n_timestamps = max_position - min_position\n",
    "    paa_factor   = np.maximum(1, n_timestamps // max_points)\n",
    "\n",
    "    min_points   = max(1,min(min_points, data.shape[0]))\n",
    "    max_points   = min(data.shape[0], min(max_points, max_position-min_position))\n",
    "\n",
    "    if verbose > 1:\n",
    "        print_flush(f\"[ Downsample | downsample_propose_crop ] Max points: {max_points}\")\n",
    "        print_flush(f\"[ Downsample | downsample_propose_crop ] Min points: {min_points}\")\n",
    "    \n",
    "    \n",
    "    min_points   = min(min_points, max_points)\n",
    "    \n",
    "    if verbose > 1:\n",
    "        print_flush(f\"[ Downsample | downsample_propose_crop ] N timestamps {n_timestamps}\")\n",
    "        print_flush(f\"[ Downsample | downsample_propose_crop ] PAA factor: {paa_factor}\")\n",
    "        \n",
    "        print_flush(f\"[ Downsample | downsample_propose_crop ] allow_crop: {allow_crop}\")\n",
    "\n",
    "    potential_segments = np.floor(n_timestamps / paa_factor).astype(int)\n",
    "    \n",
    "    N = max_position-min_position\n",
    "    \n",
    "    if verbose > 1:\n",
    "        print_flush(f\"[ Downsample | downsample_propose_crop ] N: {N}\")\n",
    "        print_flush(f\"[ Downsample | downsample_propose_crop ] potential_segments: {potential_segments}\")\n",
    "        \n",
    "    n_segments, N = downsample_propose_crop_(\n",
    "        N             = N, \n",
    "        min_points    = min_points,\n",
    "        max_points    = max_points,\n",
    "        verbose       = verbose-1,\n",
    "        allow_crop    = allow_crop,\n",
    "        nearest_val   = allow_crop, # If allow_crop, try to get as near of potential_segment as possible\n",
    "        potential_val = potential_segments # The most desired one \n",
    "    ) \n",
    "\n",
    "    if allow_crop: \n",
    "        if verbose > 1: print_flush(f\"[ Downsample | downsample_propose_crop ] Allow crop => change n_timestamp | Before {n_timestamps}\")\n",
    "        max_position = min_position + N\n",
    "        if verbose > 1: print_flush(f\"[ Downsample | downsample_propose_crop ] Allow crop => change n_timestamp | After {n_timestamps}\")\n",
    "    \n",
    "    data = data[min_position:max_position]\n",
    "    n_timestamps = data.shape[0]\n",
    "\n",
    "    if verbose > 0: \n",
    "        print_flush(f\"[ Downsample | downsample_propose_crop --> ] | N segments: {n_segments} | Data ~ {data.shape}\")\n",
    "        print_flush(f\"[ Downsample | downsample_propose_crop --> ] | N = {N} | n_timestamps = {n_timestamps} | min_position {min_position} | max_position {max_position}\")\n",
    "\n",
    "    if n_timestamps < max_points: \n",
    "        if verbose > 0: \n",
    "            print_flush(f\"[ Downsample ] n_timestamps {n_timestamps} < max_points {max_points}\")\n",
    "        return data, 1\n",
    "        \n",
    "    #| export\n",
    "    paa_pipeline = Pipeline([\n",
    "        (\n",
    "            # Step for interpolating NaNs in the original data\n",
    "            'interpolator', \n",
    "            Interpolator(\n",
    "                method             = 'polynomial', \n",
    "                n_segments         = n_segments, \n",
    "                plot_original_data = show_plots,\n",
    "                plot_interpolated  = show_plots\n",
    "            )\n",
    "        ),\n",
    "        (\n",
    "            # Step for applying Peicewise Aggregated Approximation\n",
    "            'paa', PAATransformer(\n",
    "                n_segments      = n_segments, \n",
    "                plot_aggregated = show_plots\n",
    "            )\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    ts_paa = paa_pipeline.fit_transform(data[min_position:max_position])[0]\n",
    "    if verbose > 0: \n",
    "        print_flush(f\"Downsample | ts_paa~{len(ts_paa)}\")\n",
    "        print_flush(f\"Downsample ------------------------>\")\n",
    "    return ts_paa, paa_factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "foo_data = np.array([1.0, 2.0, np.nan, 4.0, 5.0, np.nan, 7.0, 8.0])\n",
    "foo_data_2 = downsample(foo_data, min_points = 3, max_points = 5, verbose = 5, show_plots = True)\n",
    "print_flush(foo_data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Testing failed case\n",
    "foo = np.random.rand(7397222)\n",
    "downsample(foo, min_points = 10000, max_points = 20000, verbose = 5, show_plots = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic sequence length selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Following ClaSP example: best sequence length\n",
    "In a similar way to ClaSP algorithm, our algorithms take the window size, w as hyper-parameter (see https://github.com/aeon-toolkit/aeon/blob/main/examples/segmentation/segmentation_with_clasp.ipynb). A simple method for choosing the window size is the dominant frequency of the Fourier Transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aeon.segmentation._clasp import ClaSPSegmenter, find_dominant_window_sizes\n",
    "from aeon.datasets import load_electric_devices_segmentation\n",
    "from aeon.visualisation import plot_series_with_change_points, plot_series_with_profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "? aeon.segmentation._clasp.find_dominant_window_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "ts, period_size, true_cps = load_electric_devices_segmentation()\n",
    "_ = plot_series_with_change_points(ts, true_cps, title=\"Electric Devices\")\n",
    "dominant_period_size = find_dominant_window_sizes(ts)\n",
    "print_flush(f\"Dominant Period {dominant_period_size}\")\n",
    "#| hide\n",
    "clasp = ClaSPSegmenter(period_length=dominant_period_size, n_cps=5)\n",
    "found_cps = clasp.fit_predict(ts)\n",
    "profiles = clasp.profiles\n",
    "scores = clasp.scores\n",
    "\n",
    "_ = plot_series_with_profiles(\n",
    "    ts,\n",
    "    profiles,\n",
    "    true_cps=true_cps,\n",
    "    found_cps=found_cps,\n",
    "    title=\"ElectricDevices\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best nsizes sequence lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def find_dominant_window_sizes_list_single_old(\n",
    "        X            : List [ float ],\n",
    "        nsizes       : int  = 1,\n",
    "        offset       : float= 0.05, \n",
    "        min_distance : int  = 1,\n",
    "        verbose      : int  = 0\n",
    "    ) -> List [ int ]:\n",
    "\n",
    "    if verbose > 0: print_flush(f\"---> Find_dominant_window_sizes_list\")\n",
    "    if verbose > 1:\n",
    "        print_flush( f\"Find_dominant_window_sizes_list | X ~ {X.shape}\" )\n",
    "        print_flush( f\"Find_dominant_window_sizes_list | Looking for - at most - the best {nsizes} window sizes\")\n",
    "        print_flush( f\"Find_dominant_window_sizes_list | Offset {offset} max size: {offset*len(X)}\")\n",
    "    if verbose > 0: print_flush( \"Find_dominant_window_sizes_list | --> Freqs\")\n",
    "        \n",
    "    X = np.array(X)\n",
    "    \n",
    "    fourier = np.absolute(np.fft.fft(X))   \n",
    "    freqs = np.fft.fftfreq(X.shape[0], 1)\n",
    "    \n",
    "    if verbose > 1: \n",
    "        print_flush( f\"Find_dominant_window_sizes_list | Freqs {freqs} -->\")\n",
    "        print_flush( f\"Find_dominant_window_sizes_list | coefs {fourier} -->\")\n",
    "    if verbose > 0: print_flush( f\"Find_dominant_window_sizes_list | Freqs -->\")\n",
    "\n",
    "    coefs = []\n",
    "    window_sizes = []\n",
    "\n",
    "    for coef, freq in zip(fourier, freqs):\n",
    "        if coef and freq > 0:\n",
    "            coefs.append(coef)\n",
    "            window_sizes.append(1 / freq)\n",
    "\n",
    "    coefs = np.array(coefs)\n",
    "    window_sizes = np.asarray(window_sizes, dtype=np.int64)\n",
    "    \n",
    "    if verbose > 0: \n",
    "        print_flush( \"Find_dominant_window_sizes_list | Coefs and window_sizes -->\")\n",
    "        print_flush( \"Find_dominant_window_sizes_list | --> Find and return valid window_sizes\")\n",
    "\n",
    "    idx = np.argsort(coefs)[::-1]\n",
    "    \n",
    "    if verbose > 1: \n",
    "        print_flush( f\"Find_dominant_window_sizes_list | Find and return valid window_sizes | ... 0 ... {idx}\")\n",
    "        \n",
    "    sorted_window_sizes = window_sizes[idx]\n",
    "    \n",
    "    if verbose > 1: \n",
    "        print_flush( \"Find_dominant_window_sizes_list | Find and return valid window_sizes | ... 1 ...\")\n",
    "\n",
    "    # Find and return all valid window sizes\n",
    "    valid_window_sizes = [\n",
    "        int(window_size / 2) for window_size in sorted_window_sizes\n",
    "        #if 20 <= window_size < int(X.shape[0] * offset)\n",
    "        if 20 <= window_size < int(len(X) * offset)\n",
    "    ]\n",
    "    \n",
    "    if verbose > 1: \n",
    "        print_flush( \"Find_dominant_window_sizes_list | Find and return valid window_sizes | ... 2 ...\")\n",
    "\n",
    "    # If no valid window sizes are found, return the first from sorted list\n",
    "    if not valid_window_sizes:\n",
    "        if verbose > 1: print_flush( f\"Find_dominant_window_sizes_list | Find and return valid window_sizes | ... 2a ... {nsizes}\")\n",
    "        sizes = [sorted_window_sizes[0] // 2][:nsizes]\n",
    "    else:\n",
    "        if verbose > 1: print_flush( f\"Find_dominant_window_sizes_list | Find and return valid window_sizes | ... 2b ... {nsizes}\")\n",
    "        sizes = valid_window_sizes[:nsizes]\n",
    "        \n",
    "    if verbose > 0: \n",
    "        print_flush( \"Find_dominant_window_sizes_list | Find and return valid window_sizes -->\")\n",
    "    if verbose > 1:\n",
    "        print_flush(f\"Find_dominant_window_sizes_list | Sizes: {sizes}\")\n",
    "    if verbose > 0:\n",
    "        print_flush( \"Find dominant_window_sizes_list --->\" )\n",
    "    \n",
    "    return sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def select_separated_sizes(\n",
    "    xs : List [ int ],\n",
    "    min_distance : int = 1,\n",
    "    nsizes          : int = 1\n",
    ") -> List [ int ]:\n",
    "    ys = []\n",
    "    for window_size in xs:\n",
    "        if not ys or abs(window_size - ys[-1]) >= min_distance:\n",
    "            ys.append(window_size)\n",
    "        if len(ys) == nsizes:\n",
    "            break\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "xs = [0, 1, 3, 5, 4, 14, 23, 10, 6, 9, 13, 18, 16, 11, 15, 19, 8, 2, 22, 26, 7, 28, 12, 24, 17, 38, 34, 25, 30, 20, 54, 49, 27, 31, 55, 39, 128, 52, 46, 36, 53, 21, 68, 50, 89, 56, 41, 117, 66, 67, 40, 60, 112, 96, 114, 87, 65, 88, 81, 64, 75, 57, 235, 135, 43, 58, 59, 86, 63, 70, 177, 62, 125, 93, 32, 162, 172, 143, 84, 110, 95, 100, 77, 35, 150, 161, 48, 51, 82, 164, 192, 29, 101, 42, 91, 73, 78, 215, 156, 230, 108, 45, 214, 248, 165, 145, 115, 90, 136, 80, 104, 160, 208, 94, 216, 140, 263, 85, 159, 126, 242, 148, 151, 218, 79, 168, 131, 270, 223, 265, 179, 271, 232, 186, 207, 111, 71, 191, 266, 204, 132, 250, 174, 129, 37, 210, 205, 166, 121, 189, 154, 201, 139, 190, 74, 175, 141, 118, 196, 197, 228, 123, 113, 149, 147, 217, 102, 182, 134, 173, 33, 240, 259, 105, 269, 171, 130, 243, 76, 195, 72, 245, 167, 198, 47, 97, 120, 137, 241, 153, 98, 219, 116, 187, 267, 238, 220, 124, 251, 260, 92, 106, 188, 169, 253, 133, 170, 44, 142, 194, 61, 213, 99, 193, 212, 200, 203, 234, 256, 262, 236, 69, 246, 225, 264, 222, 273, 122, 158, 258, 202, 119, 247, 244, 229, 231, 183, 233, 261, 181, 227, 224, 127, 138, 144, 226, 254, 103, 176, 237, 157, 155, 146, 83, 199, 255, 107, 252, 221, 180, 209, 184, 268, 257, 178, 206, 211, 185, 239, 109, 163, 249, 272, 152]\n",
    "select_separated_sizes(xs, 5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def find_dominant_window_sizes_list_single(\n",
    "        X            : List[float],\n",
    "        nsizes       : int               = 1,\n",
    "        offset       : float             = 0.05, \n",
    "        min_distance : int               = 1,    # Asegurar distancia mínima entre tamaños\n",
    "        # Print options\n",
    "        verbose      : int               = 0,\n",
    "        print_to_path                   : bool          = False,\n",
    "        print_path                      : str           = \"~/data/logs/logs.txt\",\n",
    "        print_mode                      : str           = 'a'\n",
    "    ) -> List[int]:\n",
    "\n",
    "    if verbose > 0: print_flush( \"---> Find_dominant_window_sizes_list\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose )\n",
    "    if verbose > 1:\n",
    "        print_flush( f\"Find_dominant_window_sizes_list | X ~ {X.shape}\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose )\n",
    "        print_flush( f\"Find_dominant_window_sizes_list | Looking for - at most - the best {nsizes} window sizes\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose )\n",
    "        print_flush( f\"Find_dominant_window_sizes_list | Offset {offset} max size: {offset*len(X)}\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose )\n",
    "    if verbose > 0: print_flush( \"Find_dominant_window_sizes_list | --> Freqs\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose )\n",
    "        \n",
    "    X = np.array(X)\n",
    "    \n",
    "    fourier = np.absolute(np.fft.fft(X))   \n",
    "    freqs = np.fft.fftfreq(X.shape[0], 1)\n",
    "    \n",
    "    if verbose > 2: \n",
    "        print_flush( f\"Find_dominant_window_sizes_list | Freqs {freqs} -->\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose )\n",
    "        print_flush( f\"Find_dominant_window_sizes_list | coefs {fourier} -->\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose )\n",
    "    if verbose > 0: print_flush( f\"Find_dominant_window_sizes_list | Freqs -->\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose )\n",
    "\n",
    "    coefs = []\n",
    "    window_sizes = []\n",
    "\n",
    "    for coef, freq in zip(fourier, freqs):\n",
    "        if coef and freq > 0:\n",
    "            coefs.append(coef)\n",
    "            window_sizes.append(1 / freq)\n",
    "\n",
    "    coefs = np.array(coefs)\n",
    "    window_sizes = np.asarray(window_sizes, dtype=np.int64)\n",
    "    \n",
    "    if verbose > 0: \n",
    "        print_flush( \"Find_dominant_window_sizes_list | Coefs and window_sizes -->\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose )\n",
    "        print_flush( \"Find_dominant_window_sizes_list | --> Find and return valid window_sizes\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose )\n",
    "\n",
    "    idx = np.argsort(coefs)[::-1]\n",
    "    \n",
    "    if verbose > 1: \n",
    "        print_flush(f\"Find_dominant_window_sizes_list | Find and return valid window_sizes | ... 0 ... {idx}\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose )\n",
    "        \n",
    "    sorted_window_sizes = window_sizes[idx]\n",
    "    \n",
    "    if verbose > 1: \n",
    "        print_flush(f\"Find_dominant_window_sizes_list | Find and return valid window_sizes | ... 1 ...\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose )\n",
    "\n",
    "    # Find and return all valid window sizes\n",
    "    valid_window_sizes = [\n",
    "        int(window_size) for window_size in sorted_window_sizes\n",
    "        if window_size < int(len(X) * offset)\n",
    "    ]\n",
    "    \n",
    "    if verbose > 1: \n",
    "        print_flush( \"Find_dominant_window_sizes_list | Find and return valid window_sizes | ... 2 ...\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose )\n",
    "\n",
    "    # Ensure sizes separated at least at \"min_distance\" \n",
    "    sizes = select_separated_sizes(valid_window_sizes, min_distance, nsizes)\n",
    "\n",
    "    # If no valid window sizes are found, return the first from sorted list\n",
    "    if not sizes:\n",
    "        if verbose > 1: print_flush(f\"Find_dominant_window_sizes_list | Find and return valid window_sizes | ... 2a ... {nsizes}\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose )\n",
    "        sizes = sorted_window_sizes[0][:nsizes]\n",
    "    else:\n",
    "        if verbose > 1: print_flush(f\"Find_dominant_window_sizes_list | Find and return valid window_sizes | ... 2b ... {nsizes}\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose )\n",
    "\n",
    "    if verbose > 0: \n",
    "        print_flush(f\"Find_dominant_window_sizes_list | Find and return valid window_sizes -->\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose )\n",
    "    if verbose > 1:\n",
    "        print_flush(f\"Find_dominant_window_sizes_list | Sizes: {sizes}\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose )\n",
    "    if verbose > 0:\n",
    "        print_flush( \"Find dominant_window_sizes_list --->\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose )\n",
    "    return sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "steam_df = pd.read_csv(\"https://zenodo.org/record/4273921/files/STUMPY_Basics_steamgen.csv?download=1\")\n",
    "steam_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "foo = steam_df['steam flow']\n",
    "dominant_period_size = find_dominant_window_sizes(\n",
    "    foo, \n",
    "    offset = 0.05\n",
    ")\n",
    "print_flush(f\"Dominant Period {dominant_period_size}\")\n",
    "dominant_period_sizes = find_dominant_window_sizes_list(\n",
    "    foo, \n",
    "    nsizes = 5, \n",
    "    offset = 0.05,\n",
    "    verbose = 1\n",
    ")\n",
    "print_flush(f\"Dominant Period Sizes {dominant_period_sizes}\")\n",
    "dominant_period_sizes = find_dominant_window_sizes_list(\n",
    "    foo, \n",
    "    nsizes = 5, \n",
    "    offset = 0.6,\n",
    "    verbose = 1\n",
    ")\n",
    "print_flush(f\"Dominant Period Sizes {dominant_period_sizes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def group_similar_sizes(vars_sizes, nsizes, tolerance=2):\n",
    "    \"\"\"\n",
    "    Selects the best window sizes across multiple variables,\n",
    "    ensuring no repetitions and that the sizes are sufficiently close.\n",
    "    \"\"\"\n",
    "    indices = [0] * len(vars_sizes)  # Indices for each variable\n",
    "    selected_sizes = []  # Selected window sizes\n",
    "\n",
    "    while len(selected_sizes) < nsizes:\n",
    "        # Get the smallest available size across all variables\n",
    "        current_sizes = [vars_sizes[i][indices[i]] for i in range(len(vars_sizes)) if indices[i] < len(vars_sizes[i])]\n",
    "        min_size = min(current_sizes)\n",
    "\n",
    "        # Select sizes close to the minimum and avoid duplicates\n",
    "        for i in range(len(vars_sizes)):\n",
    "            if indices[i] < len(vars_sizes[i]) and abs(vars_sizes[i][indices[i]] - min_size) <= tolerance:\n",
    "                if vars_sizes[i][indices[i]] not in selected_sizes:  # Avoid duplicates\n",
    "                    selected_sizes.append(vars_sizes[i][indices[i]])\n",
    "                indices[i] += 1  # Move to the next size for that variable\n",
    "\n",
    "                if len(selected_sizes) >= nsizes:\n",
    "                    break\n",
    "\n",
    "        # End if no more sizes are left in any variable\n",
    "        if all(idx >= len(vars_sizes[i]) for i, idx in enumerate(indices)):\n",
    "            break\n",
    "\n",
    "    # Remove duplicates from the selected sizes and return the first nsizes\n",
    "    selected_sizes = list(dict.fromkeys(selected_sizes))  # Remove duplicates\n",
    "    return selected_sizes[:nsizes]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def find_dominant_window_sizes_list(\n",
    "        X,\n",
    "        nsizes          : int   = 1,\n",
    "        offset          : float = 0.05, \n",
    "        verbose         : int   = 0,\n",
    "        min_distance    : int   = 1,\n",
    "        #- Printing options for debugging\n",
    "        print_to_path   : bool  = False,\n",
    "        print_path      : str   = \"~/data/logs/logs.txt\",\n",
    "        print_mode      : str   = 'a'\n",
    "    ) -> List [ int ]:\n",
    "\n",
    "    if verbose > 0:\n",
    "        print_flush( f\"---> Find_dominant_window_sizes_list\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose, print_time = print_to_path)\n",
    "    \n",
    "    if len(X.shape) == 1: \n",
    "        sizes = find_dominant_window_sizes_list_single(X,nsizes,offset, min_distance, verbose, print_to_path = print_to_path, print_path = print_path, print_mode = 'a')\n",
    "    else: \n",
    "        if ( isinstance(X, pd.DataFrame ) ): X = X.values\n",
    "        if verbose > 0: print_flush( f\"Find_dominant_window_sizes_list | X ~ {X.shape}\", print_to_path = print_to_path, print_path = print_path, print_mode = 'a', verbose = verbose, print_time = print_to_path)\n",
    "        vars_sizes = []\n",
    "        for var in range( X.shape[1] ):\n",
    "            if verbose > 1: print_flush( f\"Find_dominant_window_sizes_list | Get sizes for var {var}\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose )\n",
    "            var_sizes = find_dominant_window_sizes_list_single(X[:, var], nsizes, offset, min_distance, verbose, print_to_path = print_to_path, print_path = print_path, print_mode = 'a')\n",
    "            vars_sizes.append(var_sizes)\n",
    "            if verbose > 1: \n",
    "                print_flush( f\"Find_dominant_window_sizes_list | Get sizes for var {var} | {var_sizes}\", print_to_path = print_to_path, print_path = print_path, print_mode = 'a', verbose = verbose, print_time = print_to_path)\n",
    "        if verbose > 0: print_flush( f\"Find_dominant_window_sizes_list | Grouping sizes\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose )\n",
    "        sizes = group_similar_sizes(vars_sizes, nsizes, tolerance = 2)\n",
    "        if verbose > 1:\n",
    "            print_flush(f\"find_dominant_window_sizes_list | Final selected window sizes: {sizes}\", print_to_path = print_to_path, print_path = print_path, print_mode = 'a', verbose = verbose, print_time = print_to_path)\n",
    "    if verbose > 0: print_flush( f\"Find_dominant_window_sizes_list -->\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose )\n",
    "    return sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
