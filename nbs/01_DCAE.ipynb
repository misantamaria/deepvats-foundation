{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp dcae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCAE: Deep Convolutional Autoencoder\n",
    "\n",
    "> This notebook tries to apply the ideas of the paper [TimeCluster](https://link.springer.com/article/10.1007/s00371-019-01673-y),\n",
    "with regard to the application of DCAEs for compressing multivariate time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from fastcore import test\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv1D, MaxPool1D, Reshape, UpSampling1D, InputLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from pacmel_mining_use_case.load import fpreprocess_numeric_vars, fread_mining_monitoring_files\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "from yaml import load, FullLoader\n",
    "from fastcore.utils import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the experiment tracking and hyperparameter we will use the tool **Weights & Biases**. The actual version of the library that we are using can be installed via `\n",
    "pip install https://github.com/wandb/client/archive/artifacts/next.zip`. It contains features that are not part of the public release version, such as the management of artifacts for things like dataset versioning.\n",
    "\n",
    "Before running this notebook, make sure you run in a terminal `wandb login [API_KEY]`. You can see your API_KEY in the settings of your wandb account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/vrodriguezf/timecluster-extension\" target=\"_blank\">https://app.wandb.ai/vrodriguezf/timecluster-extension</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/vrodriguezf/timecluster-extension/runs/ems5xcrs\" target=\"_blank\">https://app.wandb.ai/vrodriguezf/timecluster-extension/runs/ems5xcrs</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "run_dcae = wandb.init(project=\"timecluster-extension\", \n",
    "                 job_type = 'train_DCAE',\n",
    "                 allow_val_change=True)\n",
    "config = wandb.config  # Object for storing hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset \n",
    "\n",
    "To load the dataset we will download a specific dataset artifact from the collection of artifacts\n",
    "stored in the weights and biases (wandb) project associated to this experiment. In order to log a new dataset artifact into wandb, please use the script `tmp/wandb_create_dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_artifact = run_dcae.use_artifact(type='dataset', \n",
    "                                    name='pacmel_mining_dataset:a77b7c7e39fa5564203b74e76fecfb94')\n",
    "datadir = Path(ds_artifact.download())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    }
   ],
   "source": [
    "# parameters (uncomment to override the yaml file)\n",
    "config.update(allow_val_change=True,\n",
    "              params={\n",
    "                  'ds_artifact_type_name': ds_artifact.artifact_type_name,\n",
    "                  'ds_artifact_digest': ds_artifact.digest\n",
    "              })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The artifact directory contains the list of CSV files that comprise the dataset. We must load them and concat them into a single dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data that will be used in the rest of the notebook will be stored in the dataframe `df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:1: DtypeWarning: Columns (18,19,20,21,22,23,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,93,94,95) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df = fread_mining_monitoring_files(datadir.ls())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py:3509: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    }
   ],
   "source": [
    "df = fpreprocess_numeric_vars(df, cname_ts='description')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding window features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a continuous multivariate time-series data $D$ of dimension $d$ with $n$ time-steps, $D = X_1,X_2,\\dots,X_n$ , where each $X_i = \\{x_i^1,\\dots,x_i^d\\}$ . Let $w$ be the window width, $s$ the stride, and $t$ the start time of a sliding window in the data.\n",
    "\n",
    "Define a new matrix $Z_k$ where each row is a vector of size $w$ of data extracted from the $k^{th}$ dimension.\n",
    "\n",
    "\\begin{aligned}&Z_k(w,s,t)\\\\&\\quad =\\begin{bmatrix} x_{t}^k&\\quad x_{t+1}^k&\\quad \\dots&\\quad x_{t+w-1}^k \\\\ x_{t+s}^k&\\quad x_{t+s+1}^k&\\quad \\dots&\\quad x_{t+s+w-1}^k \\\\ \\vdots&\\quad \\vdots&\\quad \\ddots&\\quad \\vdots \\\\ x_{t+(r-1)s}^k&\\quad x_{t+(r-1)s+1}^k&\\quad \\dots&\\quad x_{t+(r-1)s+w-1}^k \\end{bmatrix} \\end{aligned}\n",
    "\n",
    "where $r$ is the number of desired rows, and $t+(r-1)s+w-1 \\le n$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Z$ is a $w \\times s \\times t$ matrix. The first step consists in slicing the original multivariate time series into slices of shape ($w \\times d$), as shown in this figure from the paper.\n",
    "<img src=\"https://i.imgur.com/R9Fx8uO.png\" style=\"width:800px;height:400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    }
   ],
   "source": [
    "# parameters (uncomment to override the yaml file)\n",
    "config.update(allow_val_change=True,\n",
    "              params={\n",
    "                  'w': 48,\n",
    "                  'stride': 1,\n",
    "                  't': 0  # Not supported yet\n",
    "              })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.equals(config.w % 12, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export load\n",
    "def fslicer(df, w, s=1, padding=False, padding_value=0):\n",
    "    \"Transform a numeric dataframe `df` into slices (sub-dataframes) of `w` \\\n",
    "    rows and the same number of columns than the original dataframe. The \\\n",
    "    distance between each slice is given by the stride `s`. If `padding` is \\\n",
    "    equals to True, the last slices which have less than `w` points are filled \\\n",
    "    with the value marked in the argument `padding_value`. Otherwise, those \\\n",
    "    slices are removed from the result.\"\n",
    "    aux = [df.iloc[x:x+w] for x in range(0, len(df), s)]\n",
    "    if padding:\n",
    "        with_padding = [x.append(pd.DataFrame(\n",
    "            np.full((w - len(x), len(df.columns)), padding_value), \n",
    "            columns=df.columns.values)) if len(x) < w else x for x in aux]\n",
    "    else:\n",
    "        with_padding = [x for x in aux if len(x) == w]\n",
    "    return with_padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172753"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_slices = fslicer(df, w=config.w, s=config.stride)\n",
    "len(df_slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Status hydrauliki strona prawa', 'Status hydrauliki strona lewa',\n",
       "       'Status ciągnika lewego', 'Status ramienia lewego', 'timestamp'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_slices[0].columns[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the number of slices and the size of each slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_nwindows = (int)((len(df) - config.w)/config.stride + 1)\n",
    "expected = [(config.w, len(df.columns))]*expected_nwindows\n",
    "actual = [x.shape for x in df_slices]\n",
    "test.all_equal(expected, actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract important features from the multivariate time series data through Deep Convolutional Autoencoders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Convolutional Auto Encoders (DCAE) is a powerful method for learning high-level and mid-level abstractions from low-level raw data. It has the ability to extract features from complex and large time-series in an unsupervised manner. This is useful to overcome the complexity of multivariate time-series.\n",
    "\n",
    "Compared to the conventional auto-encoder, DCAE has fewer parameters than the conventional auto-encoder which means less training time. Also, DCAE uses local information to reconstruct the signal while conventional auto-encoders utilize fully connected layers to globally do the reconstruction. DCAE is an unsupervised model for representation learning which maps inputs into a new representation space. It has two main parts which are the encoding part that is used to project the data into a set of feature spaces and the decoding part that reconstructs the original data. The latent space representation is the space where the data lie in the bottleneck layers.\n",
    "\n",
    "The loss function of the DCAE is defined as the error between the input and the output. DCAE aims to find a code for each input by minimizing the mean squared error (MSE) between its input (original data) and output (reconstructed data). The MSE is used which assists to minimize the loss; thus, the network is forced to learn a low-dimensional representation of the input.\n",
    "\n",
    "We will implement the DCAE of the paper [TimeCluster](https://link.springer.com/article/10.1007/s00371-019-01673-y), whose architecture is shown in the table below:\n",
    "\n",
    "![](https://i.imgur.com/3EjuAfQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, in the paper, the input shape is $60 \\times 3$, due to multivariate time series has 3 variables and the window size is 60. Generally, the size of the input/output of the autoencoder will depend on the shape of each slice obtained in the previos step. The number of latent features to be discovered is $60$ in the table above, but we can consider this as a free hyperparameter $\\delta$. Also, according to the paper: \"*The number of feature maps, size of filter and depth of the model are set based on the reconstruction error on validation set.*\". Thus, we must provide flexibility in the creation of the DCAE in terms of these hyperparameters.º"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you are not using a config file, you can also uncomment the following cell and define the hyperparameters in the fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "config.update(allow_val_change=True,\n",
    "    params = {\n",
    "        'lr': 0.0009044187712482472,\n",
    "        'n_filters': [32, 16, 12],\n",
    "        'filter_sizes': [20, 10, 10],\n",
    "        'output_filter_size': 20,\n",
    "        'pool_sizes': [2, 2, 3],\n",
    "        'delta': config.w,\n",
    "        'batch_size': 92,\n",
    "        'epochs': 36,\n",
    "        'val_pct': 0.2\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "test.all_equal([len(x) for x in [config.n_filters, config.filter_sizes, config.pool_sizes]], np.repeat(len(config.n_filters), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model\n",
    "\n",
    "The implementation of the DCAE is done using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def createDCAE(w, d, delta, n_filters=[64,32,12], filter_sizes=[10,5,5], pool_sizes=[2,2,3], output_filter_size=10):\n",
    "    \"Create a Deep Convolutional Autoencoder for multivariate time series of `d` dimensions, \\\n",
    "    sliced with a window size of `w`. The parameter `delta` sets the number of latent features that will be \\\n",
    "    contained in the Dense layer of the network. The the number of features \\\n",
    "    maps (filters), the filter size and the pool size can also be adjusted.\"\n",
    "    # Test that the parameterization of the model is correct\n",
    "    # 1. n_filters, filter_sizes and pool_sizes have the same length\n",
    "    assert test.all_equal([len(x) for x in [n_filters, filter_sizes, pool_sizes]], np.repeat(len(n_filters), 3))\n",
    "    # 2. Test that the number of filters in the last convLayer is equal to the product of the pool sizes\n",
    "    assert np.prod(pool_sizes) == n_filters[-1]\n",
    "    # 3. Test that the product of pool sizes is a divisor of the window size\n",
    "    assert w % np.prod(pool_sizes).all() == 0\n",
    "    # Create the model\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=(w,d)))\n",
    "    for (i, x) in enumerate(n_filters):    \n",
    "        model.add(Conv1D(filters=n_filters[i], kernel_size=filter_sizes[i], activation='relu', padding='same'))\n",
    "        model.add(MaxPool1D(pool_size=pool_sizes[i]))\n",
    "    aux_shape = model.output_shape[1:]\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=np.prod(aux_shape), activation='linear', name='latent_features'))\n",
    "    model.add(Reshape(target_shape=aux_shape))\n",
    "    for i, x in reversed(list(enumerate(n_filters))):\n",
    "        model.add(Conv1D(filters=n_filters[i], kernel_size=filter_sizes[i], activation='relu', padding='same'))\n",
    "        model.add(UpSampling1D(size=pool_sizes[i]))\n",
    "    model.add(Conv1D(filters=d, kernel_size=output_filter_size, activation='linear', padding='same'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 48, 32)            24352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 24, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 24, 16)            5136      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 12, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 12, 12)            1932      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 4, 12)             0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "latent_features (Dense)      (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 4, 12)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 4, 12)             1452      \n",
      "_________________________________________________________________\n",
      "up_sampling1d (UpSampling1D) (None, 12, 12)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 12, 16)            1936      \n",
      "_________________________________________________________________\n",
      "up_sampling1d_1 (UpSampling1 (None, 24, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 24, 32)            10272     \n",
      "_________________________________________________________________\n",
      "up_sampling1d_2 (UpSampling1 (None, 48, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 48, 38)            24358     \n",
      "=================================================================\n",
      "Total params: 71,790\n",
      "Trainable params: 71,790\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m = createDCAE(config.w, len(df_slices[0].columns), config.delta, n_filters=config.n_filters, \n",
    "              filter_sizes=config.filter_sizes, pool_sizes=config.pool_sizes,\n",
    "              output_filter_size=config.output_filter_size)\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sliced data must be converted to a numpy array with shape $(n \\times w \\times d)$, where $n$ is the length of the time series, $w$ is the window size and $d$ is the number of dimensions in the time series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export load\n",
    "def slices2array(slices):\n",
    "    \"`slices` is a list of dataframes, each of them containing an slice of a multivariate time series.\"\n",
    "    return np.rollaxis(np.dstack([x.values for x in slices]), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: It is important to remove the timestamp column from the sliced df, an array can only contain the numeric variables!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = slices2array([x.drop('timestamp', axis=1) for x in df_slices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.equals(input_data.shape, (len(df_slices), len(df_slices[0]), len(df_slices[0].columns)-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, it is very handy to provide a function to preprocess a dataframe as a sliced tensor suitable for model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export load\n",
    "def fmultiTSloader(df_paths, w, stride, ts_colname='description', **kwargs):\n",
    "    \"Preprocess a dataframe with multivariate time series from a set of paths `df_paths`, \\\n",
    "    preprocess it calling `fpreprocess_numeric_vars` \\\n",
    "    slice it into time windows of length `w` and stride `stride` calling `fslicer`, and \\\n",
    "    conert the result into a numpy array, suitable for ML libraries. Optional arguments for \\\n",
    "    the intermediate functions can be passed through `**kwargs`\"\n",
    "    df = fread_mining_monitoring_files(df_paths)\n",
    "    df = fpreprocess_numeric_vars(df, cname_ts=ts_colname)\n",
    "    df_slices = fslicer(df, w, stride)\n",
    "    array_slices = slices2array([x.drop('timestamp', axis=1) for x in df_slices])\n",
    "    return (df, df_slices, array_slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: DtypeWarning: Columns (18,19,20,21,22,23,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,93,94,95) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py:3509: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    }
   ],
   "source": [
    "f_df, f_df_slices, f_array_slices = fmultiTSloader(datadir.ls(), config.w, config.stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.equals(df, f_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.equals(df_slices, f_df_slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.equals(input_data, f_array_slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(learning_rate=config.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.compile(loss='mean_squared_error', optimizer=opt, metrics=['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    }
   ],
   "source": [
    "history = m.fit(x=input_data, y=input_data, batch_size=config.batch_size, \n",
    "      validation_split=config.val_pct, epochs=config.epochs, verbose=0, \n",
    "      callbacks=[WandbCallback(log_weights=True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To track the performance of this model fit, go to the project dashboard in Weights & Biases. The link is provided at the beginning of this notebook, after the execution of the function `wandb.init()'' "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
