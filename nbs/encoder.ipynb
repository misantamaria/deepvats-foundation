{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fa093821-a793-49fd-a8c2-32cacdbdc964",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6f6adb50-be0c-4f0b-9beb-38d4e78c5e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#%load_ext autoreload --> Not working TODO:REVISAR\n",
    "# %autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "67bedfb7-6a74-4f6c-a769-cc79fe37686c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from dvats.memory import *\n",
    "from dvats.utils import Time\n",
    "from dvats.utils import print_flush"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a11f72ce-0e76-4e2a-9394-1c4c632b5860",
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- Classes & types\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional, Tuple, Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6a4a02-5015-4a3e-9a64-6089de4e9532",
   "metadata": {},
   "source": [
    "# Encoder\n",
    "\n",
    "> Architectures and functions for creating encoders that create the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b014a18f-8538-4ed7-98ee-ddb165692553",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fastcore.all import *\n",
    "from tsai.callback.MVP import *\n",
    "from tsai.imports import *\n",
    "from tsai.models.InceptionTimePlus import InceptionTimePlus\n",
    "from tsai.models.explainability import get_acts_and_grads\n",
    "from tsai.models.layers import *\n",
    "from tsai.data.validation import combine_split_data\n",
    "from fastai.callback.hook import hook_outputs\n",
    "from momentfm import MOMENTPipeline\n",
    "from gluonts.dataset.pandas import PandasDataset\n",
    "import time\n",
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "33c94f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from tsai.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04637a46",
   "metadata": {},
   "source": [
    "### Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c036898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export \n",
    "class DCAE_torch(Module):\n",
    "    def __init__(self, c_in, seq_len, delta, nfs=[64, 32, 12], kss=[10, 5, 5],\n",
    "                 pool_szs=[2,2,3], output_fsz=10):\n",
    "        \"\"\"\n",
    "        Create a Deep Convolutional Autoencoder for multivariate time series of `d` dimensions,\n",
    "        sliced with a window size of `w`. The parameter `delta` sets the number of latent features that will be\n",
    "        contained in the Dense layer of the network. The the number of features\n",
    "        maps (filters), the filter size and the pool size can also be adjusted.\"\n",
    "        \"\"\"\n",
    "        assert all_equal([len(x) for x in [nfs, kss, pool_szs]], np.repeat(len(nfs), 3)), \\\n",
    "            'nfs, kss, and pool_szs must have the same length'\n",
    "        assert np.prod(pool_szs) == nfs[-1], \\\n",
    "            'The number of filters in the last conv layer must be equal to the product of pool sizes'\n",
    "        assert seq_len % np.prod(pool_szs) == 0, \\\n",
    "            'The product of pool sizes must be a divisor of the window size'\n",
    "        layers = []\n",
    "        for i in range_of(kss):\n",
    "            layers += [Conv1d(ni=nfs[i-1] if i>0 else c_in, nf=nfs[i], ks=kss[i]),\n",
    "                       nn.MaxPool1d(kernel_size=pool_szs[i])]\n",
    "        self.downsample = nn.Sequential(*layers)\n",
    "        self.bottleneck = nn.Sequential(OrderedDict([\n",
    "            ('flatten', nn.Flatten()),\n",
    "            ('latent_in', nn.Linear(seq_len, delta)),\n",
    "            ('latent_out', nn.Linear(delta, seq_len)),\n",
    "            ('reshape', Reshape(nfs[-1], seq_len // np.prod(pool_szs)))\n",
    "        ]))\n",
    "        layers = []\n",
    "        for i in reversed(range_of(kss)):\n",
    "            layers += [Conv1d(ni=nfs[i+1] if i != (len(nfs)-1) else nfs[-1],\n",
    "                              nf=nfs[i], ks=kss[i]),\n",
    "                       nn.Upsample(scale_factor=pool_szs[i])]\n",
    "        layers += [Conv1d(ni=nfs[0], nf=c_in, kernel_size=output_fsz)]\n",
    "        self.upsample = nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.downsample(x)\n",
    "        x = self.bottleneck(x)\n",
    "        x = self.upsample(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e59aa6e7-36df-4697-993e-60f737bb0f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 48])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "foo = torch.rand(3, 1, 48)\n",
    "m = DCAE_torch(c_in=foo.shape[1], seq_len=foo.shape[2], delta=12)\n",
    "m(foo).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1a4a1d-389c-481a-a54d-3f86cd2115f5",
   "metadata": {},
   "source": [
    "### Dictionary to get the default backbone modules to get the embeddings from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3eb9f1b6-ae45-4b6e-b535-c7f121208721",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "ENCODER_EMBS_MODULE_NAME = {\n",
    "    InceptionTimePlus: 'backbone', # for mvp based models\n",
    "    DCAE_torch: 'bottleneck.latent_in'#,\n",
    "    #MoiraiForecast: 'mask_encoding' #TODO: check\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05db8a5d-177c-48f5-a1b3-1d0adc4ccce6",
   "metadata": {},
   "source": [
    "### Get activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8ddd69f0-e3b8-4ebc-8fdf-0808eb9e0539",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def kwargs_to_gpu_(**kwargs):\n",
    "    for key in kwargs:\n",
    "        try: #if not able to be moved, just not move it\n",
    "            kwargs[key] = kwargs[key].to(\"cuda\")\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "def kwargs_to_cpu_(**kwargs):\n",
    "    for key in kwargs:\n",
    "        try: #if not able to be moved, just not move it\n",
    "            kwargs[key] = kwargs[key].cpu()\n",
    "        except:\n",
    "            continue\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9ac0f9ca-a5c1-4292-8e7b-d3f76872c1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_acts(\n",
    "    model : torch.nn.Module, \n",
    "    module: torch.nn.Module, \n",
    "    cpu   : bool, \n",
    "    verbose : int = 0,\n",
    "    retry: bool = False,\n",
    "    acts_indices: List [ int ] = None,\n",
    "    **model_kwargs #Parameters of the model\n",
    "):\n",
    "    if verbose > 0:\n",
    "        print_flush(f\"--> get acts | acts indices: {acts_indices}\")\n",
    "    if cpu:\n",
    "        if verbose > 0: print_flush(f\"get acts | Moving to cpu\")\n",
    "        for key in model_kwargs:\n",
    "            try: #if not able to be moved, just not move it\n",
    "                model_kwargs[key] = model_kwargs[key].cpu()\n",
    "            except:\n",
    "                continue\n",
    "        model.to(\"cpu\")\n",
    "    else:\n",
    "        if verbose > 0: print_flush(f\"get acts | Moving to gpu\")\n",
    "        for key in model_kwargs:\n",
    "            try: #if not able to be moved, just not move it\n",
    "                model_kwargs[key] = model_kwargs[key].to(\"cuda\")\n",
    "            except:\n",
    "                continue\n",
    "        model.to(\"cuda\")\n",
    "    if verbose > 0: print_flush(f\"get acts | Add hooks\")\n",
    "    h_act = hook_outputs([module], detach = True, cpu = cpu, grad = False)\n",
    "    with torch.no_grad():\n",
    "        if verbose > 0: print_flush(f\"get acts | --> Run forward\")\n",
    "        if retry:\n",
    "            if verbose > 0: print_flush(f\"get acts | Retry\")\n",
    "            try: \n",
    "                preds = model.eval()(**model_kwargs)\n",
    "            except Exception as e:\n",
    "                print_flush(f\"get acts | Retry | Error: {e}\")\n",
    "                print_flush(f\"get acts | Retry | Kwargs: {model_kwargs}\")\n",
    "                if not cpu:\n",
    "                    print_flush(f\"get acts | Retry | Moving to cpu\")\n",
    "                    for key in model_kwargs:\n",
    "                        try: #if not able to be moved, just not move it\n",
    "                            model_kwargs[key] = model_kwargs[key].cpu()\n",
    "                        except:\n",
    "                            continue\n",
    "                    model.to(\"cpu\")\n",
    "                    if verbose > 0: print_flush(f\"get acts | Retry | cpu\")\n",
    "                    print_flush(f\"get acts | Retry | Get acts\")\n",
    "                    preds = model.eval()(**model_kwargs)\n",
    "        else:\n",
    "            if verbose > 2: print_flush(f\"get acts | No Retry | Get acts | model kwargs: {model_kwargs}\")\n",
    "            preds = model.eval()(**model_kwargs)\n",
    "    if acts_indices is None:\n",
    "        res = [o.stored for o in h_act]\n",
    "    else: \n",
    "        stored = [o.stored for o in h_act]\n",
    "        res = [stored[i] for i in acts_indices]\n",
    "        if len(acts_indices) == 1:\n",
    "            res = res[0]\n",
    "        del stored\n",
    "    if verbose > 0: print_flush(f\"get acts | Run forward -->\")\n",
    "    if verbose > 0:print_flush(f\"get acts -->\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6807aee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_acts_moment(enc_learn, cpu, verbose, y, mask = None, padd_step = 100, retry = False, max_trials = 5, acts_indices = [0]):\n",
    "    success = False \n",
    "    trial = 0\n",
    "    embs = None\n",
    "    while not success and trial < max_trials:\n",
    "        trial += 1\n",
    "        try:\n",
    "            if verbose > 0: print_flush(f\"get_acts_moment | Trial {trial} | x_enc ~ {y.shape}\")\n",
    "            embs = get_acts(\n",
    "                model = enc_learn,\n",
    "                #module = enc_learn.encoder.dropout,\n",
    "                module = enc_learn.head.dropout,\n",
    "                cpu = cpu,\n",
    "                verbose = 0,\n",
    "                x_enc = y,\n",
    "                retry = retry,\n",
    "                acts_indices = acts_indices,\n",
    "                mask = mask\n",
    "            )\n",
    "            success = True\n",
    "            if verbose > 0 and acts_indices == [0] : print_flush(f\"get_acts_moment | Trial {trial} | embs ~ {embs.shape}\")\n",
    "        except Exception as e:\n",
    "            if trial == max_trials - 1 : raise\n",
    "            if verbose > 0:\n",
    "                print_flush(f\"get_acts_moment | Trial {trial} | About to pad X (encoder input) | exception {e} | padd step: {padd_step}\")\n",
    "                print_flush(f\"get_acts_moment | Trial {trial} | y ~ {y.shape}\")\n",
    "            if \"tensor a\" in str(e) and \"tensor b\" in str(e):\n",
    "                match = re.search(r'tensor a \\((\\d+)\\) must match the size of tensor b \\((\\d+)\\)', str(e))\n",
    "                tensor_a_size = int(match.group(1))\n",
    "                tensor_b_size = int(match.group(2))\n",
    "                padd = True\n",
    "                if trial > 1: \n",
    "                    if verbose > 0: print_flush(f\"------------------- Trial {trial}  -----------------\")\n",
    "                    if tensor_a_size > tensor_a_size_old:\n",
    "                        if verbose > 0:  print_flush(f\"------------------- Trial {trial} | a > a_old -----------------\")\n",
    "                        padd = False\n",
    "                        y = y [ ..., : tensor_a_size - tensor_b_size]\n",
    "                        if verbose > 0: print_flush(f\"------------------- Trial {trial} |a > a_old | Reduced |  y ~ {y.shape} -----------------\")\n",
    "                if padd:\n",
    "                    if verbose > 0: print_flush(f\"------------------- Trial {trial} | Padd -----------------\")\n",
    "                    if tensor_a_size > tensor_b_size: \n",
    "                        if verbose > 0: print_flush(f\"------------------- Trial {trial} | Padd | a > b -----------------\")\n",
    "                        padd_step = tensor_a_size - tensor_b_size\n",
    "                    y = torch.nn.functional.pad(y,(0,padd_step))\n",
    "                tensor_a_size_old = tensor_a_size\n",
    "            else:\n",
    "                if verbose > 0: print_flush(\"Not the usual error. No padding, just fail\")\n",
    "                raise\n",
    "                \n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f86af7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def sure_eval_moment(enc_learn, cpu, verbose, y, input_mask = None, mask = None, padd_step = 100, retry = False, max_trials = 5, acts_indices = [0]):\n",
    "    y.to(\"cpu\")\n",
    "    y_copy = y.clone()\n",
    "    \n",
    "    if verbose > 0: print_flush(f\"---> sure_eval_moment\")\n",
    "    success = False \n",
    "    trial = 0\n",
    "    output = None\n",
    "    if cpu:\n",
    "        enc_learn.to(\"cpu\")\n",
    "        y.to(\"cpu\")\n",
    "        if input_mask is not None: input_mask.to(\"cpu\")\n",
    "        if mask is not None: mask.to(\"cpu\")\n",
    "    else:\n",
    "        enc_learn.to(\"cuda\")\n",
    "        y.to(\"cuda\")\n",
    "        if input_mask is not None: input_mask.to(\"cuda\")\n",
    "        if mask is not None: mask.to(\"cuda\")\n",
    "    while not success and trial < max_trials:\n",
    "        trial += 1\n",
    "        try:\n",
    "            if verbose > 0: print_flush(f\"sure_eval_moment | Trial {trial} | x_enc ~ {y.shape}\")\n",
    "            \n",
    "            output = enc_learn(x_enc = y, input_mask = input_mask, mask = mask)\n",
    "            success = True\n",
    "            if verbose > 0 and acts_indices == [0] : print_flush(f\"sure_eval_moment | Trial {trial} | embs ~ {embs.shape}\")\n",
    "        except Exception as e:\n",
    "            if verbose > 0:\n",
    "                print_flush(f\"sure_eval_moment | Trial {trial} | About to pad X (encoder input) | exception {e} | padd step: {padd_step}\")\n",
    "                print_flush(f\"sure_eval_moment | Trial {trial} | y ~ {y.shape}\")\n",
    "            if \"tensor a\" in str(e) and \"tensor b\" in str(e) and \"dimension\" in str(e):\n",
    "                match = re.search(r'tensor a \\((\\d+)\\) must match the size of tensor b \\((\\d+)\\) at non-singleton dimension (\\d+)', str(e))\n",
    "                tensor_a_size = int(match.group(1))\n",
    "                tensor_b_size = int(match.group(2))\n",
    "                dimension = int(match.group(3))\n",
    "                match dimension:\n",
    "                    case 2 | 1:\n",
    "                        padd = True\n",
    "                        if trial > 1: \n",
    "                            if verbose > 0: print_flush(f\"------------------- Trial {trial}  -----------------\")\n",
    "                            if tensor_a_size > tensor_a_size_old:\n",
    "                                if verbose > 0: print_flush(f\"------------------- Trial {trial} | a > a_old -----------------\")\n",
    "                                padd = False\n",
    "                                y = y [ ..., : tensor_a_size - tensor_b_size]\n",
    "                                if verbose > 0: print_flush(f\"------------------- Trial {trial} |a > a_old | Reduced |  y ~ {y.shape} -----------------\")\n",
    "                        if padd:\n",
    "                            if verbose > 0: print_flush(f\"------------------- Trial {trial} | Padd -----------------\")\n",
    "                            if tensor_a_size > tensor_b_size: \n",
    "                                if verbose > 0: print_flush(f\"------------------- Trial {trial} | Padd | a > b -----------------\")\n",
    "                                padd_step = tensor_a_size - tensor_b_size\n",
    "                            y = torch.nn.functional.pad(y,(0,padd_step))\n",
    "                        tensor_a_size_old = tensor_a_size\n",
    "                    #case 1: \n",
    "                    #    if verbose > 0:\n",
    "                    #        print_flush(f\"sure_eval_moment | Trial {trial} | Error dimension 0 | mask ~ {mask.shape} | mask_input ~ {input_mask.shape} | batch ~ {y.shape}\")\n",
    "                    #        if mask.shape[1] < y.shape[2]: mask = torch.nn.functional.pad(mask,(0,y.shape[2]-mask.shape[1]))\n",
    "                    #        if input_mask.shape[2] < y.shape[2]: mask = torch.nn.functional.pad(input_mask,(0,y.shape[2]-input_mask.shape[2]))\n",
    "\n",
    "                    case 0:\n",
    "                        if verbose > 0: \n",
    "                            print_flush(f\"sure_eval_moment | Trial {trial} | Error dimension 0 | mask ~ {mask.shape} | mask_input ~ {input_mask.shape} | batch ~ {y.shape}\")                    \n",
    "                        if mask.shape[0] > y.shape[0]:\n",
    "                            mask = mask[:y.shape[0]]\n",
    "                        if input_mask.shape[0] > y.shape[0]:\n",
    "                            input_mask = input_mask[:y.shape[0]]\n",
    "                        \n",
    "                        if mask.shape[0] < y.shape[0]:\n",
    "                            extra_rows_shape = (-mask.shape[0]+y.shape[0],mask.shape[1])\n",
    "                            if verbose > 0: print_flush(f\"sure_eval_moment | Trial {trial} | Mask lower than batch | rows to add: {extra_rows_shape }\")                    \n",
    "                            extra_rows = torch.zeros(extra_rows_shape, dtype = torch.float32)\n",
    "                            mask = torch.cat((mask, extra_rows), dim=0)\n",
    "                        if input_mask.shape[0] < y.shape[0]:\n",
    "                            extra_rows_shape = (-input_mask.shape[0]+y.shape[0],y.shape[1], y.shape[2])\n",
    "                            if verbose > 0: print_flush(f\"sure_eval_moment | Trial {trial} | Mask lower than batch | rows to add: {extra_rows_shape }\")                    \n",
    "                            extra_rows = torch.zeros(extra_rows_shape, dtype = torch.float32)\n",
    "                            input_mask = torch.cat((input_mask, extra_rows), dim=0)\n",
    "            else:\n",
    "                if verbose > 0: \n",
    "                    print_flush(\"Not the usual error. No padding, just fail\")\n",
    "                raise\n",
    "        if verbose > 0: print_flush(f\"sure_eval_moment -->\")\n",
    "    y = y_copy\n",
    "    if not cpu: y.to(\"cuda\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268a268f-8b4e-4432-b203-79263a247c4c",
   "metadata": {},
   "source": [
    "### Getting the embeddings (activations) from the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cb5216e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastai.learner import Learner\n",
    "from tsai.data.core import TSDataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5a992422",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_enc_embs_ensure_batch_size_(\n",
    "    dls        : TSDataLoaders,\n",
    "    batch_size : int = None,\n",
    "    verbose    : int = 0\n",
    ") -> None:\n",
    "    if batch_size is None:\n",
    "        if verbose > 1: \n",
    "            print_flush(\"[ Get Encoder Embeddings Ensure Batch Size ] No batch size proposed\")\n",
    "        if dls.bs == 0: \n",
    "            if verbose > 1: \n",
    "                print_flush(\"[ Get Encoder Embeddings Ensure Batch Size ] Using value 64 as 0 is not a valid value.\")\n",
    "            enc_learn.dls.bs = 64\n",
    "        elif verbose > 1: \n",
    "            print_flush(f\"[ Get Encoder Embeddings Ensure Batch Size ] Using the original value: {dls.bs}\")\n",
    "    else:\n",
    "        dls.bs = batch_size\n",
    "        if verbose > 1: \n",
    "            print_flush(f\"[ Get Encoder Embeddings Ensure Batch Size ] Batch size proposed. Using {dls.bs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "65c66ae6-3178-49dc-bd16-64c082012e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_enc_embs_MVP(\n",
    "    X               : List [ List [ List [ float ] ] ], \n",
    "    enc_learn       : Learner, \n",
    "    module          : str  = None, \n",
    "    cpu             : bool = False, \n",
    "    average_seq_dim : bool = True, \n",
    "    to_numpy        : bool = True,\n",
    "    batch_size      : int  = None,\n",
    "    verbose         : int  = 0\n",
    "):\n",
    "    \"\"\"\n",
    "        Get the embeddings of X from an encoder, passed in `enc_learn as a fastai\n",
    "        learner. By default, the embeddings are obtained from the last layer\n",
    "        before the model head, although any layer can be passed to `model`.\n",
    "        Input\n",
    "        - `cpu`: Whether to do the model inference in cpu of gpu (GPU recommended)\n",
    "        - `average_seq_dim`: Whether to aggregate the embeddings in the sequence dimensions\n",
    "        - `to_numpy`: Whether to return the result as a numpy array (if false returns a tensor)\n",
    "        - `batch_size`: force data loader to use the input batch size\n",
    "        - `verbose`: print flag. More big, more information.\n",
    "    \"\"\"\n",
    "    \n",
    "    if cpu:\n",
    "        if verbose > 0: print_flush(\"[ Get Encoder Embeddings ] CPU\")\n",
    "        enc_learn.dls.cpu()\n",
    "        enc_learn.cpu()\n",
    "    else:\n",
    "        if verbose > 0: print_flush(\"[ Get Encoder Embeddings ] --> GPU\")\n",
    "        if verbose > 1: print_flush(\"[ Get Encoder Embeddings ] GPU | Ensure empty cache\")\n",
    "        torch.cuda.empty_cache()\n",
    "        if verbose > 1: print_flush(\"[ Get Encoder Embeddings ] GPU | Move & exec into CUDA\")\n",
    "        enc_learn.dls.cuda()\n",
    "        enc_learn.cuda()\n",
    "        if torch.cuda.is_available():\n",
    "            if verbose > 1: \n",
    "                print_flush(\"[ Get Encoder Embeddings ] GPU | CUDA is available\")\n",
    "                print_flush(f\"[ Get Encoder Embeddings ] GPU | CUDA is available | current device id {torch.cuda.current_device()}\")\n",
    "                print_flush(f\"[ Get Encoder Embeddings ] GPU | CUDA is available | current device name {torch.cuda.get_device_name(torch.cuda.current_device())}\")            \n",
    "        else:\n",
    "            if verbose > 1: print_flush(\"[ Get Encoder Embeddings ] GPU | CUDA is not available\")\n",
    "        if verbose > 0: print_flush(\"[ Get Encoder Embeddings ] GPU -->\")\n",
    "\n",
    "    #if verbose > 0: print_flush(\"[ Get Encoder Embeddings ] Ensure the correct batch size\")\n",
    "    #get_enc_embs_ensure_batch_size_(enc_learn.dls, batch_size, verbose)\n",
    "    \n",
    "    if verbose > 0: print_flush(\"[ Get Encoder Embeddings ] Set dataloader from X (enc_learn does not contain dls)\")\n",
    "    aux_dl = enc_learn.dls.valid.new_dl(X=X)\n",
    "    get_enc_embs_ensure_batch_size_(aux_dl, batch_size, verbose)\n",
    "    if verbose > 0: print_flush(\"[ Get Encoder Embeddings ] Get module\")\n",
    "    module = nested_attr(enc_learn.model,ENCODER_EMBS_MODULE_NAME[type(enc_learn.model)]) if module is None else module\n",
    "    \n",
    "    if verbose > 0: print_flush(\"[ Get Encoder Embeddings ] get_acts_and_grads \")\n",
    "    if verbose > 1: print_flush(f\"[ Get Encoder Embeddings ] get_acts_and_grads bs = {aux_dl.bs}\")\n",
    "    \n",
    "    embs = [\n",
    "        get_acts_and_grads(\n",
    "            model   = enc_learn.model,\n",
    "            modules = module,\n",
    "            x       = xb[0], \n",
    "            cpu     = cpu\n",
    "        )[0] \n",
    "        for xb in aux_dl\n",
    "    ]\n",
    "    if verbose > 0: print_flush(\"[ Get Encoder Embeddings ] get_acts_and_grads | --> Concat\")\n",
    "    if not cpu:\n",
    "        if verbose > 1: print_flush(\"[ Get Encoder Embeddings ] get_acts_and_grads | Concat | Check neccesary & free memory\")\n",
    "        total_emb_size = sum([emb.element_size() * emb.nelement() for emb in embs])\n",
    "        free_memory = torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()\n",
    "        if (total_emb_size < free_memory):\n",
    "            if verbose > 1: print_flush(\"[ Get Encoder Embeddings ] get_acts_and_grads | Concat | Check neccesary & free memory | Fits in GPU -> Computing in GPU\")\n",
    "            embs=[emb.cuda() for emb in embs]\n",
    "        else:\n",
    "            if verbose > 1: print_flush(\"[ Get Encoder Embeddings ] get_acts_and_grads | Concat | Check neccesary & free memory | Does not fit in GPU -> Computing in CPU\")\n",
    "            embs=[emb.cpu() for emb in embs]\n",
    "    if verbose > 1: print_flush(\"[ Get Encoder Embeddings ] get_acts_and_grads | Concat | to_concat\")\n",
    "    embs = to_concat(embs)\n",
    "    if verbose > 0: print_flush(\"[ Get Encoder Embeddings ] get_acts_and_grads | Concat -->\")\n",
    "    \n",
    "    if verbose > 0: print_flush(\"[ Get Encoder Embeddings ] Reduce to 2 dimensions.\")\n",
    "    if embs.ndim == 3 and average_seq_dim: embs = embs.mean(axis=2)\n",
    "    if verbose > 0: print_flush(\"[ Get Encoder Embeddings ] Ensure CPU saving & numpy format\")\n",
    "    if to_numpy: embs = embs.numpy() if cpu else embs.cpu().numpy()\n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "51ad3b28-43c0-4df3-be57-3eecb76b17d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_enc_embs_MVP_set_stride_set_batch_size(\n",
    "    X                  : List [ List [ List [ float ] ] ], \n",
    "    enc_learn          : Learner, \n",
    "    stride             : int, \n",
    "    batch_size         : int, \n",
    "    module             : str  = None, \n",
    "    cpu                : bool = False, \n",
    "    average_seq_dim    : bool = True, \n",
    "    to_numpy           : bool = True, \n",
    "    verbose            : int  = 0, \n",
    "    time_flag          : bool = False, \n",
    "    chunk_size         : int  = 0, \n",
    "    check_memory_usage : bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "        Get the embeddings of X from an encoder, passed in `enc_learn as a fastai\n",
    "        learner. By default, the embeddings are obtained from the last layer\n",
    "        before the model head, although any layer can be passed to `model`.\n",
    "        Input\n",
    "        - `X`: encoder input\n",
    "        - `enc_learn`: trained encoder\n",
    "        - `stride`: stride used for the training. Neccesary for adjusting the encoder input\n",
    "        - `batch_size`: value to force the dataloader to use.\n",
    "        - `module`: for geting the embeddings of an specific layer.\n",
    "        - `cpu`: Whether to do the model inference in cpu of gpu (GPU recommended)\n",
    "        - `average_seq_dim`: Whether to aggregate the embeddings in the sequence dimensions\n",
    "        - `to_numpy`: Whether to return the result as a numpy array (if false returns a tensor)\n",
    "        - `verbose`: For printing messages. More big, more messages.\n",
    "        - `time_flag`: To take note of the execution time required by this function\n",
    "        - `chunk_size`: For spliting the embedings reading in batches of `chunk_size` size.\n",
    "        - `check_memory_usage`: For showing messages of the current state of the memory.\n",
    "    \"\"\"\n",
    "    if time_flag:\n",
    "        t_start = time.time()\n",
    "    if verbose > 0:\n",
    "        print_flush(\"--> get_enc_embs_set_stride_set_batch_size\")\n",
    "    if check_memory_usage: gpu_memory_status()\n",
    "    X = X[::stride]\n",
    "    enc_learn.dls.bs = batch_size \n",
    "\n",
    "    get_enc_embs_ensure_batch_size_(enc_learn.dls, batch_size, verbose)\n",
    "    \n",
    "    if verbose > 0: print_flush(\"get_enc_embs_set_stride_set_batch_size | Check CUDA | X ~ \", X.shape[0])\n",
    "    if cpu:\n",
    "        if verbose > 0: print_flush(\"get_enc_embs_set_stride_set_batch_size | Get enc embs CPU\")\n",
    "        enc_learn.dls.cpu()\n",
    "        enc_learn.cpu()\n",
    "    else:\n",
    "        if torch.cuda.is_available():\n",
    "            if verbose > 0: \n",
    "                print_flush(\"get_enc_embs_set_stride_set_batch_size | CUDA device id:\", torch.cuda.current_device())\n",
    "                print_flush(\"get_enc_embs_set_stride_set_batch_size | CUDA device name: \", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "                print_flush(\"get_enc_embs_set_stride_set_batch_size | Ensure empty cache & move 2 GPU\")\n",
    "            torch.cuda.empty_cache()\n",
    "            enc_learn.dls.cuda()\n",
    "            enc_learn.cuda()\n",
    "        else:\n",
    "            if verbose > 0: print_flush(\"get_enc_embs_set_stride_set_batch_size | No cuda available. Set CPU = true\")\n",
    "            cpu = True\n",
    "            \n",
    "    get_enc_embs_ensure_batch_size_(enc_learn.dls, batch_size, verbose)\n",
    "\n",
    "    if verbose > 0: print_flush(\"get_enc_embs_set_stride_set_batch_size | Set dataset from X (enc_learn does not contain dls)\")\n",
    "    aux_dl = enc_learn.dls.valid.new_dl(X=X)\n",
    "    aux_dl.bs = enc_learn.dls.bs if enc_learn.dls.bs>0 else 64\n",
    "    if verbose > 0: print_flush(\"get_enc_embs_set_stride_set_batch_size | Get module\")\n",
    "    module = nested_attr(enc_learn.model,ENCODER_EMBS_MODULE_NAME[type(enc_learn.model)]) if module is None else module\n",
    "    \n",
    "    if verbose > 0: \n",
    "        #print_flush(\"get_enc_embs_set_stride_set_batch_size | Get acts and grads | module \", module)\n",
    "        print_flush(\"get_enc_embs_set_stride_set_batch_size | Get acts and grads | aux_dl len\", len(aux_dl))\n",
    "        print_flush(\"get_enc_embs_set_stride_set_batch_size | Get acts and grads | aux_dl.batch_len \", len(next(iter(aux_dl))))\n",
    "        print_flush(\"get_enc_embs_set_stride_set_batch_size | Get acts and grads | aux_dl.bs \", aux_dl.bs)\n",
    "        if (not cpu):\n",
    "            total = torch.cuda.get_device_properties(device).total_memory\n",
    "            used = torch.cuda.memory_allocated(torch.cuda.current_device())\n",
    "            reserved = torch.cuda.memory_reserved(torch.cuda.current_device())\n",
    "            print_flush(\"get_enc_embs_set_stride_set_batch_size | Get acts and grads | total_mem \", total)\n",
    "            print_flush(\"get_enc_embs_set_stride_set_batch_size | Get acts and grads | used_mem \", used)\n",
    "            print_flush(\"get_enc_embs_set_stride_set_batch_size | Get acts and grads | reserved_mem \", reserved)\n",
    "            print_flush(\"get_enc_embs_set_stride_set_batch_size | Get acts and grads | available_mem \", total-reserved)\n",
    "            sys.stdout.flush()\n",
    "                                              \n",
    "    if (cpu or ( chunk_size == 0 )):\n",
    "        embs = [\n",
    "            get_acts_and_grads(\n",
    "                model=enc_learn.model,\n",
    "                modules=module, \n",
    "                x=xb[0], \n",
    "                cpu=cpu\n",
    "            )[0] \n",
    "            for xb in aux_dl\n",
    "        ]\n",
    "        if not cpu: embs=[emb.cpu() for emb in embs]\n",
    "    else:\n",
    "        embs = []\n",
    "        total_chunks=max(1,round(len(X)/chunk_size))\n",
    "        if verbose > 0: print_flush(\"get_enc_embs_set_stride_set_batch_size | Get acts and grads | aux_dl len | \" + str(len(X)) + \" chunk size: \" + str(chunk_size) + \" => \" + str(total_chunks) + \" chunks\")\n",
    "        for i in range(0, total_chunks):\n",
    "            if verbose > 0: \n",
    "                print_flush(\"get_enc_embs_set_stride_set_batch_size | Get acts and grads | Chunk [ \" + str(i) + \"/\"+str(total_chunks)+\"] => \" + str(round(i*100/total_chunks)) + \"%\")\n",
    "                sys.stdout.flush()\n",
    "            chunk = [batch for (n, batch) in enumerate(aux_dl) if (chunk_size*i <= n  and chunk_size*(i+1) > n) ]\n",
    "            chunk_embs = [\n",
    "                get_acts_and_grads(\n",
    "                    model=enc_learn.model,\n",
    "                    modules=module,\n",
    "                    x=xb[0], \n",
    "                    cpu=cpu\n",
    "                )[0]\n",
    "                for xb in chunk\n",
    "            ]\n",
    "            # Mueve los embeddings del bloque a la CPU\n",
    "            chunk_embs = [emb.cpu() for emb in chunk_embs]\n",
    "            embs.extend(chunk_embs)\n",
    "            torch.cuda.empty_cache()\n",
    "        if verbose > 0: \n",
    "            print_flush(\"get_enc_embs_set_stride_set_batch_size | Get acts and grads | 100%\")\n",
    "            sys.stdout.flush()\n",
    "    \n",
    "    if verbose > 0: print_flush(\"get_enc_embs_set_stride_set_batch_size | concat embeddings\")\n",
    "    \n",
    "    embs = to_concat(embs)\n",
    "    \n",
    "    if verbose > 0: print_flush(\"get_enc_embs_set_stride_set_batch_size | Reduce\")\n",
    "    \n",
    "    if embs.ndim == 3 and average_seq_dim: embs = embs.mean(axis=2)\n",
    "    \n",
    "    if verbose > 0: print_flush(\"get_enc_embs_set_stride_set_batch_size | Convert to numpy\")\n",
    "    \n",
    "    if to_numpy: \n",
    "        if cpu or chunk_size > 0:\n",
    "            embs = embs.numpy() \n",
    "        else: \n",
    "            embs = embs.cpu().numpy()\n",
    "            torch.cuda.empty_cache()\n",
    "    if time_flag:\n",
    "        t = time.time()-t_start\n",
    "        if verbose > 0:\n",
    "            print_flush(\"get_enc_embs_set_stride_set_batch_size \" + str(t) + \" seconds -->\")\n",
    "        else:\n",
    "            print_flush(\"get_enc_embs_set_stride_set_batch_size \" + str(t) + \" seconds\")\n",
    "    if check_memory_usage: gpu_memory_status()\n",
    "    if verbose > 0: \n",
    "        print_flush(\"get_enc_embs_set_stride_set_batch_size -->\")\n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5a17863e-f5c1-4c4c-8023-37d197598ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_enc_embs_moment(\n",
    "    X               : List [ List [ List [ float ] ] ], \n",
    "    enc_learn       : Learner, \n",
    "    cpu             : bool = False, \n",
    "    to_numpy        : bool = True,\n",
    "    verbose         : int  = 0,\n",
    "    average_seq_dim : bool = True\n",
    "):\n",
    "    if verbose > 0: \n",
    "        print_flush(\"--> get_enc_embs_moment\")\n",
    "    # Move tensor and model to GPU\n",
    "    if cpu or not torch.cuda.is_available():\n",
    "        if verbose > 0: \n",
    "            print_flush(\"get_enc_embs_moment | Using CPU (maybe no cuda available)\")\n",
    "        cpu = True\n",
    "        enc_learn.cpu()\n",
    "    else:\n",
    "        if verbose > 0: \n",
    "            print_flush(\"get_enc_embs_moment | Using CUDA\")\n",
    "        enc_learn.to(\"cuda\")\n",
    "    if verbose > 0: print_flush(\"get_enc_embs_moment | Convert y\")\n",
    "    enc_learn.eval()\n",
    "    if cpu:\n",
    "        y = torch.from_numpy(X).cpu().float()\n",
    "    else:\n",
    "        y = torch.from_numpy(X).to(\"cuda\").float()\n",
    "    # Get output\n",
    "    with torch.no_grad():\n",
    "        if verbose > 0: \n",
    "            print_flush(\"get_enc_embs_moment | Get outputs\")\n",
    "        outputs = enc_learn(y)\n",
    "        if verbose > 0:\n",
    "            print_flush(f\"get_enc_embs_moment | Final shape: X ~ {y.shape}\")\n",
    "                \n",
    "    #| move tensors and models back to CPU\n",
    "    if not cpu:\n",
    "        y = y.detach().cpu().numpy()\n",
    "    if verbose > 0: \n",
    "        print_flush(\"get_enc_embs_moment | Get Embeddings\")\n",
    "    embeddings = outputs.embeddings.detach().cpu()\n",
    "    if average_seq_dim: \n",
    "        embeddings = embeddings.mean(dim = 1)\n",
    "    if to_numpy:\n",
    "        embeddings = embeddings.cpu().numpy()\n",
    "    if verbose > 0: \n",
    "        print_flush(\"get_enc_embs_moment -->\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "aac3e43c-fd7f-4022-8d1a-be1b47e580de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_enc_embs_moment_reconstruction(\n",
    "    X               : List [ List [ List [ float ] ] ], \n",
    "    enc_learn       : Learner, \n",
    "    cpu             : bool = False, \n",
    "    to_numpy        : bool = True,\n",
    "    verbose         : int  = 0,\n",
    "    average_seq_dim : bool = True,\n",
    "    padd_step       : int  = 2\n",
    "):\n",
    "    \"\"\"\n",
    "    For reconstruction sometimes mask get invalid values\n",
    "    To avoid them, the last dimension (sequence length) is padded with 0's until the error is skippedd\n",
    "    It should only get one iteration as it seems to be some MOMENT internal configuration for patches.\n",
    "    \"\"\"\n",
    "    if cpu:\n",
    "        enc_learn.cpu()\n",
    "        y = torch.from_numpy(X).cpu().float()\n",
    "    else:\n",
    "        enc_learn.to(\"cuda\")\n",
    "        y = torch.from_numpy(X).to(\"cuda\").float()\n",
    "    embs = get_acts_moment(\n",
    "        enc_learn = enc_learn, \n",
    "        cpu = cpu, \n",
    "        verbose = verbose, \n",
    "        y = y, \n",
    "        mask = None,\n",
    "        padd_step = padd_step,\n",
    "        retry = False ,\n",
    "        max_trials = 5\n",
    "    )\n",
    "    if average_seq_dim: \n",
    "        embs = embs.mean(dim = 1).mean(dim = 1)\n",
    "    if to_numpy:\n",
    "        embs = embs.cpu().numpy()\n",
    "    return embs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51f46bd",
   "metadata": {},
   "source": [
    "---> TODO: averiguar de qué module salen realmente los embeddings y usar el get_acts_and_grads como en MVP <---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1e1d1254-1421-4017-8dc1-4ef8adfeeccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import uni2ts.model.moirai.module as moirai\n",
    "import uni2ts.model.moirai.forecast as moirai_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e2ccce71-3238-44b0-90f5-7efaf62c1643",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch.profiler as profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "73adfca7-6b31-4030-ab06-d24e5bd07821",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def watch_gpu(func, **kwargs):\n",
    "    \"\"\"\n",
    "    Wrapper to execute GPU profiler\n",
    "    Parameters: \n",
    "    - func: function to monitor\n",
    "    - kwargs: func parameters\n",
    "    Returns:\n",
    "    - result of /func/.\n",
    "    \"\"\"\n",
    "    with profiler.profile(\n",
    "        activities=[profiler.ProfilerActivity.CPU, profiler.ProfilerActivity.CUDA],\n",
    "        schedule=profiler.schedule(wait=1, warmup=1, active=3, repeat=2),  # Configuración de ciclos\n",
    "        on_trace_ready=profiler.tensorboard_trace_handler('./log_dir'),  # Guarda los resultados en un archivo para visualización\n",
    "        record_shapes=True,  # Registra la forma de los tensores\n",
    "        profile_memory=True,  # Perfil de memoria\n",
    "        with_stack=True  # Incluye la información de la pila\n",
    "    ) as prof:\n",
    "        # Ejecuta la función dentro del perfilador\n",
    "        result = func(**kwargs)\n",
    "    \n",
    "    # Mostrar el uso de la GPU durante y después de la ejecución\n",
    "    print_flush(prof.key_averages().table(sort_by=\"cuda_memory_usage\", row_limit=10))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d9a70cd5-5aa5-44c2-b7cf-4d290e9349ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_enc_embs_moirai(\n",
    "    enc_input       : List [ List [ List [ Float ] ] ], \n",
    "    enc_model       : moirai.MoiraiModule, \n",
    "    cpu             : False,\n",
    "    average_seq_dim : bool = True, \n",
    "    verbose         : int  = 0,\n",
    "    to_numpy        : bool = True,\n",
    "    patch_size      : int  = 8,\n",
    "    time            : bool = False\n",
    "):\n",
    "    if time: \n",
    "        timer = Time()\n",
    "        timer.start()\n",
    "    if verbose > 0: \n",
    "        print_flush(\"--> get_enc_embs_moirai\")\n",
    "    # Move tensor and model to GPU\n",
    "    past_target = einops.rearrange(\n",
    "        torch.as_tensor(enc_input, dtype = torch.float32),\n",
    "        \"n_windows n_vars window_size -> n_windows window_size n_vars\"\n",
    "    )\n",
    "    if cpu or not torch.cuda.is_available():\n",
    "        if verbose > 0: print_flush(\"get_enc_embs_moirai | Using CPU (maybe no cuda available)\")\n",
    "        cpu = True\n",
    "        enc_model.cpu()\n",
    "        past_target.cpu()\n",
    "    else:\n",
    "        if verbose > 0: print_flush(\"get_enc_embs_moirai | Using CUDA\")\n",
    "        enc_model.to(\"cuda\")\n",
    "        past_target.to(\"cuda\")\n",
    "        \n",
    "    if verbose > 0: print_flush(\"get_enc_embs_moirai | Get Outputs\")\n",
    "\n",
    "    \n",
    "    past_observed_target = torch.ones_like(past_target, dtype=torch.bool)\n",
    "    past_is_pad = torch.zeros_like(past_target, dtype=torch.bool)[...,:,-1] # Kill last dimension\n",
    "\n",
    "    if (verbose > 1):\n",
    "        print_flush(f\"--> get_enc_embs_moirai | past_target ~ {past_target.shape}\")\n",
    "        print_flush(f\"--> get_enc_embs_moirai | past_observed_target ~ {past_observed_target.shape}\")\n",
    "        print_flush(f\"--> get_enc_embs_moirai | past_is_pad ~ {past_is_pad.shape}\")\n",
    "        print_flush(f\"--> get_enc_embs_moirai | Auxiliar model\")\n",
    "        print_flush(f\"--> get_enc_embs_moirai | Auxiliar model | Before Memory:\")\n",
    "        gpu_memory_status()\n",
    "    \n",
    "    # Auxiliar model for conversions just to ensure correct sizes\n",
    "    #not neccesary, is the same module initially downloaded...\n",
    "    #module = moirai.MoiraiModule.from_pretrained(f\"Salesforce/moirai-1.1-R-small\")\n",
    "    \n",
    "    forecast_model =  moirai_forecast.MoiraiForecast(\n",
    "        module=enc_model,\n",
    "        prediction_length=past_target.shape[2], #random, just for getting the model\n",
    "        context_length=past_target.shape[1],\n",
    "        patch_size=patch_size,\n",
    "        num_samples=100, #Random, is the number of forecasting, not interesting for us\n",
    "        target_dim=past_target.shape[2],\n",
    "        feat_dynamic_real_dim=0,\n",
    "        past_feat_dynamic_real_dim=0,\n",
    "    )\n",
    "    \n",
    "    if verbose > 0:\n",
    "        print_flush(f\"--> get_enc_embs_moirai | Auxiliar model | After Memory:\")\n",
    "        gpu_memory_status()\n",
    "        print_flush(f\"--> get_enc_embs_moirai | Convert sizes\")\n",
    "    (\n",
    "    target,\n",
    "    observed_mask,\n",
    "    sample_id,\n",
    "    time_id,\n",
    "    variate_id,\n",
    "    prediction_mask,\n",
    "    ) = forecast_model._convert(\n",
    "        patch_size,\n",
    "        past_target,\n",
    "        past_observed_target,\n",
    "        past_is_pad\n",
    "    )\n",
    "    if verbose > 1:\n",
    "        print_flush(f\"get_enc_embs_moirai | target ~ {target.shape}\")\n",
    "        print_flush(f\"get_enc_embs_moirai | observed_mask ~ {observed_mask.shape}\")\n",
    "        print_flush(f\"get_enc_embs_moirai | sample_id ~ {sample_id.shape}\")\n",
    "        print_flush(f\"get_enc_embs_moirai | time_id ~ {time_id.shape}\")\n",
    "        print_flush(f\"get_enc_embs_moirai | variate_id ~ {variate_id.shape}\")\n",
    "        print_flush(f\"get_enc_embs_moirai | prediction_mask ~ {prediction_mask.shape}\")\n",
    "        gpu_memory_status()\n",
    "    forecast_model = None\n",
    "    torch.cuda.empty_cache()\n",
    "    if verbose > 0:\n",
    "        print_flush(f\"--> get_enc_embs_moirai | Delete Auxiliar model | After Memory:\")\n",
    "        gpu_memory_status()\n",
    "    \n",
    "    model_kwargs={\n",
    "        'target': target, \n",
    "        'observed_mask': observed_mask,\n",
    "        'sample_id': sample_id,\n",
    "        'time_id': time_id,\n",
    "        'variate_id': variate_id,\n",
    "        'prediction_mask': prediction_mask,\n",
    "        'patch_size': torch.ones_like(sample_id, dtype = torch.float32)*patch_size\n",
    "    } \n",
    "    if verbose > 0: \n",
    "        print_flush(f\"get_enc_embs_moirai | About to get activations\")\n",
    "    acts = get_acts(\n",
    "        model  = enc_model, \n",
    "        module = enc_model.encoder.norm, \n",
    "        cpu    = cpu,\n",
    "        verbose = verbose,\n",
    "        retry = True,\n",
    "        acts_indices = [0],\n",
    "        **model_kwargs #Parameters of the model\n",
    "    )\n",
    "    \n",
    "    embs = acts\n",
    "    acts = None\n",
    "    if average_seq_dim :\n",
    "        if verbose > 0: \n",
    "            print_flush(f\"get_enc_embs_moirai | About to reduce activations\")\n",
    "        embs = embs.mean(dim = 1)\n",
    "    \n",
    "    if not cpu:\n",
    "        #print_flush(f\"get_enc_embs_moirai | enc_input to cpu\")\n",
    "        #enc_input.cpu()\n",
    "        print_flush(f\"get_enc_embs_moirai | enc_model to cpu\")\n",
    "        enc_model.cpu()\n",
    "        print_flush(f\"get_enc_embs_moirai | torch cuda empty cache\")\n",
    "        torch.cuda.empty_cache()\n",
    "    if to_numpy: \n",
    "        if cpu > 0:\n",
    "            embs = embs.numpy() \n",
    "        else: \n",
    "            embs = embs.cpu().numpy()\n",
    "            torch.cuda.empty_cache()\n",
    "    if verbose > 0: \n",
    "        print_flush(f\"get_enc_embs_moirai | embs ~ {embs.shape}\")\n",
    "        print_flush(\"get_enc_embs_moirai -->\")\n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc5a4d2-8961-4dd6-829d-0572468b185c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "86667613-82af-438f-b66c-c4c5e894634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def get_enc_embs(\n",
    "    X               , \n",
    "    enc_learn       : Learner, \n",
    "    module          : str  = None, \n",
    "    cpu             : bool = False, \n",
    "    average_seq_dim : bool = True, \n",
    "    to_numpy        : bool = True,\n",
    "    verbose         : int  = 0,\n",
    "    **kwargs        \n",
    "):\n",
    "    embs = None\n",
    "    enc_learn_class = str(enc_learn.__class__)[8:-2]\n",
    "    match enc_learn_class:\n",
    "        case \"momentfm.models.moment.MOMENTPipeline\":\n",
    "            match enc_learn.task_name:\n",
    "                case \"embedding\":\n",
    "                    embs = get_enc_embs_moment(X, enc_learn, cpu, to_numpy, verbose, average_seq_dim, **kwargs)\n",
    "                case \"reconstruction\":\n",
    "                    embs = get_enc_embs_moment_reconstruction(X, enc_learn, cpu, to_numpy, verbose, average_seq_dim, **kwargs)\n",
    "                case _:\n",
    "                    print_flush(f\"Model embeddings for moment-{enc_learn.task_name} is not yet implemented.\")\n",
    "        case \"fastai.learner.Learner\":\n",
    "            embs = get_enc_embs_MVP_set_stride_set_batch_size(X, enc_learn, stride, batch_size, module, cpu, average_seq_dim, to_numpy, verbose, False, 0, False)\n",
    "        case \"uni2ts.model.moirai.module.MoiraiModule\":\n",
    "            embs = get_enc_embs_moirai(\n",
    "                enc_input  = X, \n",
    "                enc_model  = enc_learn,\n",
    "                cpu        = cpu, \n",
    "                average_seq_dim = average_seq_dim,\n",
    "                verbose    = verbose,\n",
    "                **kwargs\n",
    "            )\n",
    "        case _:\n",
    "            print_flush(f\"Model embeddings implementation is not yet implemented for {enc_learn_class}.\")\n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "13ccce1d-aca2-44c1-8b20-4339628e3919",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_enc_embs_set_stride_set_batch_size(\n",
    "    X                  : List [ List [ List [ float ] ] ], \n",
    "    enc_learn          : Learner, \n",
    "    stride             : int, \n",
    "    batch_size         : int, \n",
    "    module             : str  = None, \n",
    "    cpu                : bool = False, \n",
    "    average_seq_dim    : bool = True, \n",
    "    to_numpy           : bool = True, \n",
    "    verbose            : int  = 0, \n",
    "    time_flag          : bool = False, \n",
    "    chunk_size         : int  = 0, \n",
    "    check_memory_usage : bool = False,\n",
    "    **kwargs\n",
    "):\n",
    "    print_flush(\"--> get_enc_embs_set_stride_set_batch_size\")\n",
    "    embs = None\n",
    "    enc_learn_class = str(enc_learn.__class__)[8:-2]\n",
    "    match enc_learn_class:\n",
    "        case \"momentfm.models.moment.MOMENTPipeline\":\n",
    "            if verbose > 0: \n",
    "                print_flush(f\"get_enc_embs_set_stride_set_batch_size | Moment | {average_seq_dim}\")\n",
    "            match enc_learn.task_name:\n",
    "                case \"embedding\":\n",
    "                    embs = get_enc_embs_moment( X = X, enc_learn = enc_learn, cpu = cpu, to_numpy = to_numpy, verbose = verbose, average_seq_dim = average_seq_dim)\n",
    "                case \"reconstruction\":\n",
    "                    embs = get_enc_embs_moment_reconstruction(X= X, enc_learn = enc_learn, cpu = cpu, to_numpy = to_numpy, verbose = verbose, average_seq_dim = average_seq_dim, **kwargs)\n",
    "                case _:\n",
    "                    print_flush(f\"Model embeddings for moment-{enc_learn.task_name} is not yet implemented.\")\n",
    "        case \"fastai.learner.Learner\":\n",
    "            if verbose > 0: \n",
    "                print_flush(f\"get_enc_embs_set_stride_set_batch_size | MVP | {average_seq_dim}\")\n",
    "            embs = get_enc_embs_MVP_set_stride_set_batch_size(\n",
    "                X = X, \n",
    "                enc_learn = enc_learn, \n",
    "                stride = stride, \n",
    "                batch_size = batch_size, \n",
    "                module = module, \n",
    "                cpu = cpu, \n",
    "                average_seq_dim = average_seq_dim,\n",
    "                to_numpy = to_numpy, \n",
    "                verbose = verbose, \n",
    "                time_flag = time_flag, \n",
    "                chunk_size = chunk_size, \n",
    "                check_memory_usage = check_memory_usage\n",
    "            )\n",
    "        case \"uni2ts.model.moirai.module.MoiraiModule\":\n",
    "            if verbose > 0: \n",
    "                print_flush(f\"get_enc_embs_set_stride_set_batch_size | Moirai | {average_seq_dim}\")\n",
    "            embs = get_enc_embs_moirai(\n",
    "                enc_input  = X, \n",
    "                enc_model  = enc_learn,\n",
    "                cpu        = cpu, \n",
    "                average_seq_dim = average_seq_dim,\n",
    "                verbose    = verbose,\n",
    "                to_numpy = to_numpy,\n",
    "                **kwargs\n",
    "            )\n",
    "        case _:\n",
    "            print_flush(f\"[ get_enc_embs_set_stride_set_batch_size ] Model embeddings implementation is not yet implemented for {enc_learn_class}.\")\n",
    "    # Ñapa: TODO: Gestionar que no se queden en memoria los modelos porque ocupan el 40% de la GPU al llamarlos desde R\n",
    "    if verbose > 0: print_flush(f\"get_enc_embs_set_stride_set_batch_size | Before moving to CPU | embs~{embs.shape}\")\n",
    "    if cpu:\n",
    "        #X.cpu()\n",
    "        enc_learn.cpu()\n",
    "        try: \n",
    "            enc_lear.dls.cpu()\n",
    "        except Exception as e: \n",
    "            print_flush(f\"get_enc_embs_set_stride_set_batch_size | Exception: {e}\")\n",
    "        #kwargs_to_cpu_(**kwargs)\n",
    "    if verbose > 0: print_flush(f\"get_enc_embs_set_stride_set_batch_size | embs~{embs.shape} -->\")\n",
    "    return embs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a1ef71",
   "metadata": {},
   "source": [
    "## Fine-tunning\n",
    "> Take a look on [HuggingFace - Fine-tune a pretrained model](https://huggingface.co/docs/transformers/training) if not used to few-shot learning or fine-tuning models.\n",
    "\n",
    "Steps: \n",
    "\n",
    "1) Prepare the dataset\n",
    "2) Batch the data\n",
    "   - Remember splitting between train & test dataset\n",
    "   - Remember to use DataLoader to iterate over batches\n",
    "4) Load the trained model and check if any modification is needed\n",
    "   - Check wether any layer may be substituted by an \"identity\" if not needed for your case\n",
    "   - Check if any dimension in a conversion layer may be changed to fit your dataset.\n",
    "5) Select an optimizer from torch.optim (Adam)\n",
    "6) ¿If using transformer, lr_scheduler? \n",
    "7) Training loop\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b0d63a-96d8-450f-8a4b-0c5555b4ff25",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "11f146b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def random_windows(X : List [ List [ List [ float ]]], n_windows = None, percent = 0.2, verbose = 0):\n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "    - X: Numpy array of windows. Expected shape: [batch_size or n_samples, n_vars, window_len]\n",
    "    Given a numpy array of windows, selects:\n",
    "    - n_windows random windows from the array, if n_windows is given.\n",
    "    - ceil(percent*len(X)) random windows otherwise\n",
    "    \"\"\"\n",
    "    n_windows = int(min(X.shape[0], n_windows) if n_windows is not None else np.ceil(percent*X.shape[0]))\n",
    "    if verbose > 0: print_flush(f\"Random windows | n_windows: {n_windows}\")\n",
    "    random_indices = np.random.randint(0, int(X.shape[0]), n_windows)\n",
    "    windows = X[ random_indices ]\n",
    "    windows = torch.from_numpy(windows)\n",
    "    if verbose > 0: print_flush(f\"windows ~ {windows.shape}\")\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fca51dd",
   "metadata": {},
   "source": [
    "### Moment\n",
    "> Follow the tutorial in the original repository: [Moment - Imputation](https://github.com/moment-timeseries-foundation-model/moment/blob/main/tutorials/imputation.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a12d2cca-f4c8-40f5-a16a-9563d7ce0535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo: poner en docker\n",
    "#! pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c3ddcde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from momentfm.utils.masking import Masking\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import get_scheduler\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7548dcac-1a8e-4933-a8e9-85a3309f3b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def fine_tune_moment_compute_loss_check_sizes_(batch, output, verbose = 0):\n",
    "    if verbose > 0: print_flush(\"--> fine_tune_moment_compute_loss_check_sizes_\")\n",
    "    b = batch.clone()\n",
    "    b_2 = batch.shape[2]\n",
    "    re_2 = output.reconstruction.shape[2]\n",
    "    if b_2 > re_2:\n",
    "        if verbose > 0: print_flush(f\" Fine tune loop | TODO: Why? Original {b_2} > {re_2}  Reconstruction\")\n",
    "        b = b[...,:re_2]\n",
    "    elif re_2 > b_2:\n",
    "        if verbose > 1: print_flush(f\" Fine tune loop | Why ? Original {b_2} < {re_2} Reconstruction ? Padding\")\n",
    "        output.reconstruction = output.reconstruction[...,:b_2]\n",
    "    else: \n",
    "        if verbose > 1: print_flush(f\" Fine tune loop | re_2 {re_2} == {b_2} y_2\")\n",
    "    if verbose > 1: \n",
    "        print_flush(f\"---------- Checking loss  ------- | reconstruction ~ {output.reconstruction.shape} | original_ ~ {b.shape}\")\n",
    "    if verbose > 0: print_flush(\"fine_tune_moment_compute_loss_check_sizes_ -->\")\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1e36e8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def fine_tune_moment_compute_loss(batch, output, criterion = torch.nn.MSELoss, verbose = 0, input_mask = None, mask = None):\n",
    "    if verbose > 0: print_flush(\"--> fine_tune_moment_compute_loss\")\n",
    "    b = fine_tune_moment_compute_loss_check_sizes_(batch = batch, output = output, verbose = verbose)\n",
    "    if verbose > 0: print_flush(f\"fine_tune_moment_compute_loss | b~{b.shape} | o~{output.reconstruction.shape}\")\n",
    "    compute_loss = criterion()\n",
    "    recon_loss = compute_loss(output.reconstruction, b)\n",
    "    batch_masks = output.input_mask if input_mask is None else input_mask\n",
    "    mask = output.pretrain_mask if mask is None else mask\n",
    "    if verbose > 1: print_flush(f\"fine_tune_moment_compute_loss | batch ~ {b.shape}\")\n",
    "    if verbose > 1: print_flush(f\"fine_tune_moment_compute_loss | batch_masks ~ {batch_masks.shape}\")\n",
    "    if verbose > 1: print_flush(f\"fine_tune_moment_compute_loss | mask ~ {mask.shape}\")\n",
    "    observed_mask = batch_masks * (1-mask)\n",
    "    masked_loss = observed_mask * recon_loss\n",
    "    loss = masked_loss.nansum() / (observed_mask.nansum() + 1e-7)\n",
    "    if verbose > 1: print_flush(f\"fine_tune_moment_compute_loss | loss: {loss.item()}\")\n",
    "    if verbose > 0: print_flush(f\"Loss type: {type(loss)}\")  # Debe ser <class 'torch.Tensor'>\n",
    "    if verbose > 0: print_flush(\"fine_tune_moment_compute_loss -->\")\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff7b693",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def fine_tune_moment_eval_preprocess(\n",
    "    predictions : List [ List [ float ]],\n",
    "    references : List [ List [ float ]],\n",
    "    verbose : int = 0\n",
    "):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - predictions torch (float)\n",
    "    - references torch (float)\n",
    "    Returns: \n",
    "        - Predictions and references ensuring same shape and no NaN values. \n",
    "        - Uses the shape of the smallest torch for the modification.\n",
    "    \"\"\"\n",
    "    if verbose > 0: \n",
    "        print(f\"fine_tune_moment_eval | Before reshape | preds~{predictions.shape}\")            \n",
    "        print(f\"fine_tune_moment_eval | Before reshape | refs~{references.shape}\")\n",
    "    predictions = einops.rearrange(predictions, \"b v w -> (b v) w\")\n",
    "    references = einops.rearrange(references, \"b v w -> (b v) w\")\n",
    "    # Avoid NaN \n",
    "    if predictions.shape[1] > references.shape[1]: predictions = predictions[:,:references.shape[1]]\n",
    "    if predictions.shape[1] < references.shape[1]: references = references[:,:predictions.shape[1]]\n",
    "    if verbose > 0: \n",
    "        print(f\"Eval | After reshape | preds~{predictions.shape}\")\n",
    "        print(f\"Eval | After reshape | refs~{references.shape}\")\n",
    "        \n",
    "    nan_mask = torch.isnan(predictions) | torch.isnan(references)\n",
    "    predictions = torch.where(nan_mask, torch.tensor(0.0), predictions)\n",
    "    references = torch.where(nan_mask, torch.tensor(0.0), references)\n",
    "    if verbose > 0: \n",
    "        print(f\"Eval | After NaN | preds~{predictions.shape}\")\n",
    "        print(f\"Eval | After NaN | refs~{references.shape}\")\n",
    "    return predictions, references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0cc18bf9-fd6f-4758-9862-e0e73206c9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def fine_tune_moment_eval_(\n",
    "    enc_learn : Learner,\n",
    "    dl_eval   : DataLoader,\n",
    "    num_epochs: int = 1,\n",
    "    cpu       : bool = False,\n",
    "    verbose   : int = 0\n",
    "):\n",
    "    # Select device\n",
    "    device = \"cpu\" if cpu else torch.cuda.current_device()\n",
    "    # Load metrics\n",
    "    mse_metric = evaluate.load('mse', \"multilist\")\n",
    "    rmse_metric = evaluate.load('mse', \"multilist\")\n",
    "    mae_metric = evaluate.load('mae', \"multilist\")\n",
    "    smape_metric = evaluate.load(\"smape\", \"multilist\")\n",
    "    num_evaluation_steps = len(dl_eval)\n",
    "    progress_bar = tqdm(range(num_evaluation_steps))\n",
    "    # Predict evaluation dataset\n",
    "    enc_learn.eval()\n",
    "    for batch in dl_eval:\n",
    "        with torch.no_grad():\n",
    "            output = sure_eval_moment(\n",
    "                enc_learn = enc_learn, \n",
    "                cpu = cpu,\n",
    "                verbose = verbose,                     \n",
    "                y = batch, \n",
    "                input_mask = None,\n",
    "                mask = None,\n",
    "                padd_step = 100, \n",
    "                max_trials = 5, \n",
    "                acts_indices = None\n",
    "            )\n",
    "        logits = output.logits\n",
    "        predictions = output.reconstruction\n",
    "        references = batch\n",
    "        predictions, references = fine_tune_moment_eval_preprocess(predictions = predictions, references = references, verbose = verbose)\n",
    "        mse_metric.add_batch(predictions=predictions, references = references)\n",
    "        rmse_metric.add_batch(predictions=predictions, references = references)\n",
    "        mae_metric.add_batch(predictions=predictions, references = references)\n",
    "        smape_metric.add_batch(predictions=predictions, references = references)\n",
    "        progress_bar.update(1)\n",
    "    mse   = mse_metric.compute(squared = False)\n",
    "    rmse  = rmse_metric.compute(squared = True)\n",
    "    mae   = mae_metric.compute()\n",
    "    smape = smape_metric.compute()\n",
    "    eval_results = {\n",
    "        \"mse\": mse,\n",
    "        \"rmse\": rmse,\n",
    "        \"mae\": mae,\n",
    "        \"smape\": smape\n",
    "    }\n",
    "    enc_learn.train()\n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8f06b0ec-8c25-470c-a315-fb89c40e2e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def fine_tune_moment_train_(\n",
    "    enc_learn : Learner, \n",
    "    dl_train: DataLoader,\n",
    "    ds_train,\n",
    "    batch_size : int,\n",
    "    num_epochs : int = 1,\n",
    "    criterion = torch.nn.MSELoss, \n",
    "    optimizer = torch.optim.Adam, \n",
    "    lr = 1e-4, \n",
    "    lr_scheduler_flag = False, \n",
    "    lr_scheduler_name = \"linear\",\n",
    "    lr_scheduler_num_warmup_steps = 0,\n",
    "    cpu = False,\n",
    "    verbose = 0\n",
    "):\n",
    "    # Optimizer and learning rate scheduler\n",
    "    optimizer = torch.optim.Adam(enc_learn.parameters(), lr)\n",
    "    num_training_steps = num_epochs * len(dl_train)\n",
    "    losses = []\n",
    "    if lr_scheduler_flag:\n",
    "        lr_scheduler = get_scheduler(\n",
    "            name = lr_scheduler_name,\n",
    "            optimizer = optimizer,\n",
    "            num_warmup_steps = lr_scheduler_num_warmup_steps,\n",
    "            num_training_steps = num_training_steps\n",
    "        )\n",
    "    # Select device\n",
    "    device = \"cpu\" if cpu else torch.cuda.current_device()\n",
    "        \n",
    "    # Training loop\n",
    "    if verbose > 1: print_flush(\"fine_tune_moment_ | Training loop\")\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "    # Masks\n",
    "    mask_generator = Masking(mask_ratio = 0.3)\n",
    "    n_samples, n_channels, window_size = ds_train.shape\n",
    "    batch_masks = torch.ones(\n",
    "        (batch_size, window_size), \n",
    "        device = device\n",
    "    ).long()\n",
    "        \n",
    "    if verbose > 1: print_flush(\"fine_tune_moment_ | Fine tune loop\")        \n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in dl_train:\n",
    "            batch.to(device)\n",
    "            bms = batch_masks\n",
    "            if batch.shape[0] < batch_masks.shape[0]:  \n",
    "                bms = batch_masks[:batch.shape[0]]\n",
    "            if verbose > 1: \n",
    "                print_flush(\n",
    "                    f\"fine_tune_moment_ | Fine tune loop | batch ~ {batch.shape} | batch_masks ~ {bms.shape}\"\n",
    "                )\n",
    "            mask = mask_generator.generate_mask(\n",
    "                x = batch,\n",
    "                input_mask = bms\n",
    "            )\n",
    "            if mask.shape[0] < bms.shape[0]:  bms = batch_masks[:mask.shape[0]]\n",
    "            if mask.shape[1]  < batch_masks.shape[1] : mask = torch.nn.functional.pad(mask,(0,batch_masks.shape[1]-mask.shape[1]))\n",
    "            if verbose > 1: \n",
    "                print_flush(\n",
    "                    f\"fine_tune_moment_ | Fine tune loop | batch ~ {batch.shape} | batch_masks ~ {bms.shape} | mask ~ {mask.shape}\"\n",
    "                )\n",
    "            output = sure_eval_moment(\n",
    "                enc_learn = enc_learn, \n",
    "                cpu = cpu,\n",
    "                verbose = verbose-1, \n",
    "                y = batch, \n",
    "                input_mask = batch_masks, # None\n",
    "                mask = mask, # None\n",
    "                padd_step = 100, \n",
    "                max_trials = 5, \n",
    "                acts_indices = None\n",
    "            )\n",
    "            # Compute output loss\n",
    "            loss = fine_tune_moment_compute_loss(batch, output, criterion, verbose = verbose, input_mask = bms, mask = mask )\n",
    "            losses.append(loss.item())\n",
    "            # Update weights\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # Optimize\n",
    "            optimizer.step()\n",
    "            if lr_scheduler_flag: lr_scheduler.step()\n",
    "            progress_bar.update(1)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ae7818",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from torch.nn.modules.loss import _Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f21133e9-63d4-41e6-8fa0-42265ede334c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def fine_tune_moment_(\n",
    "    X                               : List [ List [ List [ float ]]], \n",
    "    enc_learn                       : Learner, \n",
    "    stride                          : int   = 1,      \n",
    "    batch_size                      : int   = 32,\n",
    "    cpu                             : bool  = False,\n",
    "    to_numpy                        : bool  = True, \n",
    "    verbose                         : int   = 0, \n",
    "    time_flag                       : bool  = False,\n",
    "    n_windows                       : int   = None,\n",
    "    n_windows_percent               : float = 0.2,\n",
    "    validation_percent              : float = 0.2, \n",
    "    training_percent                : float = 0.2,\n",
    "    num_epochs                      : int   = 3,\n",
    "    shot                            : bool  = True,\n",
    "    eval                            : bool  = True,\n",
    "    criterion                       : _Loss = torch.nn.MSELoss, \n",
    "    optimizer                               = torch.optim.Adam, \n",
    "    lr                              : float =  1e-4, \n",
    "    lr_scheduler_flag               : bool  = False, \n",
    "    lr_scheduler_name               : str   = \"linear\",\n",
    "    lr_scheduler_num_warmup_steps   : int   = 0,\n",
    "):   \n",
    "    if verbose > 0: print_flush(\"--> fine_tune_moment_\")\n",
    "    t_shot = 0\n",
    "    t_eval_1 = 0\n",
    "    t_eval_2 = 0\n",
    "    losses = []\n",
    "    eval_results_pre = \"\"\n",
    "    eval_results_post = \"\"\n",
    "\n",
    "    if time_flag:\n",
    "        timer = Time()\n",
    "    # Prepare the dataset\n",
    "    train_split_index = min(X.shape[0], n_windows) if n_windows is not None else np.ceil(training_percent * X.shape[0])\n",
    "    eval_split_index = min(X.shape[0], n_windows) if n_windows is not None else np.ceil(validation_percent * X.shape[0])\n",
    "    train_split_index = int(train_split_index)\n",
    "    eval_split_index = int(eval_split_index)\n",
    "    if shot: ds_train = X[:train_split_index]\n",
    "    if eval: ds_test  = torch.from_numpy(X[:eval_split_index]).float()\n",
    "    # -- Select only the small percentage for few-shot\n",
    "    if verbose > 1: print_flush(\"fine_tune_moment_ | Random windows\")\n",
    "    if shot:\n",
    "        ds_train = random_windows(ds_train, n_windows, n_windows_percent, verbose-1)\n",
    "        ds_train = ds_train.float()\n",
    "        # Create the dataloader\n",
    "        dl_train = DataLoader(ds_train, batch_size = batch_size, shuffle = True)\n",
    "    if eval: dl_eval  = DataLoader(ds_test, batch_size = batch_size, shuffle = False)\n",
    "    if eval:\n",
    "        if time_flag: timer.start()\n",
    "        eval_results_pre = fine_tune_moment_eval_(\n",
    "            enc_learn = enc_learn,\n",
    "            dl_eval = dl_eval,\n",
    "            num_epochs = num_epochs,\n",
    "            cpu = cpu,\n",
    "            verbose = verbose-1\n",
    "        )\n",
    "        if time_flag: \n",
    "            timer.end()\n",
    "            t_eval_1 = timer.duration()\n",
    "            if verbose > 0: timer.show()\n",
    "    if shot:\n",
    "        if time_flag: timer.start()\n",
    "        losses = fine_tune_moment_train_(\n",
    "            enc_learn = enc_learn,\n",
    "            dl_train = dl_train,\n",
    "            ds_train = ds_train,\n",
    "            batch_size = batch_size,\n",
    "            num_epochs = num_epochs,\n",
    "            criterion = torch.nn.MSELoss, \n",
    "            optimizer = torch.optim.Adam, \n",
    "            lr = 1e-4, \n",
    "            lr_scheduler_flag = False, \n",
    "            lr_scheduler_name = \"linear\",\n",
    "            lr_scheduler_num_warmup_steps = 0,\n",
    "            cpu = cpu,\n",
    "            verbose = verbose-1\n",
    "        )\n",
    "        if time_flag:\n",
    "            timer.end()\n",
    "            t_shot = timer.duration()\n",
    "            if verbose > 0: timer.show()\n",
    "    if eval:    \n",
    "        if time_flag: timer.start()\n",
    "        eval_results_post = fine_tune_moment_eval_(\n",
    "            enc_learn = enc_learn,\n",
    "            dl_eval = dl_eval,\n",
    "            num_epochs = num_epochs,\n",
    "            cpu = cpu,\n",
    "            verbose = verbose-1\n",
    "        )\n",
    "        if time_flag:\n",
    "            timer.end()\n",
    "            t_eval_2 = timer.duration()\n",
    "            if verbose > 0: timer.show()\n",
    "        if verbose > 0: print(\"Evaluation summary\")\n",
    "    return losses, eval_results_pre, eval_results_post, t_shot, t_eval_1, t_eval_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a5c9b646-2a6c-460b-bf56-e10fb740ac17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#import wandb\n",
    "#from dvats.utils import *\n",
    "#wandb_api = wandb.Api()\n",
    "#enc_artifact = wandb_api.artifact('deepvats/mvp-SWV:latest')\n",
    "#enc_learner = enc_artifact.to_obj()\n",
    "#X = torch.rand(9, 1, 48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c8101184-0680-416f-a378-998c8fd00502",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#%time\n",
    "#embs = get_enc_embs(X, enc_learner, cpu=True)\n",
    "#test_eq(embs.shape[0], X.shape[0])\n",
    "#embs.shape, embs.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e04ade89-a3ad-4358-960d-5d21806ba01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#%%time #TODO dont work with nb2py\n",
    "#embs = get_enc_embs(X, enc_learner, cpu=False, to_numpy=False)\n",
    "#test_eq(embs.shape[0], X.shape[0])\n",
    "#embs.shape, embs.__class__, embs.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "83e63662-c5d3-47ea-a43f-5db9a2437745",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "#%%time #TODO --> dont works with nb2py\n",
    "#embs = get_enc_embs(X, enc_learner, cpu=False, to_numpy=True)\n",
    "#test_eq(embs.shape[0], X.shape[0])\n",
    "#embs.shape, embs.__class__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
