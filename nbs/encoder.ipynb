{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa093821-a793-49fd-a8c2-32cacdbdc964",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6adb50-be0c-4f0b-9beb-38d4e78c5e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#%load_ext autoreload --> Not working TODO:REVISAR\n",
    "# %autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67bedfb7-6a74-4f6c-a769-cc79fe37686c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/share/miniconda3/envs/env/lib/python3.10/site-packages/aeon/base/__init__.py:24: FutureWarning: The aeon package will soon be releasing v1.0.0 with the removal of legacy modules and interfaces such as BaseTransformer and BaseForecaster. This will contain breaking changes. See aeon-toolkit.org for more information. Set aeon.AEON_DEPRECATION_WARNING or the AEON_DEPRECATION_WARNING environmental variable to 'False' to disable this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import warnings\n",
    "import math\n",
    "from dvats.memory import *\n",
    "import dvats.utils as ut\n",
    "from dvats.config import show_attrdict\n",
    "from copy import deepcopy\n",
    "## -- Classes & types\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional, Tuple, Callable, Union, Any\n",
    "\n",
    "# Fastai\n",
    "#| export\n",
    "from fastai.learner import Learner\n",
    "from tsai.data.core import TSDataLoaders\n",
    "# Moirai\n",
    "import uni2ts.model.moirai.module as moirai\n",
    "import uni2ts.model.moirai.forecast as moirai_forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6a4a02-5015-4a3e-9a64-6089de4e9532",
   "metadata": {},
   "source": [
    "# Encoder\n",
    "\n",
    "> Architectures and functions for creating encoders that create the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b014a18f-8538-4ed7-98ee-ddb165692553",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fastcore.all import *\n",
    "from tsai.callback.MVP import *\n",
    "from tsai.imports import *\n",
    "from tsai.models.InceptionTimePlus import InceptionTimePlus\n",
    "from tsai.models.explainability import get_acts_and_grads\n",
    "from tsai.models.layers import *\n",
    "from tsai.data.validation import combine_split_data\n",
    "from tsai.basics import *\n",
    "from fastai.callback.hook import hook_outputs\n",
    "from momentfm import MOMENTPipeline\n",
    "from gluonts.dataset.pandas import PandasDataset\n",
    "from tsai.data.validation import TimeSplitter\n",
    "from fastai.callback.wandb import WandbCallback\n",
    "from fastai.callback.progress import ShowGraphCallback\n",
    "from fastai.callback.schedule import *\n",
    "from fastai.callback.tracker import EarlyStoppingCallback\n",
    "from fastai.callback.tracker import SaveModelCallback\n",
    "import time\n",
    "import einops\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33c94f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from tsai.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec211d6-c9be-4e42-8c58-47df6678ef79",
   "metadata": {},
   "source": [
    "## Encoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df581235",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class EncoderInput:\n",
    "    # Data\n",
    "    _data               : Union [ pd.DataFrame, List [ List [ List [ float ]]] ] = None\n",
    "    _size               : int                               = None\n",
    "    _shape              : Optional [ Tuple [ int, ... ] ]   = None\n",
    "    _shapes             : List [ Tuple [ int, ...]]         = None\n",
    "    stride              : int                               = None\n",
    "    batch_size          : int                               = None\n",
    "    _update_size        : bool                              = True\n",
    "    _update_shape       : bool                              = True\n",
    "    # Windows                   \n",
    "    n_windows           : int                               = None\n",
    "    n_windows_percent   : float                             = None\n",
    "    validation_percent  : float                             = None\n",
    "    training_percent    : float                             = None\n",
    "    window_mask_percent : float                             = None\n",
    "    # Time                  \n",
    "    time_flag           : bool                              = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self._update_size       = True\n",
    "        self._update_shape      = True\n",
    "        #Todo: check how to validate the input dataset allowing both windowed or not\n",
    "        # --- Not working\n",
    "        ###self._data              = ut._check_value(self.data, None, \"_data\", pd.DataFrame, allow_none = True )\n",
    "        ###if self._data is None: \n",
    "        ###    self._data = ut._validate_nested_list(self._data, None, \"_data\", [float, int], 3, False, False, False)\n",
    "        self.stride,_               = ut._check_value(self.stride, 1, \"stride\", int, positive = True)\n",
    "        self.batch_size,_           = ut._check_value(self.batch_size, 32, \"batch_size\", int,  )\n",
    "        self.validation_percent,_   = ut._check_value(self.validation_percent, 0.2, \"validation_percent\", percent = True)\n",
    "        self.training_percent,_     = ut._check_value(self.training_percent, 0.2, \"training_percent\", percent = True)\n",
    "        self.window_mask_percent,_  = ut._check_value(self.window_mask_percent, 0.3, \"training_percent\", percent = True)\n",
    "        self.time_flag,_            = ut._check_value(self.time_flag, False, \"time_flag\", bool)\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        if self._data is not None and ( self._update_size or self._size is None or self._size == 0):\n",
    "            self._size          = len(self._data)\n",
    "            self._update_size   = False\n",
    "            self._size,_ = ut._check_value(self._size, 0, \"_size\", int)\n",
    "            self._size = max(self._size, 0)\n",
    "        elif self._update_size: \n",
    "            self._size = 0\n",
    "            self._update_size = True\n",
    "        return self._size\n",
    "    \n",
    "    @property\n",
    "    def shape(self) -> Tuple[int, ...]:\n",
    "        if (\n",
    "                self._data is not None and \n",
    "                ( self._update_shape or self._shape is None or self._shape == 0 )\n",
    "        ):\n",
    "            try: \n",
    "                self._shape     = self._data.shape\n",
    "                self._shapes    = [ self._shape ]\n",
    "            except:\n",
    "                self._shape  = self._data[0].shape\n",
    "                self._shapes = [ self._data[i].shape for i in range(len(self._data))]\n",
    "            self._update_shape = False\n",
    "        elif self._update_shape: \n",
    "            self._shape = 0,\n",
    "            self._shapes = []\n",
    "            self._update_shape = True\n",
    "        return self._shape\n",
    "    @property\n",
    "    def shapes(self) -> List [ Tuple [ int, ... ]]:\n",
    "        if (\n",
    "            self._data is not None and \n",
    "            ( self._update_shape or self._shapes is None or self._shapes ==[])\n",
    "        ):\n",
    "            try: \n",
    "                self._shape     = self._data.shape\n",
    "                self._shapes    = [ self._shape ]\n",
    "            except:\n",
    "                self._shape     = self._data[0].shape\n",
    "                self._shapes    = [ self._data[i].shape for i in range(len(self._data))]\n",
    "            self._update_shape  = False\n",
    "        elif self._update_shape: \n",
    "            self._shape         = 0,\n",
    "            self._shapes        = []\n",
    "            self._update_shape  = True\n",
    "        return self._shapes\n",
    "            \n",
    "    @property\n",
    "    def data(self):\n",
    "        return self._data\n",
    "    \n",
    "    @data.setter\n",
    "    def data(self, value):\n",
    "        self._data          = value\n",
    "        self._update_size   = True\n",
    "        self._update_shape  = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1687d91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class LRScheduler:\n",
    "    lr              : float = None\n",
    "    flag            : bool  = None\n",
    "    name            : str   = None\n",
    "    num_warmup_steps: int   = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.lr                 = self._check_lr(self.lr, 1e-5)\n",
    "        self.flag               = self._check_flag(self.flag, False)\n",
    "        self.name               = self._check_name(self.name, \"OneCycleR\")\n",
    "        self.num_warmup_steps   = self._check_steps(self.num_warmup_steps, 0)\n",
    "\n",
    "    # Validation methods\n",
    "    def _check_lr(self, value, default):\n",
    "        if not isinstance(value, (float, int)) or not math.isfinite(value) or value <= 0:\n",
    "            warnings.warn(f\"Invalid learning rate 'lr' ({value}). Using default: {default}\")\n",
    "            return default\n",
    "        return float(value)\n",
    "\n",
    "    def _check_flag(self, value, default):\n",
    "        if not isinstance(value, bool):\n",
    "            warnings.warn(f\"Invalid type for 'flag' ({type(value)}). Using default: {default}\")\n",
    "            return default\n",
    "        return value\n",
    "\n",
    "    def _check_name(self, value, default):\n",
    "        if not isinstance(value, str):\n",
    "            warnings.warn(f\"Invalid type for 'name' ({type(value)}). Using default: {default}\")\n",
    "            return default\n",
    "        return value\n",
    "\n",
    "    def _check_steps(self, value, default):\n",
    "        if not isinstance(value, int) or value < 0:\n",
    "            warnings.warn(f\"Invalid type or negative value for 'num_warmup_steps' ({value}). Using default: {default}\")\n",
    "            return default\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886179ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class EncoderOptimizer():\n",
    "    criterion   : Optional   [ torch.nn.Module ]          = torch.nn.MSELoss\n",
    "    optimizer   : Optional   [ torch.optim.Optimizer ]    = None\n",
    "    lr          : Union      [ float, LRScheduler ]       = 1e-5\n",
    "\n",
    "    def _post__init__(self):\n",
    "        self.lr,_ = ut._check_value( self.lr, 1e-5, \"lr\", [ int, float ], False, True, False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d32d39e-3043-4f5a-98c3-8d86c26249b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class Encoder():\n",
    "    model               : Tuple [ \n",
    "                            MOMENTPipeline,\n",
    "                            Learner,\n",
    "                            moirai.MoiraiModule\n",
    "                        ]                   = None\n",
    "    input               : EncoderInput      = EncoderInput()\n",
    "    mssg                : ut.Mssg           = ut.Mssg()\n",
    "    cpu                 : bool              = False\n",
    "    to_numpy            : bool              = False\n",
    "    num_epochs          : int               = 1\n",
    "    optim               : EncoderOptimizer  = EncoderOptimizer()\n",
    "    mask_stateful       : bool              = False\n",
    "    mask_future         : bool              = False\n",
    "    mask_sync           : bool              = False\n",
    "    eval_stats_pre      : AttrDict          = None\n",
    "    eval_stats_post     : AttrDict          = None\n",
    "    use_moment_masks    : bool              = False\n",
    "    model_class         : str               = None\n",
    "    time_flag           : bool              = False\n",
    "    use_wandb           : bool              = False\n",
    "    analysis_mode       : str               = 'online'\n",
    "    splits              : Tuple             = None\n",
    "    show_plot           : bool              = False\n",
    "    norm_by_sample      : bool              = True\n",
    "    norm_use_single_batch : bool            = True\n",
    "    metrics             : List [ Callable ] = None\n",
    "    #mvp_ws              : Tuple [ int, int ]= 0,0\n",
    "    def __post_init__(self):\n",
    "        self.model          , _ = ut._check_value(self.model, None, \"model\", [ MOMENTPipeline, Learner, moirai.MoiraiModule ], True, False, False, mssg = self.mssg)\n",
    "        self.model              = self.set_model_(self.model)\n",
    "        ## TODO: check how to do this check\n",
    "        #self.input          , _ = ut._check_value(self.input, EncoderInput(), \"input\", EncoderInput, True)\n",
    "        self.mssg           , _ = ut._check_value(self.mssg, ut.Mssg(), \"mssg\", ut.Mssg, mssg = self.mssg)\n",
    "        self.cpu            , _ = ut._check_value(self.cpu, False, \"cpu\", bool, mssg = self.mssg)\n",
    "        self.to_numpy       , _ = ut._check_value(self.to_numpy, False, \"to_numpy\", bool,  mssg = self.mssg)\n",
    "        self.num_epochs     , _ = ut._check_value(self.num_epochs, 1, \"num_epochs\", int, False, True,  mssg = self.mssg)\n",
    "        ## TODO: check how to do this check\n",
    "        #self.optim          , _ = ut._check_value(self.optim, EncoderOptimizer(), \"optim\", EncoderOptimizer)\n",
    "        self.mask_stateful  , _ = ut._check_value(self.mask_stateful, False, \"mask_statefull\", bool,  mssg = self.mssg)\n",
    "        self.mask_future    , _ = ut._check_value(self.mask_future, False, \"mask_future\", bool,  mssg = self.mssg)\n",
    "        self.mask_sync      , _ = ut._check_value(self.mask_sync, False, \"mask_sync\", bool,  mssg = self.mssg)\n",
    "        self.eval_stats_pre , _ = ut._check_value(self.eval_stats_pre, None, \"eval_stats_pre\", AttrDict, True,  mssg = self.mssg)\n",
    "        self.eval_stats_post, _ = ut._check_value(self.eval_stats_post, None, \"eval_stats_post\", AttrDict, True,  mssg = self.mssg)\n",
    "        self.use_moment_masks, _= ut._check_value(self.use_moment_masks, False, \"use_moment_masks\", bool,  mssg = self.mssg)\n",
    "        self.model_class        = None # Must be computed through get_model_class to avoid errors\n",
    "        self.time_flag      , _ = ut._check_value(self.time_flag, False, \"time_flag\", bool,  mssg = self.mssg)\n",
    "        self.show_plot      , _ = ut._check_value(self.show_plot, False, \"show_plot\", bool, mssg = self.mssg)\n",
    "    \n",
    "    def print(self, **kwargs):\n",
    "        self.mssg.print(**kwargs)\n",
    "\n",
    "    def get_model_class(self, force : bool = False): \n",
    "        if force or self.model_class is None:\n",
    "            self.model_class = str(self.model.__class__)[8:-2]\n",
    "        return self.model_class\n",
    "    def set_model_(self, model):\n",
    "        if model is not None:\n",
    "            self.model          = model\n",
    "            self.model_class    = self.get_model_class() \n",
    "            try: # Initially it may not be defined and that would result in an execution error\n",
    "                self.fine_tune_     = self.set_fine_tune_()\n",
    "            except:\n",
    "                self.fine_tune_ = None\n",
    "        return self.model\n",
    "    \n",
    "    def get_splits_(self, n_sample: int = None):\n",
    "        self.mssg.initial_(ut.funcname())\n",
    "        #TODO: add checks for datatype to ensure the dataset is not already windowed\n",
    "        assert self.analysis_mode in [ 'ofline', 'online'], 'Invalid analysis mode'\n",
    "        X = self.input.data if n_sample is None else self.input.data[n_sample]\n",
    "        self.mssg.print(f\"len(X)={len(X)}\")\n",
    "        match self.analysis_mode:\n",
    "            case 'online':\n",
    "                self.mssg.print(\"Online analysis\", verbose_level = self.mssg.level+1)\n",
    "                self.splits = TimeSplitter(valid_size = 0.2, show_plot = self.show_plot)(X)\n",
    "            case 'offline':\n",
    "                self.mssg.print(\"Offline analysis\", verbose_level = self.mssg.level+1)\n",
    "                self.splits = get_splits(np.arange(len(X)), valid_size=self.valid_size, show_plot = self.show_plot)\n",
    "            case _:\n",
    "                raise NotImplementedError(f\"Encoderl{ut.funcname()} | Case {self.analysis_mode} not implemented. Use one of the following options: <online|offline>.\")\n",
    "        self.mssg.print(f\"X~{X.shape}\")\n",
    "        self.mssg.print(f\"Train: {len(self.splits[0])} | Test { len(self.splits[1])}\")\n",
    "        self.mssg.final()\n",
    "        return X\n",
    "\n",
    "    #TODO: poner los equivalentes para train, eval, get_embeddings, get_acts, etc.\n",
    "    \n",
    "    # Fine_tune_single_\n",
    "    def fine_tune_moment_single_(self):\n",
    "        raise NotImplementedError(f\"Encoder.{ut.funcname()} not yet implemented\")\n",
    "    def fine_tune_mvp_single_(self):\n",
    "        raise NotImplementedError(f\"Encoder.{ut.funcname()} not yet implemented\")\n",
    "    def fine_tune_moirai_single_(self):\n",
    "        raise NotImplementedError(f\"Encoder.{ut.funcname()} not yet implemented\")\n",
    "    def fine_tune_single_(self):\n",
    "        raise NotImplementedError(f\"Encoder.{ut.funcname()} not yet implemented\")\n",
    "\n",
    "    # Fine_tune_\n",
    "    def fine_tune_moment_(self, eval_pre = False, eval_post = False, shot = True, time_flag = False, use_moment_masks = False): \n",
    "        raise NotImplementedError(f\"Encoder.{ut.funcname()} not yet implemented\")\n",
    "    def fine_tune_mvp_(self, eval_pre = False, eval_post = False, shot = True, time_flag = False): \n",
    "        raise NotImplementedError(f\"Encoder.{ut.funcname()} not yet implemented\")\n",
    "    def fine_tune_moirai_(self, eval_pre = False, eval_post = False, shot = True, time_flag = False): \n",
    "        raise NotImplementedError(f\"Encoder.{ut.funcname()} not yet implemented\")\n",
    "    def fine_tune_(self, eval_pre = False, eval_post = False, shot = True, time_flag = False):\n",
    "        raise NotImplementedError(f\"Encoder.{ut.funcname()} not yet implemented\")\n",
    "    def set_fine_tune_(self):\n",
    "        raise NotImplementedError(f\"Encoder.{ut.funcname()} not yet implemented\")\n",
    "    def show_eval_stats(self):\n",
    "        raise NotImplementedError(f\"Encoder.{ut.funcname()} not yet implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e10213",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def set_fine_tune_single_(\n",
    "    self: Encoder\n",
    ") -> Callable:\n",
    "    self.mssg.initial_(ut.funcname())\n",
    "    model_class = self.get_model_class()\n",
    "    self.mssg.print(f\"Model class: {model_class}\")\n",
    "    match model_class:\n",
    "        case \"momentfm.models.moment.MOMENTPipeline\":\n",
    "            self.fine_tune_single_ = self.fine_tune_moment_single_\n",
    "        case \"fastai.learner.Learner\":\n",
    "            self.fine_tune_single_ = self.fine_tune_mvp_single_\n",
    "        case \"uni2ts.model.moirai.module.MoiraiModule\":\n",
    "            self.fine_tune_single_ = self.fine_tune_moirai_single_\n",
    "        case _:\n",
    "            self.mssg.print(f\"Fine-tune single shot implementation is not yet implemented for {self.model_class}.\", verbose_level = self.mssg.level+1)\n",
    "            raise NotImplementedError(f\"fine_tune_single_ | Not yet implemented for {self.model_class}\")\n",
    "    self.mssg.final(ut.funcname())\n",
    "    return(self.fine_tune_single_)\n",
    "Encoder.set_fine_tune_single_ = set_fine_tune_single_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcdc396",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def set_fine_tune_(\n",
    "    self: Encoder\n",
    ") -> Callable:\n",
    "    self.mssg.initial_(\"set_fine_tune_\")\n",
    "    model_class = self.get_model_class()\n",
    "    self.mssg.print(f\"Model class: {model_class}\")\n",
    "    match model_class:\n",
    "        case \"momentfm.models.moment.MOMENTPipeline\":\n",
    "            self.mssg.print(f\"Moment\")\n",
    "            self.fine_tune_ = self.fine_tune_moment_\n",
    "        case \"fastai.learner.Learner\":\n",
    "            self.mssg.print(f\"MVP\")\n",
    "            self.fine_tune_ = self.fine_tune_mvp_\n",
    "        case \"uni2ts.model.moirai.module.MoiraiModule\":\n",
    "            self.mssg.print(f\"Moirai\")\n",
    "            self.fine_tune_ = self.fine_tune_moirai_\n",
    "        case _:\n",
    "            self.mssg.print(f\"Fine-tune implementation is not yet implemented for {self.model_class}.\", verbose_level = self.mssg.level+1)\n",
    "            raise NotImplementedError(f\"fine_tune | Not yet implemented for {self.model_class}\")\n",
    "    self.mssg.final(ut.funcname())\n",
    "    return(self.fine_tune_)\n",
    "Encoder.set_fine_tune_ = set_fine_tune_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9002505f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def show_eval_stats(\n",
    "    self            : Encoder, \n",
    "    print_to_path   : bool      = None, \n",
    "    print_path      : str       = None, \n",
    "    print_mode      : str       = None,\n",
    "    eval_pre        : bool = False,\n",
    "    eval_post       : bool = False,\n",
    "    eval_stats_pre  : AttrDict = None,\n",
    "    eval_stats_post : AttrDict = None,\n",
    "    func_name       : str = \"\"\n",
    "):\n",
    "    self.mssg.print(f\"{func_name} | Evaluation summary\")\n",
    "    self.eval_stats_pre = self.eval_stats_pre if eval_stats_pre is None else eval_stats_pre\n",
    "    self.eval_stats_post = self.eval_stats_post if eval_stats_post is None else eval_stats_post\n",
    "    self.mssg.to_path = self.mssg.to_path if print_to_path is None else print_to_path\n",
    "    self.mssg.path = self.mssg.path if print_path is None else print_path\n",
    "    self.mssg.mode = self.mssg.mode if print_mode is None else print_mode        \n",
    "    if (eval_pre):\n",
    "        self.mssg.print(f\"Eval pre: \")\n",
    "        show_attrdict(\n",
    "            self.eval_stats_pre,\n",
    "            print_to_path   = self.mssg.to_path,\n",
    "            print_path      = self.mssg.path,\n",
    "            print_mode      = self.mssg.mode\n",
    "        )\n",
    "    if eval_post:\n",
    "        self.mssg.print(f\"Eval post: \")\n",
    "        show_attrdict(\n",
    "            self.eval_stats_post,\n",
    "            print_to_path   = self.mssg.to_path,\n",
    "            print_path      = self.mssg.path,\n",
    "            print_mode      = self.mssg.mode \n",
    "        )\n",
    "Encoder.show_eval_stats = show_eval_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04637a46",
   "metadata": {},
   "source": [
    "### Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c036898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export \n",
    "class DCAE_torch(Module):\n",
    "    def __init__(self, c_in, seq_len, delta, nfs=[64, 32, 12], kss=[10, 5, 5],\n",
    "                 pool_szs=[2,2,3], output_fsz=10):\n",
    "        \"\"\"\n",
    "        Create a Deep Convolutional Autoencoder for multivariate time series of `d` dimensions,\n",
    "        sliced with a window size of `w`. The parameter `delta` sets the number of latent features that will be\n",
    "        contained in the Dense layer of the network. The the number of features\n",
    "        maps (filters), the filter size and the pool size can also be adjusted.\"\n",
    "        \"\"\"\n",
    "        assert all_equal([len(x) for x in [nfs, kss, pool_szs]], np.repeat(len(nfs), 3)), \\\n",
    "            'nfs, kss, and pool_szs must have the same length'\n",
    "        assert np.prod(pool_szs) == nfs[-1], \\\n",
    "            'The number of filters in the last conv layer must be equal to the product of pool sizes'\n",
    "        assert seq_len % np.prod(pool_szs) == 0, \\\n",
    "            'The product of pool sizes must be a divisor of the window size'\n",
    "        layers = []\n",
    "        for i in range_of(kss):\n",
    "            layers += [Conv1d(ni=nfs[i-1] if i>0 else c_in, nf=nfs[i], ks=kss[i]),\n",
    "                       nn.MaxPool1d(kernel_size=pool_szs[i])]\n",
    "        self.downsample = nn.Sequential(*layers)\n",
    "        self.bottleneck = nn.Sequential(OrderedDict([\n",
    "            ('flatten', nn.Flatten()),\n",
    "            ('latent_in', nn.Linear(seq_len, delta)),\n",
    "            ('latent_out', nn.Linear(delta, seq_len)),\n",
    "            ('reshape', Reshape(nfs[-1], seq_len // np.prod(pool_szs)))\n",
    "        ]))\n",
    "        layers = []\n",
    "        for i in reversed(range_of(kss)):\n",
    "            layers += [Conv1d(ni=nfs[i+1] if i != (len(nfs)-1) else nfs[-1],\n",
    "                              nf=nfs[i], ks=kss[i]),\n",
    "                       nn.Upsample(scale_factor=pool_szs[i])]\n",
    "        layers += [Conv1d(ni=nfs[0], nf=c_in, kernel_size=output_fsz)]\n",
    "        self.upsample = nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.downsample(x)\n",
    "        x = self.bottleneck(x)\n",
    "        x = self.upsample(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e59aa6e7-36df-4697-993e-60f737bb0f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 48])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "foo = torch.rand(3, 1, 48)\n",
    "m = DCAE_torch(c_in=foo.shape[1], seq_len=foo.shape[2], delta=12)\n",
    "m(foo).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1a4a1d-389c-481a-a54d-3f86cd2115f5",
   "metadata": {},
   "source": [
    "### Dictionary to get the default backbone modules to get the embeddings from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3eb9f1b6-ae45-4b6e-b535-c7f121208721",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "ENCODER_EMBS_MODULE_NAME = {\n",
    "    InceptionTimePlus: 'backbone', # for mvp based models\n",
    "    DCAE_torch: 'bottleneck.latent_in'#,\n",
    "    #MoiraiForecast: 'mask_encoding' #TODO: check\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05db8a5d-177c-48f5-a1b3-1d0adc4ccce6",
   "metadata": {},
   "source": [
    "## Get activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ddd69f0-e3b8-4ebc-8fdf-0808eb9e0539",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def kwargs_to_gpu_(**kwargs):\n",
    "    for key in kwargs:\n",
    "        try: #if not able to be moved, just not move it\n",
    "            kwargs[key] = kwargs[key].to(\"cuda\")\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "def kwargs_to_cpu_(**kwargs):\n",
    "    for key in kwargs:\n",
    "        try: #if not able to be moved, just not move it\n",
    "            kwargs[key] = kwargs[key].cpu()\n",
    "        except:\n",
    "            continue\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ac0f9ca-a5c1-4292-8e7b-d3f76872c1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_acts(\n",
    "    model : torch.nn.Module, \n",
    "    module: torch.nn.Module, \n",
    "    cpu   : bool, \n",
    "    verbose : int = 0,\n",
    "    retry: bool = False,\n",
    "    acts_indices: List [ int ] = None,\n",
    "    #- Printing options for debugging\n",
    "    print_to_path   : bool          = False,\n",
    "    print_path      : str           = \"~/data/logs/logs.txt\",\n",
    "    print_mode      : str           = 'a',\n",
    "    continue_if_fail: bool          = False,\n",
    "    **model_kwargs #Parameters of the model\n",
    "):\n",
    "    if verbose > 0:\n",
    "        ut.print_flush(f\"--> get acts | acts indices: {acts_indices}\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose, print_time = print_to_path)\n",
    "    if cpu:\n",
    "        if verbose > 0: ut.print_flush(f\"get acts | Moving to cpu\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose, print_time = print_to_path)\n",
    "        for key in model_kwargs:\n",
    "            try: #if not able to be moved, just not move it\n",
    "                model_kwargs[key] = model_kwargs[key].cpu()\n",
    "            except:\n",
    "                continue\n",
    "        model.to(\"cpu\")\n",
    "    else:\n",
    "        if verbose > 0: ut.print_flush(f\"get acts | Moving to gpu\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose, print_time = print_to_path)\n",
    "        for key in model_kwargs:\n",
    "            try: #if not able to be moved, just not move it\n",
    "                model_kwargs[key] = model_kwargs[key].to(\"cuda\")\n",
    "            except:\n",
    "                continue\n",
    "        model.to(\"cuda\")\n",
    "    if verbose > 0: ut.print_flush(f\"get acts | Add hooks\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose, print_time = print_to_path)\n",
    "    h_act = hook_outputs([module], detach = True, cpu = cpu, grad = False)\n",
    "    with torch.no_grad():\n",
    "        if verbose > 0: ut.print_flush(f\"get acts | --> Run forward\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose, print_time = print_to_path)\n",
    "        if retry:\n",
    "            if verbose > 0: ut.print_flush(f\"get acts | Retry\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose, print_time = print_to_path)\n",
    "            try: \n",
    "                preds = model.eval()(**model_kwargs)\n",
    "            except Exception as e:\n",
    "                ut.print_flush(f\"get acts | Retry | Error: {e}\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose, print_time = print_to_path)\n",
    "                ut.print_flush(f\"get acts | Retry | Kwargs: {model_kwargs}\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose, print_time = print_to_path)\n",
    "                if not cpu:\n",
    "                    ut.print_flush(f\"get acts | Retry | Moving to cpu\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose, print_time = print_to_path)\n",
    "                    for key in model_kwargs:\n",
    "                        try: #if not able to be moved, just not move it\n",
    "                            model_kwargs[key] = model_kwargs[key].cpu()\n",
    "                        except:\n",
    "                            continue\n",
    "                    model.to(\"cpu\")\n",
    "                    if verbose > 0: ut.print_flush(f\"get acts | Retry | cpu\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose, print_time = print_to_path)\n",
    "                    ut.print_flush(f\"get acts | Retry | Get acts\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose, print_time = print_to_path)\n",
    "                    preds = model.eval()(**model_kwargs)\n",
    "        else:\n",
    "            if verbose > 2: ut.print_flush(f\"get acts | No Retry | Get acts | model kwargs: {model_kwargs}\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose, print_time = print_to_path)\n",
    "            preds = model.eval()(**model_kwargs)\n",
    "    if acts_indices is None:\n",
    "        res = [o.stored for o in h_act]\n",
    "    else: \n",
    "        stored = [o.stored for o in h_act]\n",
    "        res = [stored[i] for i in acts_indices]\n",
    "        if len(acts_indices) == 1:\n",
    "            res = res[0]\n",
    "        del stored\n",
    "    if verbose > 0: ut.print_flush(f\"get acts | Run forward -->\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose, print_time = print_to_path)\n",
    "    if verbose > 0:ut.print_flush(f\"get acts -->\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose, print_time = print_to_path)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6807aee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_acts_moment(\n",
    "    enc_learn, \n",
    "    cpu             : bool          = False, \n",
    "    verbose         : int           = 0, \n",
    "    y               : List [ float ]= [], \n",
    "    mask                            = None, \n",
    "    padd_step       : int           = 100, \n",
    "    # Parameters for avoiding errors\n",
    "    retry           : bool          = False, \n",
    "    max_trials      : int           = 5,\n",
    "    # Activation selector (various vectors in the acts)\n",
    "    acts_indices    : List [ int ]  = [0],\n",
    "    #- Printing options for debugging\n",
    "    print_to_path   : bool          = False,\n",
    "    print_path      : str           = \"~/data/logs/logs.txt\",\n",
    "    print_mode      : str           = 'a',\n",
    "    continue_if_fail: bool          = False\n",
    "):\n",
    "    success = False \n",
    "    trial = 0\n",
    "    embs = None\n",
    "    while not success and trial < max_trials:\n",
    "        trial += 1\n",
    "        try:\n",
    "            if verbose > 0: ut.print_flush(f\"get_acts_moment | Trial {trial} | x_enc ~ {y.shape}\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose, print_time = print_to_path)\n",
    "            embs = get_acts(\n",
    "                model = enc_learn,\n",
    "                #module = enc_learn.encoder.dropout,\n",
    "                module = enc_learn.head.dropout,\n",
    "                cpu = cpu,\n",
    "                verbose = 0,\n",
    "                x_enc = y,\n",
    "                retry = retry,\n",
    "                acts_indices = acts_indices,\n",
    "                mask = mask,\n",
    "                print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, print_time = print_to_path\n",
    "            )\n",
    "            success = True\n",
    "            if verbose > 0 and acts_indices == [0] : ut.print_flush(f\"get_acts_moment | Trial {trial} | embs ~ {embs.shape}\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose, print_time = print_to_path)\n",
    "        except Exception as e:\n",
    "            if trial == max_trials - 1 : raise\n",
    "            if verbose > 0:\n",
    "                ut.print_flush(f\"get_acts_moment | Trial {trial} | About to pad X (encoder input) | exception {e} | padd step: {padd_step}\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose, print_time = print_to_path)\n",
    "                ut.print_flush(f\"get_acts_moment | Trial {trial} | y ~ {y.shape}\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose, print_time = print_to_path)\n",
    "            if \"tensor a\" in str(e) and \"tensor b\" in str(e):\n",
    "                match = re.search(r'tensor a \\((\\d+)\\) must match the size of tensor b \\((\\d+)\\)', str(e))\n",
    "                tensor_a_size = int(match.group(1))\n",
    "                tensor_b_size = int(match.group(2))\n",
    "                padd = True\n",
    "                if trial > 1: \n",
    "                    if verbose > 0: ut.print_flush(f\"------------------- Trial {trial}  -----------------\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose, print_time = print_to_path)\n",
    "                    if tensor_a_size > tensor_a_size_old:\n",
    "                        if verbose > 0:  ut.print_flush(f\"------------------- Trial {trial} | a > a_old -----------------\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose, print_time = print_to_path)\n",
    "                        padd = False\n",
    "                        y = y [ ..., : tensor_a_size - tensor_b_size]\n",
    "                        if verbose > 0: ut.print_flush(f\"------------------- Trial {trial} |a > a_old | Reduced |  y ~ {y.shape} -----------------\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose, print_time = print_to_path)\n",
    "                if padd:\n",
    "                    if verbose > 0: ut.print_flush(f\"------------------- Trial {trial} | Padd -----------------\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose, print_time = print_to_path)\n",
    "                    if tensor_a_size > tensor_b_size: \n",
    "                        if verbose > 0: ut.print_flush(f\"------------------- Trial {trial} | Padd | a > b -----------------\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose, print_time = print_to_path)\n",
    "                        padd_step = tensor_a_size - tensor_b_size\n",
    "                    y = torch.nn.functional.pad(y,(0,padd_step))\n",
    "                tensor_a_size_old = tensor_a_size\n",
    "            else:\n",
    "                if verbose > 0: ut.print_flush(\"Not the usual error. No padding, just fail\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose, print_time = print_to_path)\n",
    "                raise\n",
    "                \n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f86af7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def sure_eval_moment(\n",
    "    enc_learn, \n",
    "    cpu, \n",
    "    verbose, \n",
    "    y, \n",
    "    input_mask = None, \n",
    "    mask = None, \n",
    "    padd_step = 100, \n",
    "    retry = False, \n",
    "    max_trials = 5, \n",
    "    acts_indices = [0],\n",
    "    #- Printing options for debugging\n",
    "    print_to_path   : bool          = False,\n",
    "    print_path      : str           = \"~/data/logs/logs.txt\",\n",
    "    print_mode      : str           = 'a',\n",
    "    continue_if_fail: bool          = False\n",
    "):\n",
    "    if verbose > 0: ut.print_flush(f\"---> sure_eval_moment\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose, print_time = print_to_path)\n",
    "    device = \"cpu\" if cpu else torch.cuda.current_device()\n",
    "    y_copy = y.clone()\n",
    "    y_copy.to(\"cpu\")\n",
    "    if verbose > 0: ut.print_flush(f\"sure_eval_moment | cpu | {cpu} | device | {device}\", print_to_path = print_to_path, print_path = print_path, print_mode = 'a', verbose = verbose, print_time = print_to_path) \n",
    "    success = False \n",
    "    trial = 0\n",
    "    output = None\n",
    "    \n",
    "    while not success and trial < max_trials:\n",
    "        trial += 1\n",
    "        try:\n",
    "            if verbose > 0: ut.print_flush(f\"sure_eval_moment | Trial {trial} | x_enc ~ {y.shape}\", print_to_path = print_to_path, print_path = print_path, print_mode = 'a', verbose = verbose, print_time = print_to_path)\n",
    "            if input_mask is not None: input_mask = input_mask.to(device)\n",
    "            if mask is not None: mask = mask.to(device)\n",
    "            y = y.to(device)\n",
    "            enc_learn = enc_learn.to(device)\n",
    "            if verbose > 0: \n",
    "                ut.print_flush(f\"sure_eval_moment | Trial {trial} | device {device} | input_mask~{input_mask.shape} device: {input_mask.device if input_mask is not None else 'None'}\", print_to_path = print_to_path, print_path = print_path, print_mode = 'a', verbose = verbose, print_time = print_to_path)\n",
    "                ut.print_flush(f\"sure_eval_moment | Trial {trial} | device {device} | mask device~{mask.shape}: {mask.device if mask is not None else 'None'}\", print_to_path = print_to_path, print_path = print_path, print_mode = 'a', verbose = verbose, print_time = print_to_path)\n",
    "                ut.print_flush(f\"sure_eval_moment | Trial {trial} | device {device} | y~{y.shape} device: {y.device}\", print_to_path = print_to_path, print_path = print_path, print_mode = 'a', verbose = verbose, print_time = print_to_path)\n",
    "            output = enc_learn(x_enc = y, input_mask = input_mask, mask = mask)\n",
    "            success = True\n",
    "            if verbose > 0 and acts_indices == [0] : \n",
    "                ut.print_flush(f\"sure_eval_moment | Trial {trial} | embs ~ {embs.shape}\", print_to_path = print_to_path, print_path = print_path, print_mode = 'a', verbose = verbose, print_time = print_to_path)\n",
    "        except Exception as e:\n",
    "            if verbose > 0:\n",
    "                ut.print_flush(f\"sure_eval_moment | Trial {trial} | About to pad X (encoder input) | exception {e} | padd step: {padd_step}\", print_to_path = print_to_path, print_path = print_path, print_mode = 'a', verbose = verbose, print_time = print_to_path)\n",
    "                ut.print_flush(f\"sure_eval_moment | Trial {trial} | y ~ {y.shape}\", print_to_path = print_to_path, print_path = print_path, print_mode = 'a', verbose = verbose, print_time = print_to_path)\n",
    "                traceback.print_exc()\n",
    "            if \"tensor a\" in str(e) and \"tensor b\" in str(e) and \"dimension\" in str(e):\n",
    "                match = re.search(r'tensor a \\((\\d+)\\) must match the size of tensor b \\((\\d+)\\) at non-singleton dimension (\\d+)', str(e))\n",
    "                tensor_a_size = int(match.group(1))\n",
    "                tensor_b_size = int(match.group(2))\n",
    "                dimension = int(match.group(3))\n",
    "                match dimension:\n",
    "                    case 2 | 1:\n",
    "                        padd = True\n",
    "                        if trial > 1: \n",
    "                            if verbose > 0: ut.print_flush(f\"------------------- Trial {trial}  -----------------\", print_to_path = print_to_path, print_path = print_path, print_mode = 'a', verbose = verbose, print_time = print_to_path)\n",
    "                            if tensor_a_size > tensor_a_size_old:\n",
    "                                if verbose > 0: ut.print_flush(f\"------------------- Trial {trial} | a > a_old -----------------\", print_to_path = print_to_path, print_path = print_path, print_mode = 'a', verbose = verbose, print_time = print_to_path)\n",
    "                                padd = False\n",
    "                                y = y [ ..., : tensor_a_size - tensor_b_size]\n",
    "                                if verbose > 0: ut.print_flush(f\"------------------- Trial {trial} |a > a_old | Reduced |  y ~ {y.shape} -----------------\", print_to_path = print_to_path, print_path = print_path, print_mode = 'a', verbose = verbose, print_time = print_to_path)\n",
    "                        if padd:\n",
    "                            if verbose > 0: ut.print_flush(f\"------------------- Trial {trial} | Padd -----------------\", print_to_path = print_to_path, print_path = print_path, print_mode = 'a', verbose = verbose, print_time = print_to_path)\n",
    "                            if tensor_a_size > tensor_b_size: \n",
    "                                if verbose > 0: ut.print_flush(f\"------------------- Trial {trial} | Padd | a > b -----------------\", print_to_path = print_to_path, print_path = print_path, print_mode = 'a', verbose = verbose, print_time = print_to_path)\n",
    "                                padd_step = tensor_a_size - tensor_b_size\n",
    "                            y = torch.nn.functional.pad(y,(0,padd_step))\n",
    "                        tensor_a_size_old = tensor_a_size\n",
    "                    #case 1: \n",
    "                    #    if verbose > 0:\n",
    "                    #        ut.print_flush(f\"sure_eval_moment | Trial {trial} | Error dimension 0 | mask ~ {mask.shape} | mask_input ~ {input_mask.shape} | batch ~ {y.shape}\")\n",
    "                    #        if mask.shape[1] < y.shape[2]: mask = torch.nn.functional.pad(mask,(0,y.shape[2]-mask.shape[1]))\n",
    "                    #        if input_mask.shape[2] < y.shape[2]: mask = torch.nn.functional.pad(input_mask,(0,y.shape[2]-input_mask.shape[2]))\n",
    "\n",
    "                    case 0:\n",
    "                        if verbose > 0: \n",
    "                            ut.print_flush(f\"sure_eval_moment | Trial {trial} | Error dimension 0 | mask ~ {mask.shape} | mask_input ~ {input_mask.shape} | batch ~ {y.shape}\", print_to_path = print_to_path, print_path = print_path, print_mode = 'a', verbose = verbose, print_time = print_to_path)                    \n",
    "                        if mask.shape[0] > y.shape[0]:\n",
    "                            mask = mask[:y.shape[0]]\n",
    "                        if input_mask.shape[0] > y.shape[0]:\n",
    "                            input_mask = input_mask[:y.shape[0]]\n",
    "                        \n",
    "                        if mask.shape[0] < y.shape[0]:\n",
    "                            extra_rows_shape = (-mask.shape[0]+y.shape[0],mask.shape[1])\n",
    "                            if verbose > 0: ut.print_flush(f\"sure_eval_moment | Trial {trial} | Mask lower than batch | rows to add: {extra_rows_shape }\", print_to_path = print_to_path, print_path = print_path, print_mode = 'a', verbose = verbose, print_time = print_to_path)\n",
    "                            extra_rows = torch.zeros(extra_rows_shape, dtype = torch.float32)\n",
    "                            mask = torch.cat((mask, extra_rows), dim=0)\n",
    "                        if input_mask.shape[0] < y.shape[0]:\n",
    "                            extra_rows_shape = (-input_mask.shape[0]+y.shape[0],y.shape[1], y.shape[2])\n",
    "                            if verbose > 0: ut.print_flush(f\"sure_eval_moment | Trial {trial} | Mask lower than batch | rows to add: {extra_rows_shape }\", print_to_path = print_to_path, print_path = print_path, print_mode = 'a', verbose = verbose, print_time = print_to_path)\n",
    "                            extra_rows = torch.zeros(extra_rows_shape, dtype = torch.float32)\n",
    "                            input_mask = torch.cat((input_mask, extra_rows), dim=0)\n",
    "            else:\n",
    "                if verbose > 0: \n",
    "                    ut.print_flush(\"Not the usual error. No padding, just fail\", print_to_path = print_to_path, print_path = print_path, print_mode = 'a', verbose = verbose, print_time = print_to_path)\n",
    "                if not continue_if_fail: raise\n",
    "        #if verbose > 0: ut.print_flush(f\"sure_eval_moment | output {output.__class__} | enc_learn {enc_learn.__class__} -->\", print_to_path = print_to_path, print_path = print_path, print_mode = 'a', verbose = verbose, print_time = print_to_path)\n",
    "        if verbose > 0: ut.print_flush(f\"sure_eval_moment | output {output.__class__} -->\", print_to_path = print_to_path, print_path = print_path, print_mode = 'a', verbose = verbose, print_time = print_to_path)\n",
    "    y = y_copy\n",
    "    if not cpu: y.to(\"cuda\")\n",
    "    \n",
    "    return output, enc_learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268a268f-8b4e-4432-b203-79263a247c4c",
   "metadata": {},
   "source": [
    "## Getting the embeddings (activations) from the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a992422",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_enc_embs_ensure_batch_size_(\n",
    "    dls        : TSDataLoaders,\n",
    "    batch_size : int = None,\n",
    "    verbose    : int = 0\n",
    ") -> None:\n",
    "    if batch_size is None:\n",
    "        if verbose > 1: \n",
    "            ut.print_flush(f\"[ Get Encoder Embeddings Ensure Batch Size ] No batch size proposed\", verbose = verbose)\n",
    "        if dls.bs == 0: \n",
    "            if verbose > 1: \n",
    "                ut.print_flush(f\"[ Get Encoder Embeddings Ensure Batch Size ] Using value 64 as 0 is not a valid value.\", verbose = verbose)\n",
    "            enc_learn.dls.bs = 64\n",
    "        elif verbose > 1: \n",
    "            ut.print_flush(f\"[ Get Encoder Embeddings Ensure Batch Size ] Using the original value: {dls.bs}\", verbose = verbose)\n",
    "    else:\n",
    "        dls.bs = batch_size\n",
    "        if verbose > 1: \n",
    "            ut.print_flush(f\"[ Get Encoder Embeddings Ensure Batch Size ] Batch size proposed. Using {dls.bs}\", verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65c66ae6-3178-49dc-bd16-64c082012e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_enc_embs_MVP(\n",
    "    X               : List [ List [ List [ float ] ] ], \n",
    "    enc_learn       : Learner, \n",
    "    module          : str  = None, \n",
    "    cpu             : bool = False, \n",
    "    average_seq_dim : bool = True, \n",
    "    to_numpy        : bool = True,\n",
    "    batch_size      : int  = None,\n",
    "    verbose         : int  = 0\n",
    "):\n",
    "    \"\"\"\n",
    "        Get the embeddings of X from an encoder, passed in `enc_learn as a fastai\n",
    "        learner. By default, the embeddings are obtained from the last layer\n",
    "        before the model head, although any layer can be passed to `model`.\n",
    "        Input\n",
    "        - `cpu`: Whether to do the model inference in cpu of gpu (GPU recommended)\n",
    "        - `average_seq_dim`: Whether to aggregate the embeddings in the sequence dimensions\n",
    "        - `to_numpy`: Whether to return the result as a numpy array (if false returns a tensor)\n",
    "        - `batch_size`: force data loader to use the input batch size\n",
    "        - `verbose`: print flag. More big, more information.\n",
    "    \"\"\"\n",
    "    \n",
    "    if cpu:\n",
    "        if verbose > 0: ut.print_flush(\"[ Get Encoder Embeddings ] CPU\")\n",
    "        enc_learn.dls.cpu()\n",
    "        enc_learn.cpu()\n",
    "    else:\n",
    "        if verbose > 0: ut.print_flush(\"[ Get Encoder Embeddings ] --> GPU\")\n",
    "        if verbose > 1: ut.print_flush(\"[ Get Encoder Embeddings ] GPU | Ensure empty cache\")\n",
    "        torch.cuda.empty_cache()\n",
    "        if verbose > 1: ut.print_flush(\"[ Get Encoder Embeddings ] GPU | Move & exec into CUDA\")\n",
    "        enc_learn.dls.cuda()\n",
    "        enc_learn.cuda()\n",
    "        if torch.cuda.is_available():\n",
    "            if verbose > 1: \n",
    "                ut.print_flush(\"[ Get Encoder Embeddings ] GPU | CUDA is available\")\n",
    "                ut.print_flush(f\"[ Get Encoder Embeddings ] GPU | CUDA is available | current device id {torch.cuda.current_device()}\")\n",
    "                ut.print_flush(f\"[ Get Encoder Embeddings ] GPU | CUDA is available | current device name {torch.cuda.get_device_name(torch.cuda.current_device())}\")            \n",
    "        else:\n",
    "            if verbose > 1: ut.print_flush(\"[ Get Encoder Embeddings ] GPU | CUDA is not available\")\n",
    "        if verbose > 0: ut.print_flush(\"[ Get Encoder Embeddings ] GPU -->\")\n",
    "\n",
    "    #if verbose > 0: ut.print_flush(\"[ Get Encoder Embeddings ] Ensure the correct batch size\")\n",
    "    #get_enc_embs_ensure_batch_size_(enc_learn.dls, batch_size, verbose)\n",
    "    \n",
    "    if verbose > 0: ut.print_flush(\"[ Get Encoder Embeddings ] Set dataloader from X (enc_learn does not contain dls)\")\n",
    "    aux_dl = enc_learn.dls.valid.new_dl(X=X)\n",
    "    get_enc_embs_ensure_batch_size_(aux_dl, batch_size, verbose)\n",
    "    if verbose > 0: ut.print_flush(\"[ Get Encoder Embeddings ] Get module\")\n",
    "    module = nested_attr(enc_learn.model,ENCODER_EMBS_MODULE_NAME[type(enc_learn.model)]) if module is None else module\n",
    "    \n",
    "    if verbose > 0: ut.print_flush(\"[ Get Encoder Embeddings ] get_acts_and_grads \")\n",
    "    if verbose > 1: ut.print_flush(f\"[ Get Encoder Embeddings ] get_acts_and_grads bs = {aux_dl.bs}\")\n",
    "    \n",
    "    embs = [\n",
    "        get_acts_and_grads(\n",
    "            model   = enc_learn.model,\n",
    "            modules = module,\n",
    "            x       = xb[0], \n",
    "            cpu     = cpu\n",
    "        )[0] \n",
    "        for xb in aux_dl\n",
    "    ]\n",
    "    if verbose > 0: ut.print_flush(\"[ Get Encoder Embeddings ] get_acts_and_grads | --> Concat\")\n",
    "    if not cpu:\n",
    "        if verbose > 1: ut.print_flush(\"[ Get Encoder Embeddings ] get_acts_and_grads | Concat | Check neccesary & free memory\")\n",
    "        total_emb_size = sum([emb.element_size() * emb.nelement() for emb in embs])\n",
    "        free_memory = torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()\n",
    "        if (total_emb_size < free_memory):\n",
    "            if verbose > 1: ut.print_flush(\"[ Get Encoder Embeddings ] get_acts_and_grads | Concat | Check neccesary & free memory | Fits in GPU -> Computing in GPU\")\n",
    "            embs=[emb.cuda() for emb in embs]\n",
    "        else:\n",
    "            if verbose > 1: ut.print_flush(\"[ Get Encoder Embeddings ] get_acts_and_grads | Concat | Check neccesary & free memory | Does not fit in GPU -> Computing in CPU\")\n",
    "            embs=[emb.cpu() for emb in embs]\n",
    "    if verbose > 1: ut.print_flush(\"[ Get Encoder Embeddings ] get_acts_and_grads | Concat | to_concat\")\n",
    "    embs = to_concat(embs)\n",
    "    if verbose > 0: ut.print_flush(\"[ Get Encoder Embeddings ] get_acts_and_grads | Concat -->\")\n",
    "    \n",
    "    if verbose > 0: ut.print_flush(\"[ Get Encoder Embeddings ] Reduce to 2 dimensions.\")\n",
    "    if embs.ndim == 3 and average_seq_dim: embs = embs.mean(axis=2)\n",
    "    if verbose > 0: ut.print_flush(\"[ Get Encoder Embeddings ] Ensure CPU saving & numpy format\")\n",
    "    if to_numpy: embs = embs.numpy() if cpu else embs.cpu().numpy()\n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51ad3b28-43c0-4df3-be57-3eecb76b17d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_enc_embs_MVP_set_stride_set_batch_size(\n",
    "    X                  : List [ List [ List [ float ] ] ], \n",
    "    enc_learn          : Learner, \n",
    "    stride             : int, \n",
    "    batch_size         : int, \n",
    "    module             : str  = None, \n",
    "    cpu                : bool = False, \n",
    "    average_seq_dim    : bool = True, \n",
    "    to_numpy           : bool = True, \n",
    "    verbose            : int  = 0, \n",
    "    time_flag          : bool = False, \n",
    "    chunk_size         : int  = 0, \n",
    "    check_memory_usage : bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "        Get the embeddings of X from an encoder, passed in `enc_learn as a fastai\n",
    "        learner. By default, the embeddings are obtained from the last layer\n",
    "        before the model head, although any layer can be passed to `model`.\n",
    "        Input\n",
    "        - `X`: encoder input\n",
    "        - `enc_learn`: trained encoder\n",
    "        - `stride`: stride used for the training. Neccesary for adjusting the encoder input\n",
    "        - `batch_size`: value to force the dataloader to use.\n",
    "        - `module`: for geting the embeddings of an specific layer.\n",
    "        - `cpu`: Whether to do the model inference in cpu of gpu (GPU recommended)\n",
    "        - `average_seq_dim`: Whether to aggregate the embeddings in the sequence dimensions\n",
    "        - `to_numpy`: Whether to return the result as a numpy array (if false returns a tensor)\n",
    "        - `verbose`: For printing messages. More big, more messages.\n",
    "        - `time_flag`: To take note of the execution time required by this function\n",
    "        - `chunk_size`: For spliting the embedings reading in batches of `chunk_size` size.\n",
    "        - `check_memory_usage`: For showing messages of the current state of the memory.\n",
    "    \"\"\"\n",
    "    if time_flag:\n",
    "        t_start = time.time()\n",
    "    if verbose > 0:\n",
    "        ut.print_flush(\"--> get_enc_embs_MVP_set_stride_set_batch_size\", verbose = verbose)\n",
    "    if check_memory_usage: gpu_memory_status()\n",
    "    X = X[::stride]\n",
    "    enc_learn.dls.bs = batch_size \n",
    "\n",
    "    get_enc_embs_ensure_batch_size_(enc_learn.dls, batch_size, verbose)\n",
    "    \n",
    "    if verbose > 0: ut.print_flush(f\"get_enc_embs_MVP_set_stride_set_batch_size | Check CUDA | X ~ {X.shape[0]}\", verbose = verbose)\n",
    "    if cpu:\n",
    "        if verbose > 0: ut.print_flush(\"get_enc_embs_MVP_set_stride_set_batch_size | Get enc embs CPU\")\n",
    "        enc_learn.dls.cpu()\n",
    "        enc_learn.cpu()\n",
    "    else:\n",
    "        if torch.cuda.is_available():\n",
    "            if verbose > 0: \n",
    "                ut.print_flush(f\"get_enc_embs_MVP_set_stride_set_batch_size | CUDA device id: {torch.cuda.current_device()}\", verbose = verbose)\n",
    "                ut.print_flush(f\"get_enc_embs_MVP_set_stride_set_batch_size | CUDA device name: {torch.cuda.get_device_name(torch.cuda.current_device())}\", verbose = verbose)\n",
    "                ut.print_flush(f\"get_enc_embs_MVP_set_stride_set_batch_size | Ensure empty cache & move 2 GPU\", verbose = verbose)\n",
    "            torch.cuda.empty_cache()\n",
    "            enc_learn.dls.cuda()\n",
    "            enc_learn.cuda()\n",
    "        else:\n",
    "            if verbose > 0: ut.print_flush(\"get_enc_embs_MVP_set_stride_set_batch_size | No cuda available. Set CPU = true\")\n",
    "            cpu = True\n",
    "            \n",
    "    get_enc_embs_ensure_batch_size_(enc_learn.dls, batch_size, verbose)\n",
    "\n",
    "    if verbose > 0: ut.print_flush(\"get_enc_embs_MVP_set_stride_set_batch_size | Set dataset from X (enc_learn does not contain dls)\", verbose = verbose)\n",
    "    aux_dl = enc_learn.dls.valid.new_dl(X=X)\n",
    "    aux_dl.bs = enc_learn.dls.bs if enc_learn.dls.bs>0 else 64\n",
    "    if verbose > 0: ut.print_flush(\"get_enc_embs_MVP_set_stride_set_batch_size | Get module\", verbose = verbose)\n",
    "    module = nested_attr(enc_learn.model,ENCODER_EMBS_MODULE_NAME[type(enc_learn.model)]) if module is None else module\n",
    "    \n",
    "    if verbose > 0: \n",
    "        #ut.print_flush(\"get_enc_embs_MVP_set_stride_set_batch_size | Get acts and grads | module \", module)\n",
    "        ut.print_flush(f\"get_enc_embs_MVP_set_stride_set_batch_size | Get acts and grads | aux_dl len {len(aux_dl)}\", verbose = verbose)\n",
    "        ut.print_flush(f\"get_enc_embs_MVP_set_stride_set_batch_size | Get acts and grads | aux_dl.batch_len {len(next(iter(aux_dl)))}\", verbose = verbose)\n",
    "        ut.print_flush(f\"get_enc_embs_MVP_set_stride_set_batch_size | Get acts and grads | aux_dl.bs {aux_dl.bs}\", verbose = verbose)\n",
    "        if (not cpu):\n",
    "            total = torch.cuda.get_device_properties(device).total_memory\n",
    "            used = torch.cuda.memory_allocated(torch.cuda.current_device())\n",
    "            reserved = torch.cuda.memory_reserved(torch.cuda.current_device())\n",
    "            ut.print_flush(f\"get_enc_embs_MVP_set_stride_set_batch_size | Get acts and grads | total_mem {total}\", verbose = verbose)\n",
    "            ut.print_flush(f\"get_enc_embs_MVP_set_stride_set_batch_size | Get acts and grads | used_mem {used}\", verbose = verbose)\n",
    "            ut.print_flush(f\"get_enc_embs_MVP_set_stride_set_batch_size | Get acts and grads | reserved_mem {reserved}\" ,verbose = verbose)\n",
    "            ut.print_flush(f\"get_enc_embs_MVP_set_stride_set_batch_size | Get acts and grads | available_mem {total-reserved}\", verbose = verbose)\n",
    "            sys.stdout.flush()\n",
    "                                              \n",
    "    if (cpu or ( chunk_size == 0 )):\n",
    "        embs = [\n",
    "            get_acts_and_grads(\n",
    "                model=enc_learn.model,\n",
    "                modules=module, \n",
    "                x=xb[0], \n",
    "                cpu=cpu\n",
    "            )[0] \n",
    "            for xb in aux_dl\n",
    "        ]\n",
    "        if not cpu: embs=[emb.cpu() for emb in embs]\n",
    "    else:\n",
    "        embs = []\n",
    "        total_chunks=max(1,round(len(X)/chunk_size))\n",
    "        if verbose > 0: ut.print_flush(f\"get_enc_embs_MVP_set_stride_set_batch_size | Get acts and grads | aux_dl len | {str(len(X))}  chunk size: {str(chunk_size) } => { str(total_chunks) }  chunks\", verbose = verbose)\n",
    "        for i in range(0, total_chunks):\n",
    "            if verbose > 0: \n",
    "                ut.print_flush(f\"get_enc_embs_MVP_set_stride_set_batch_size | Get acts and grads | Chunk [ {str(i)}/{str(total_chunks)}] => {str(round(i*100/total_chunks))}%\", verbose = verbose)\n",
    "                sys.stdout.flush()\n",
    "            chunk = [batch for (n, batch) in enumerate(aux_dl) if (chunk_size*i <= n  and chunk_size*(i+1) > n) ]\n",
    "            chunk_embs = [\n",
    "                get_acts_and_grads(\n",
    "                    model=enc_learn.model,\n",
    "                    modules=module,\n",
    "                    x=xb[0], \n",
    "                    cpu=cpu\n",
    "                )[0]\n",
    "                for xb in chunk\n",
    "            ]\n",
    "            # Mueve los embeddings del bloque a la CPU\n",
    "            chunk_embs = [emb.cpu() for emb in chunk_embs]\n",
    "            embs.extend(chunk_embs)\n",
    "            torch.cuda.empty_cache()\n",
    "        if verbose > 0: \n",
    "            ut.print_flush(\"get_enc_embs_MVP_set_stride_set_batch_size | Get acts and grads | 100%\", verbose = verbose)\n",
    "            sys.stdout.flush()\n",
    "    \n",
    "    if verbose > 0: ut.print_flush(\"get_enc_embs_MVP_set_stride_set_batch_size | concat embeddings\", verbose = verbose)\n",
    "    \n",
    "    embs = to_concat(embs)\n",
    "    \n",
    "    if verbose > 0: ut.print_flush(\"get_enc_embs_MVP_set_stride_set_batch_size | Reduce\", verbose = verbose)\n",
    "    \n",
    "    if embs.ndim == 3 and average_seq_dim: embs = embs.mean(axis=2)\n",
    "    \n",
    "    if verbose > 0: ut.print_flush(\"get_enc_embs_MVP_set_stride_set_batch_size | Convert to numpy\", verbose = verbose)\n",
    "    \n",
    "    if to_numpy: \n",
    "        if cpu or chunk_size > 0:\n",
    "            embs = embs.numpy() \n",
    "        else: \n",
    "            embs = embs.cpu().numpy()\n",
    "            torch.cuda.empty_cache()\n",
    "    if time_flag:\n",
    "        t = time.time()-t_start\n",
    "        if verbose > 0:\n",
    "            ut.print_flush(\"get_enc_embs_MVP_set_stride_set_batch_size \" + str(t) + \" seconds -->\", verbose = verbose)\n",
    "        else:\n",
    "            ut.print_flush(\"get_enc_embs_MVP_set_stride_set_batch_size \" + str(t) + \" seconds\", verbose = verbose)\n",
    "    if check_memory_usage: gpu_memory_status()\n",
    "    if verbose > 0: \n",
    "        ut.print_flush(\"get_enc_embs_MVP_set_stride_set_batch_size -->\", verbose = verbose)\n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a17863e-f5c1-4c4c-8023-37d197598ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_enc_embs_moment(\n",
    "    X               : List [ List [ List [ float ] ] ], \n",
    "    enc_learn       : Learner, \n",
    "    cpu             : bool = False, \n",
    "    to_numpy        : bool = True,\n",
    "    verbose         : int  = 0,\n",
    "    average_seq_dim : bool = True\n",
    "):\n",
    "    if verbose > 0: \n",
    "        ut.print_flush(\"--> get_enc_embs_moment\", verbose = verbose)\n",
    "    # Move tensor and model to GPU\n",
    "    if cpu or not torch.cuda.is_available():\n",
    "        if verbose > 0: \n",
    "            ut.print_flush(\"get_enc_embs_moment | Using CPU (maybe no cuda available)\", verbose = verbose)\n",
    "        cpu = True\n",
    "        enc_learn.cpu()\n",
    "    else:\n",
    "        if verbose > 0: \n",
    "            ut.print_flush(\"get_enc_embs_moment | Using CUDA\", verbose = verbose)\n",
    "        enc_learn.to(\"cuda\")\n",
    "    if verbose > 0: ut.print_flush(\"get_enc_embs_moment | Convert y\", verbose = verbose)\n",
    "    enc_learn.eval()\n",
    "    if cpu:\n",
    "        y = torch.from_numpy(X).cpu().float()\n",
    "    else:\n",
    "        y = torch.from_numpy(X).to(\"cuda\").float()\n",
    "    # Get output\n",
    "    with torch.no_grad():\n",
    "        if verbose > 0: \n",
    "            ut.print_flush(\"get_enc_embs_moment | Get outputs\", verbose = verbose)\n",
    "        outputs = enc_learn(y)\n",
    "        if verbose > 0:\n",
    "            ut.print_flush(f\"get_enc_embs_moment | Final shape: X ~ {y.shape}\", verbose = verbose)\n",
    "                \n",
    "    #| move tensors and models back to CPU\n",
    "    if not cpu:\n",
    "        y = y.detach().cpu().numpy()\n",
    "    if verbose > 0: \n",
    "        ut.print_flush(\"get_enc_embs_moment | Get Embeddings\", verbose = verbose)\n",
    "    embeddings = outputs.embeddings.detach().cpu()\n",
    "    if average_seq_dim: \n",
    "        embeddings = embeddings.mean(dim = 1)\n",
    "    if to_numpy:\n",
    "        embeddings = embeddings.cpu().numpy()\n",
    "    if verbose > 0: \n",
    "        ut.print_flush(\"get_enc_embs_moment -->\", verbose = verbose)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aac3e43c-fd7f-4022-8d1a-be1b47e580de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_enc_embs_moment_reconstruction(\n",
    "    X               : List [ List [ List [ float ] ] ], \n",
    "    enc_learn       : Learner, \n",
    "    cpu             : bool          = False, \n",
    "    to_numpy        : bool          = True,\n",
    "    verbose         : int           = 0,\n",
    "    average_seq_dim : bool          = True,\n",
    "    padd_step       : int           = 2,\n",
    "    #- Printing options for debugging\n",
    "    print_to_path   : bool          = False,\n",
    "    print_path      : str           = \"~/data/logs/logs.txt\",\n",
    "    print_mode      : str           = 'a',\n",
    "    continue_if_fail: bool          = False\n",
    "):\n",
    "    \"\"\"\n",
    "    For reconstruction sometimes mask get invalid values\n",
    "    To avoid them, the last dimension (sequence length) is padded with 0's until the error is skippedd\n",
    "    It should only get one iteration as it seems to be some MOMENT internal configuration for patches.\n",
    "    \"\"\"\n",
    "    if cpu:\n",
    "        enc_learn.cpu()\n",
    "        y = torch.from_numpy(X).cpu().float()\n",
    "    else:\n",
    "        enc_learn.to(\"cuda\")\n",
    "        y = torch.from_numpy(X).to(\"cuda\").float()\n",
    "    embs = get_acts_moment(\n",
    "        enc_learn       = enc_learn, \n",
    "        cpu             = cpu, \n",
    "        verbose         = verbose, \n",
    "        y               = y, \n",
    "        mask            = None,\n",
    "        padd_step       = padd_step,\n",
    "        retry           = False ,\n",
    "        max_trials      = 5,\n",
    "        print_to_path   = print_to_path, print_path = print_path, print_mode = print_mode\n",
    "    )\n",
    "    if average_seq_dim: \n",
    "        embs = embs.mean(dim = 1).mean(dim = 1)\n",
    "    if to_numpy:\n",
    "        embs = embs.cpu().numpy()\n",
    "    return embs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51f46bd",
   "metadata": {},
   "source": [
    "---> TODO: averiguar de qué module salen realmente los embeddings y usar el get_acts_and_grads como en MVP <---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2ccce71-3238-44b0-90f5-7efaf62c1643",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch.profiler as profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73adfca7-6b31-4030-ab06-d24e5bd07821",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def watch_gpu(func, **kwargs):\n",
    "    \"\"\"\n",
    "    Wrapper to execute GPU profiler\n",
    "    Parameters: \n",
    "    - func: function to monitor\n",
    "    - kwargs: func parameters\n",
    "    Returns:\n",
    "    - result of /func/.\n",
    "    \"\"\"\n",
    "    with profiler.profile(\n",
    "        activities=[profiler.ProfilerActivity.CPU, profiler.ProfilerActivity.CUDA],\n",
    "        schedule=profiler.schedule(wait=1, warmup=1, active=3, repeat=2),  # Configuración de ciclos\n",
    "        on_trace_ready=profiler.tensorboard_trace_handler('./log_dir'),  # Guarda los resultados en un archivo para visualización\n",
    "        record_shapes=True,  # Registra la forma de los tensores\n",
    "        profile_memory=True,  # Perfil de memoria\n",
    "        with_stack=True  # Incluye la información de la pila\n",
    "    ) as prof:\n",
    "        # Ejecuta la función dentro del perfilador\n",
    "        result = func(**kwargs)\n",
    "    \n",
    "    # Mostrar el uso de la GPU durante y después de la ejecución\n",
    "    ut.print_flush(prof.key_averages().table(sort_by=\"cuda_memory_usage\", row_limit=10))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d9a70cd5-5aa5-44c2-b7cf-4d290e9349ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_enc_embs_moirai(\n",
    "    enc_input       : List [ List [ List [ Float ] ] ], \n",
    "    enc_model       : moirai.MoiraiModule, \n",
    "    cpu             : False,\n",
    "    average_seq_dim : bool = True, \n",
    "    verbose         : int  = 0,\n",
    "    to_numpy        : bool = True,\n",
    "    patch_size      : int  = 8,\n",
    "    time            : bool = False\n",
    "):\n",
    "    mssg = ut.Mssg()\n",
    "    if time: \n",
    "        timer = ut.Time(mssg = mssg)\n",
    "        timer.start()\n",
    "    if verbose > 0: \n",
    "        ut.print_flush(\"--> get_enc_embs_moirai\", verbose = verbose)\n",
    "    # Move tensor and model to GPU\n",
    "    past_target = einops.rearrange(\n",
    "        torch.as_tensor(enc_input, dtype = torch.float32),\n",
    "        \"n_windows n_vars window_size -> n_windows window_size n_vars\"\n",
    "    )\n",
    "    if cpu or not torch.cuda.is_available():\n",
    "        if verbose > 0: ut.print_flush(\"get_enc_embs_moirai | Using CPU (maybe no cuda available)\", verbose = verbose)\n",
    "        cpu = True\n",
    "        enc_model.cpu()\n",
    "        past_target.cpu()\n",
    "    else:\n",
    "        if verbose > 0: ut.print_flush(\"get_enc_embs_moirai | Using CUDA\", verbose = verbose)\n",
    "        enc_model.to(\"cuda\")\n",
    "        past_target.to(\"cuda\")\n",
    "        \n",
    "    if verbose > 0: ut.print_flush(\"get_enc_embs_moirai | Get Outputs\", verbose = verbose)\n",
    "\n",
    "    \n",
    "    past_observed_target = torch.ones_like(past_target, dtype=torch.bool)\n",
    "    past_is_pad = torch.zeros_like(past_target, dtype=torch.bool)[...,:,-1] # Kill last dimension\n",
    "\n",
    "    if (verbose > 1):\n",
    "        ut.print_flush(f\"--> get_enc_embs_moirai | past_target ~ {past_target.shape}\")\n",
    "        ut.print_flush(f\"--> get_enc_embs_moirai | past_observed_target ~ {past_observed_target.shape}\")\n",
    "        ut.print_flush(f\"--> get_enc_embs_moirai | past_is_pad ~ {past_is_pad.shape}\")\n",
    "        ut.print_flush(f\"--> get_enc_embs_moirai | Auxiliar model\")\n",
    "        ut.print_flush(f\"--> get_enc_embs_moirai | Auxiliar model | Before Memory:\")\n",
    "        gpu_memory_status()\n",
    "    \n",
    "    # Auxiliar model for conversions just to ensure correct sizes\n",
    "    #not neccesary, is the same module initially downloaded...\n",
    "    #module = moirai.MoiraiModule.from_pretrained(f\"Salesforce/moirai-1.1-R-small\")\n",
    "    \n",
    "    forecast_model =  moirai_forecast.MoiraiForecast(\n",
    "        module=enc_model,\n",
    "        prediction_length=past_target.shape[2], #random, just for getting the model\n",
    "        context_length=past_target.shape[1],\n",
    "        patch_size=patch_size,\n",
    "        num_samples=100, #Random, is the number of forecasting, not interesting for us\n",
    "        target_dim=past_target.shape[2],\n",
    "        feat_dynamic_real_dim=0,\n",
    "        past_feat_dynamic_real_dim=0,\n",
    "    )\n",
    "    \n",
    "    if verbose > 0:\n",
    "        ut.print_flush(f\"--> get_enc_embs_moirai | Auxiliar model | After Memory:\")\n",
    "        gpu_memory_status()\n",
    "        ut.print_flush(f\"--> get_enc_embs_moirai | Convert sizes\")\n",
    "    (\n",
    "    target,\n",
    "    observed_mask,\n",
    "    sample_id,\n",
    "    time_id,\n",
    "    variate_id,\n",
    "    prediction_mask,\n",
    "    ) = forecast_model._convert(\n",
    "        patch_size,\n",
    "        past_target,\n",
    "        past_observed_target,\n",
    "        past_is_pad\n",
    "    )\n",
    "    if verbose > 1:\n",
    "        ut.print_flush(f\"get_enc_embs_moirai | target ~ {target.shape}\")\n",
    "        ut.print_flush(f\"get_enc_embs_moirai | observed_mask ~ {observed_mask.shape}\")\n",
    "        ut.print_flush(f\"get_enc_embs_moirai | sample_id ~ {sample_id.shape}\")\n",
    "        ut.print_flush(f\"get_enc_embs_moirai | time_id ~ {time_id.shape}\")\n",
    "        ut.print_flush(f\"get_enc_embs_moirai | variate_id ~ {variate_id.shape}\")\n",
    "        ut.print_flush(f\"get_enc_embs_moirai | prediction_mask ~ {prediction_mask.shape}\")\n",
    "        gpu_memory_status()\n",
    "    forecast_model = None\n",
    "    torch.cuda.empty_cache()\n",
    "    if verbose > 0:\n",
    "        ut.print_flush(f\"--> get_enc_embs_moirai | Delete Auxiliar model | After Memory:\")\n",
    "        gpu_memory_status()\n",
    "    \n",
    "    model_kwargs={\n",
    "        'target': target, \n",
    "        'observed_mask': observed_mask,\n",
    "        'sample_id': sample_id,\n",
    "        'time_id': time_id,\n",
    "        'variate_id': variate_id,\n",
    "        'prediction_mask': prediction_mask,\n",
    "        'patch_size': torch.ones_like(sample_id, dtype = torch.float32)*patch_size\n",
    "    } \n",
    "    if verbose > 0: \n",
    "        ut.print_flush(f\"get_enc_embs_moirai | About to get activations\")\n",
    "    acts = get_acts(\n",
    "        model  = enc_model, \n",
    "        module = enc_model.encoder.norm, \n",
    "        cpu    = cpu,\n",
    "        verbose = verbose,\n",
    "        retry = True,\n",
    "        acts_indices = [0],\n",
    "        **model_kwargs #Parameters of the model\n",
    "    )\n",
    "    \n",
    "    embs = acts\n",
    "    acts = None\n",
    "    if average_seq_dim :\n",
    "        if verbose > 0: \n",
    "            ut.print_flush(f\"get_enc_embs_moirai | About to reduce activations\", verbose = verbose)\n",
    "        embs = embs.mean(dim = 1)\n",
    "    \n",
    "    if not cpu:\n",
    "        #ut.print_flush(f\"get_enc_embs_moirai | enc_input to cpu\")\n",
    "        #enc_input.cpu()\n",
    "        ut.print_flush(f\"get_enc_embs_moirai | enc_model to cpu\", verbose = verbose)\n",
    "        enc_model.cpu()\n",
    "        ut.print_flush(f\"get_enc_embs_moirai | torch cuda empty cache\", verbose = verbose)\n",
    "        torch.cuda.empty_cache()\n",
    "    if to_numpy: \n",
    "        if cpu > 0:\n",
    "            embs = embs.numpy() \n",
    "        else: \n",
    "            embs = embs.cpu().numpy()\n",
    "            torch.cuda.empty_cache()\n",
    "    if verbose > 0: \n",
    "        ut.print_flush(f\"get_enc_embs_moirai | embs ~ {embs.shape}\", verbose = verbose)\n",
    "        ut.print_flush(\"get_enc_embs_moirai -->\", verbose = verbose)\n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86667613-82af-438f-b66c-c4c5e894634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def get_enc_embs(\n",
    "    X               , \n",
    "    enc_learn       : Learner, \n",
    "    module          : str  = None, \n",
    "    cpu             : bool = False, \n",
    "    average_seq_dim : bool = True, \n",
    "    to_numpy        : bool = True,\n",
    "    verbose         : int  = 0,\n",
    "    **kwargs        \n",
    "):\n",
    "    embs = None\n",
    "    enc_learn_class = str(enc_learn.__class__)[8:-2]\n",
    "    match enc_learn_class:\n",
    "        case \"momentfm.models.moment.MOMENTPipeline\":\n",
    "            match enc_learn.task_name:\n",
    "                case \"embedding\":\n",
    "                    embs = get_enc_embs_moment(X, enc_learn, cpu, to_numpy, verbose, average_seq_dim, **kwargs)\n",
    "                case \"reconstruction\":\n",
    "                    embs = get_enc_embs_moment_reconstruction(X, enc_learn, cpu, to_numpy, verbose, average_seq_dim, **kwargs)\n",
    "                case _:\n",
    "                    ut.print_flush(f\"Model embeddings for moment-{enc_learn.task_name} is not yet implemented.\", verbose = verbose)\n",
    "        case \"fastai.learner.Learner\":\n",
    "            embs = get_enc_embs_MVP_set_stride_set_batch_size(X, enc_learn, stride, batch_size, module, cpu, average_seq_dim, to_numpy, verbose, False, 0, False)\n",
    "        case \"uni2ts.model.moirai.module.MoiraiModule\":\n",
    "            embs = get_enc_embs_moirai(\n",
    "                enc_input  = X, \n",
    "                enc_model  = enc_learn,\n",
    "                cpu        = cpu, \n",
    "                average_seq_dim = average_seq_dim,\n",
    "                verbose    = verbose,\n",
    "                **kwargs\n",
    "            )\n",
    "        case _:\n",
    "            ut.print_flush(f\"Model embeddings implementation is not yet implemented for {enc_learn_class}.\", verbose = verbose)\n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "13ccce1d-aca2-44c1-8b20-4339628e3919",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_enc_embs_set_stride_set_batch_size(\n",
    "    X                  : List [ List [ List [ float ] ] ], \n",
    "    enc_learn          : Learner, \n",
    "    stride             : int, \n",
    "    batch_size         : int, \n",
    "    module             : str  = None, \n",
    "    cpu                : bool = False, \n",
    "    average_seq_dim    : bool = True, \n",
    "    to_numpy           : bool = True, \n",
    "    verbose            : int  = 0, \n",
    "    time_flag          : bool = False, \n",
    "    chunk_size         : int  = 0, \n",
    "    check_memory_usage : bool = False,\n",
    "    **kwargs\n",
    "):\n",
    "    ut.print_flush(\"--> get_enc_embs_set_stride_set_batch_size\", verbose = verbose)\n",
    "    embs = None\n",
    "    enc_learn_class = str(enc_learn.__class__)[8:-2]\n",
    "    match enc_learn_class:\n",
    "        case \"momentfm.models.moment.MOMENTPipeline\":\n",
    "            if verbose > 0: \n",
    "                ut.print_flush(f\"get_enc_embs_set_stride_set_batch_size | Moment | {average_seq_dim}\", verbose = verbose)\n",
    "            match enc_learn.task_name:\n",
    "                case \"embedding\":\n",
    "                    embs = get_enc_embs_moment( X = X, enc_learn = enc_learn, cpu = cpu, to_numpy = to_numpy, verbose = verbose, average_seq_dim = average_seq_dim)\n",
    "                case \"reconstruction\":\n",
    "                    embs = get_enc_embs_moment_reconstruction(X= X, enc_learn = enc_learn, cpu = cpu, to_numpy = to_numpy, verbose = verbose, average_seq_dim = average_seq_dim, **kwargs)\n",
    "                case _:\n",
    "                    ut.print_flush(f\"Model embeddings for moment-{enc_learn.task_name} is not yet implemented.\", verbose = verbose)\n",
    "        case \"fastai.learner.Learner\":\n",
    "            if verbose > 0: \n",
    "                ut.print_flush(f\"get_enc_embs_set_stride_set_batch_size | MVP | {average_seq_dim}\", verbose = verbose)\n",
    "            if verbose > 1:\n",
    "                ut.print_flush(f\"get_enc_embs_set_stride_set_batch_size | X ~{X.shape}\", verbose = verbose)\n",
    "            embs = get_enc_embs_MVP_set_stride_set_batch_size(\n",
    "                X = X, \n",
    "                enc_learn = enc_learn, \n",
    "                stride = stride, \n",
    "                batch_size = batch_size, \n",
    "                module = module, \n",
    "                cpu = cpu, \n",
    "                average_seq_dim = average_seq_dim,\n",
    "                to_numpy = to_numpy, \n",
    "                verbose = verbose, \n",
    "                time_flag = time_flag, \n",
    "                chunk_size = chunk_size, \n",
    "                check_memory_usage = check_memory_usage\n",
    "            )\n",
    "        case \"uni2ts.model.moirai.module.MoiraiModule\":\n",
    "            if verbose > 0: \n",
    "                ut.print_flush(f\"get_enc_embs_set_stride_set_batch_size | Moirai | {average_seq_dim}\", verbose = verbose)\n",
    "            embs = get_enc_embs_moirai(\n",
    "                enc_input  = X, \n",
    "                enc_model  = enc_learn,\n",
    "                cpu        = cpu, \n",
    "                average_seq_dim = average_seq_dim,\n",
    "                verbose    = verbose,\n",
    "                to_numpy = to_numpy,\n",
    "                **kwargs\n",
    "            )\n",
    "        case _:\n",
    "            ut.print_flush(f\"[ get_enc_embs_set_stride_set_batch_size ] Model embeddings implementation is not yet implemented for {enc_learn_class}.\", verbose = verbose)\n",
    "    # Ñapa: TODO: Gestionar que no se queden en memoria los modelos porque ocupan el 40% de la GPU al llamarlos desde R\n",
    "    if verbose > 0: ut.print_flush(f\"get_enc_embs_set_stride_set_batch_size | Before moving to CPU | embs~{embs.shape}\", verbose = verbose)\n",
    "    if cpu:\n",
    "        #X.cpu()\n",
    "        enc_learn.cpu()\n",
    "        try: \n",
    "            enc_lear.dls.cpu()\n",
    "        except Exception as e: \n",
    "            ut.print_flush(f\"get_enc_embs_set_stride_set_batch_size | Exception: {e}\", verbose = verbose)\n",
    "        #kwargs_to_cpu_(**kwargs)\n",
    "    if verbose > 0: ut.print_flush(f\"get_enc_embs_set_stride_set_batch_size | embs~{embs.shape} -->\", verbose = verbose)\n",
    "    return embs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a1ef71",
   "metadata": {},
   "source": [
    "## Fine-tunning\n",
    "> Take a look on [HuggingFace - Fine-tune a pretrained model](https://huggingface.co/docs/transformers/training) if not used to few-shot learning or fine-tuning models.\n",
    "\n",
    "Steps: \n",
    "\n",
    "1) Prepare the dataset\n",
    "2) Batch the data\n",
    "   - Remember splitting between train & test dataset\n",
    "   - Remember to use DataLoader to iterate over batches\n",
    "4) Load the trained model and check if any modification is needed\n",
    "   - Check wether any layer may be substituted by an \"identity\" if not needed for your case\n",
    "   - Check if any dimension in a conversion layer may be changed to fit your dataset.\n",
    "5) Select an optimizer from torch.optim (Adam)\n",
    "6) ¿If using transformer, lr_scheduler? \n",
    "7) Training loop\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b0d63a-96d8-450f-8a4b-0c5555b4ff25",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b04517d-0af1-4046-b3d1-84519cc19153",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import get_scheduler\n",
    "import evaluate\n",
    "from torch.nn.modules.loss import _Loss\n",
    "from tsai.data.preparation import SlidingWindow\n",
    "from dvats.utils import find_dominant_window_sizes_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "11f146b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def random_windows(\n",
    "    X           : List [ List [ List [ float ]]], \n",
    "    n_windows   : int       = None, \n",
    "    percent     : float     = None, \n",
    "    mssg        : ut.Mssg   = ut.Mssg()\n",
    "):\n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "    - X: Numpy array of windows. Expected shape: [batch_size or n_samples, n_vars, window_len]\n",
    "    Given a numpy array of windows, selects:\n",
    "    - n_windows random windows from the array, if n_windows is given.\n",
    "    - ceil(percent*len(X)) random windows otherwise\n",
    "    \"\"\"\n",
    "    mssg_ = deepcopy(mssg)\n",
    "    mssg_.initial(func_name=f\"{mssg.function} | {ut.funcname()}\")\n",
    "    mssg_.print(f\"N windows: {n_windows}\")\n",
    "    if n_windows is None and percent is None:\n",
    "        windows = torch.from_numpy(X)\n",
    "    else: \n",
    "        n_windows = int(min(X.shape[0], n_windows) if n_windows is not None else np.ceil(percent*X.shape[0]))\n",
    "        mssg_.print(f\"n_windows: {n_windows}\")\n",
    "        random_indices = np.random.randint(0, int(X.shape[0]), n_windows)\n",
    "        windows = X[ random_indices ]\n",
    "        windows = torch.from_numpy(windows)\n",
    "    mssg.print(f\"windows~{windows.shape}\")\n",
    "    mssg_.final()\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c5f2b5-026c-4822-b89f-7f564c2a1e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def windowed_dataset(\n",
    "    X                               : Union [ List [ List [ List [ float ]]], List [ float ], pd.DataFrame ],\n",
    "    stride                          : int           = 1,\n",
    "    window_sizes                    : List [int]    = None,\n",
    "    n_window_sizes                  : int           = 1,\n",
    "    window_sizes_offset             : int           = 0.05,\n",
    "    windows_min_distance            : int           = 1,\n",
    "    full_dataset                    : bool          = False,\n",
    "    mssg                            : ut.Mssg       = ut.Mssg()\n",
    "): \n",
    "    stride = 1 if stride is None else stride \n",
    "    n_window_sizes = 1 if n_window_sizes is None else n_window_sizes\n",
    "    window_sizes_offset = 0.05 if window_sizes_offset is None else window_sizes_offset\n",
    "    windows_min_distance = 1 if windows_min_distance is None else windows_min_distance\n",
    "    full_dataset = False if full_dataset is None else full_dataset\n",
    "    mssg = ut.Mssg() if mssg is None else mssg\n",
    "    mssg.initial(ut.funcname())\n",
    "    dss = []\n",
    "    if isinstance(X, list):\n",
    "        mssg_print(\"X is a list. Converting to dataFrame\")\n",
    "        X = np.array(X)\n",
    "        X = pd.DataFrame(X)        \n",
    "    if ( isinstance(X,pd.DataFrame) or full_dataset): \n",
    "        mssg.print(f\"X is a DataFrame, X~{X.shape} | window_sizes {len(window_sizes) if window_sizes is not None else 0}, n_window_sizes {n_window_sizes}\")\n",
    "        if window_sizes is None or n_window_sizes > len(window_sizes):\n",
    "            mssg.print(\"X is a DataFrame | Selecting Fourier's dominant frequences\")\n",
    "            # Select Fourier's dominant frequences\n",
    "            window_sizes_ = find_dominant_window_sizes_list(\n",
    "                X               = X, \n",
    "                nsizes          = n_window_sizes, \n",
    "                offset          = window_sizes_offset, \n",
    "                min_distance    = windows_min_distance,\n",
    "                mssg            = mssg\n",
    "            )\n",
    "            window_sizes = window_sizes_ if window_sizes is None else list(set(window_sizes + window_sizes_))[:n_window_sizes]\n",
    "            mssg.print(f\"X is a DataFrame | Window sizes: {len(window_sizes)}\", func_name = ut.funcname())\n",
    "        mssg.print(f\"Building the windows\")\n",
    "        for w in window_sizes:\n",
    "            mssg.print(f\"w = {w}\", verbose_level = mssg.level+1)\n",
    "            enc_input, _ = SlidingWindow(window_len = w, stride = stride, get_y=[])(X)\n",
    "            dss.append(enc_input)\n",
    "            mssg.print(f\"w {w} | enc_input~{enc_input.shape} | dss~{len(dss)}\",  verbose_level = mssg.level+1)\n",
    "    else: \n",
    "        mssg.print(\"X is already windowed\")\n",
    "        dss = [X]\n",
    "    mssg.print(f\"Number of windows: {len(dss)}\")\n",
    "    mssg.final()\n",
    "    return dss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e0aca76e-2c3b-4445-b761-8da138cae435",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def setup_scheduler(\n",
    "    dl_train                        : DataLoader,\n",
    "    lr_scheduler_flag               : bool= False,\n",
    "    lr_scheduler_name               : str = \"\",\n",
    "    optimizer                             = None,\n",
    "    num_epochs                      : int = 10,\n",
    "    lr_scheduler_num_warmup_steps   : int = None,\n",
    "    num_training_steps              : int = None,\n",
    "    lr_scheduler_perc_warmup_steps  : int = 0.02,\n",
    "    lr_scheduler_max_lr             : float = None,\n",
    "    lr                              : float = 1e-4\n",
    "):\n",
    "    num_training_steps = num_epochs * len(dl_train) if num_training_steps is None else num_training_steps\n",
    "    lr_scheduler_num_warmup_steps = lr_scheduler_perc_warmup_steps*num_training_steps\n",
    "    lr_scheduler_max_lr = 5 - 10 * lr if lr_scheduler_max_lr is None else lr_scheduler_max_lr\n",
    "    if lr_scheduler_flag:\n",
    "        match lr_scheduler_name:\n",
    "            case \"OneCycleLR\": \n",
    "                lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "                    optimizer           = optimizer,\n",
    "                    max_lr              = lr_scheduler_max_lr,\n",
    "                    epochs              = num_epochs,\n",
    "                    steps_per_epoch     = len(dl_train)\n",
    "                )\n",
    "            case _:\n",
    "                lr_scheduler = get_scheduler(\n",
    "                    name                = lr_scheduler_name,\n",
    "                    optimizer           = optimizer,\n",
    "                    num_warmup_steps    = lr_scheduler_num_warmup_steps,\n",
    "                    num_training_steps  = num_training_steps\n",
    "                )\n",
    "    return lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eca2ee84-52a5-43fa-8c4c-2659324e50b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def prepare_train_and_eval_dataloaders(\n",
    "    X                   : Union [ List [ List [ List [ float ]]], List [ float ], pd.DataFrame ],\n",
    "    batch_size          : int,\n",
    "    n_windows           : int       = None,\n",
    "    n_windows_percent   : int       = None,\n",
    "    training_percent    : int       = 0.4,\n",
    "    validation_percent  : int       = 0.3,\n",
    "    shot                : bool      = False,\n",
    "    eval_pre            : bool      = False,\n",
    "    eval_post           : bool      = False,\n",
    "    mssg                : ut.Mssg   = ut.Mssg()\n",
    "):\n",
    "    dl_eval  = None\n",
    "    ds_train = None,\n",
    "    dl_train = None\n",
    "    mssg.function = f\"{mssg.function} | prepare_train_and_eval_dataloaders\"\n",
    "    if n_windows is None and n_windows_percent is None:\n",
    "        train_split_index = min(X.shape[0], np.ceil(training_percent * X.shape[0]))\n",
    "        eval_split_index = min(X.shape[0], np.ceil(validation_percent * X.shape[0]))\n",
    "    else:\n",
    "        train_split_index = min(X.shape[0], np.ceil(training_percent * n_windows)) if n_windows is not None else np.ceil(training_percent * n_windows_percent * X.shape[0])\n",
    "        eval_split_index = min(X.shape[0], np.ceil(validation_percent * n_windows)) if n_windows is not None else np.ceil(validation_percent * n_windows_percent * X.shape[0])\n",
    "    \n",
    "    train_split_index = int(train_split_index)\n",
    "    eval_split_index = int(eval_split_index)\n",
    "    if shot: \n",
    "        mssg.print(f\"Selecting ds train | {train_split_index} windows\")\n",
    "        ds_train = X[:train_split_index]\n",
    "    if eval_pre or eval_post: \n",
    "        mssg.print(f\"Selecting validation train | {eval_split_index} windows\")\n",
    "        ds_test  = torch.from_numpy(X[:eval_split_index]).float()\n",
    "    # -- Select only the small percentage for few-shot\n",
    "    if shot:\n",
    "        mssg.print(f\"Train DataLoader | Random windows\")\n",
    "        mssg.verbose -= 1\n",
    "        ds_train = random_windows(ds_train, n_windows, n_windows_percent, mssg = mssg)\n",
    "        mssg.verbose += 1\n",
    "        ds_train = ds_train.float()\n",
    "        # Create the dataloader\n",
    "        mssg.print(f\"Train DataLoader | DataLoader\")\n",
    "        dl_train = DataLoader(ds_train, batch_size = batch_size, shuffle = True)\n",
    "    if eval_pre or eval_post: \n",
    "        mssg.print(f\"Validation DataLoader\")\n",
    "        dl_eval  = DataLoader(ds_test, batch_size = batch_size, shuffle = False)\n",
    "    return dl_eval, dl_train, ds_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fca51dd",
   "metadata": {},
   "source": [
    "### Moment\n",
    "> Follow the tutorial in the original repository: [Moment - Imputation](https://github.com/moment-timeseries-foundation-model/moment/blob/main/tutorials/imputation.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c3ddcde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from momentfm.utils.masking import Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7548dcac-1a8e-4933-a8e9-85a3309f3b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def fine_tune_moment_compute_loss_check_sizes_(\n",
    "    batch           : List [ List [ List [ float ] ] ], \n",
    "    output, \n",
    "    verbose         : int   = 0,\n",
    "    # Print options\n",
    "    print_to_path   : bool  = False,\n",
    "    print_path      : str   = \"~/data/logs/logs.txt\",\n",
    "    print_mode      : str   = 'a'\n",
    "):\n",
    "    if verbose > 0: ut.print_flush(\"--> fine_tune_moment_compute_loss_check_sizes_\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose, print_time = print_to_path)\n",
    "    b = batch.clone()\n",
    "    b_2 = batch.shape[2]\n",
    "    re_2 = output.reconstruction.shape[2]\n",
    "    if b_2 > re_2:\n",
    "        if verbose > 0: ut.print_flush(f\" Fine tune loop | TODO: Why? Original {b_2} > {re_2}  Reconstruction\", print_to_path = print_to_path, print_path = print_path, print_mode = 'a', verbose = verbose, print_time = print_to_path)\n",
    "        b = b[...,:re_2]\n",
    "    elif re_2 > b_2:\n",
    "        if verbose > 1: ut.print_flush(f\" Fine tune loop | Why ? Original {b_2} < {re_2} Reconstruction ? Padding\", print_to_path = print_to_path, print_path = print_path, print_mode = 'a', verbose = verbose, print_time = print_to_path)\n",
    "        output.reconstruction = output.reconstruction[...,:b_2]\n",
    "    else: \n",
    "        if verbose > 1: ut.print_flush(f\" Fine tune loop | re_2 {re_2} == {b_2} y_2\", print_to_path = print_to_path, print_path = print_path, print_mode = 'a', verbose = verbose, print_time = print_to_path)\n",
    "    if verbose > 1: \n",
    "        ut.print_flush(f\"---------- Checking loss  ------- | reconstruction ~ {output.reconstruction.shape} | original_ ~ {b.shape}\", print_to_path = print_to_path, print_path = print_path, print_mode = 'a', verbose = verbose, print_time = print_to_path)\n",
    "    if verbose > 0: ut.print_flush(\"fine_tune_moment_compute_loss_check_sizes_ -->\", print_to_path = print_to_path, print_path = print_path, print_mode = 'a', verbose = verbose, print_time = print_to_path)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1e36e8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def fine_tune_moment_compute_loss(\n",
    "    batch, \n",
    "    output, \n",
    "    criterion   = torch.nn.MSELoss, \n",
    "    verbose     = 0, \n",
    "    input_mask  = None, \n",
    "    mask        = None,\n",
    "    # Print options\n",
    "    print_to_path   : bool          = False,\n",
    "    print_path      : str           = \"~/data/logs/logs.txt\",\n",
    "    print_mode      : str           = 'a'\n",
    "):\n",
    "    if verbose > 0: ut.print_flush(\"--> fine_tune_moment_compute_loss\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose, print_time = print_to_path)\n",
    "    b = fine_tune_moment_compute_loss_check_sizes_(batch = batch, output = output, verbose = verbose, print_to_path = print_to_path, print_path = print_path, print_mode = 'a')\n",
    "    if verbose > 0: ut.print_flush(f\"fine_tune_moment_compute_loss | b~{b.shape} | o~{output.reconstruction.shape}\", print_to_path = print_to_path, print_path = print_path, print_mode = 'a', verbose = verbose, print_time = print_to_path)\n",
    "    o = output.reconstruction\n",
    "    device = b.device if b.device != \"cpu\" else o.device\n",
    "    b = b.to(device)\n",
    "    o = o.to(device)\n",
    "    compute_loss = criterion()\n",
    "    recon_loss = compute_loss(o, b)\n",
    "    batch_masks = output.input_mask if input_mask is None else input_mask\n",
    "    mask = output.pretrain_mask if mask is None else mask\n",
    "    batch_masks = batch_masks.to(device)\n",
    "    mask = mask.to(device)\n",
    "    if verbose > 1: ut.print_flush(f\"fine_tune_moment_compute_loss | batch ~ {b.shape} | {b.device}\", print_to_path = print_to_path, print_path = print_path, print_mode = 'a', verbose = verbose, print_time = print_to_path)\n",
    "    if verbose > 1: ut.print_flush(f\"fine_tune_moment_compute_loss | batch_masks ~ {batch_masks.shape} | {batch_masks.device}\", print_to_path = print_to_path, print_path = print_path, print_mode = 'a', verbose = verbose, print_time = print_to_path)\n",
    "    if verbose > 1: ut.print_flush(f\"fine_tune_moment_compute_loss | mask ~ {mask.shape} | {mask.device}\", print_to_path = print_to_path, print_path = print_path, print_mode = 'a', verbose = verbose, print_time = print_to_path)\n",
    "    \n",
    "    observed_mask = batch_masks * (1-mask)\n",
    "    masked_loss = observed_mask * recon_loss\n",
    "    loss = masked_loss.nansum() / (observed_mask.nansum() + 1e-7)\n",
    "    if verbose > 2: ut.print_flush(f\"Loss type: {type(loss)}\",print_to_path = print_to_path, print_path = print_path, print_mode = 'a', verbose = verbose, print_time = print_to_path)  # Debe ser <class 'torch.Tensor'>\n",
    "    if verbose > 1: ut.print_flush(f\"fine_tune_moment_compute_loss | loss: {loss.item()}\", print_to_path = print_to_path, print_path = print_path, print_mode = 'a', verbose = verbose, print_time = print_to_path)\n",
    "    if verbose > 0: ut.print_flush(\"fine_tune_moment_compute_loss -->\", print_to_path = print_to_path, print_path = print_path, print_mode = 'a', verbose = verbose, print_time = print_to_path)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2ff7b693",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def fine_tune_moment_eval_preprocess(\n",
    "    predictions : List [ List [ float ]],\n",
    "    references : List [ List [ float ]],\n",
    "    verbose : int = 0,\n",
    "    # Print options\n",
    "    print_to_path   : bool          = False,\n",
    "    print_path      : str           = \"~/data/logs/logs.txt\",\n",
    "    print_mode      : str           = 'a'\n",
    "):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - predictions torch (float)\n",
    "    - references torch (float)\n",
    "    Returns: \n",
    "        - Predictions and references ensuring same shape and no NaN values. \n",
    "        - Uses the shape of the smallest torch for the modification.\n",
    "    \"\"\"\n",
    "    if verbose > 0: \n",
    "        ut.print_flush(f\"fine_tune_moment_eval | Before reshape | preds~{predictions.shape}\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose, print_time = print_to_path)            \n",
    "        ut.print_flush(f\"fine_tune_moment_eval | Before reshape | refs~{references.shape}\", print_to_path = print_to_path, print_path = print_path, print_mode = 'a', verbose = verbose, print_time = print_to_path)\n",
    "    predictions = einops.rearrange(predictions, \"b v w -> (b v) w\")\n",
    "    references = einops.rearrange(references, \"b v w -> (b v) w\")\n",
    "    # Avoid NaN \n",
    "    if predictions.shape[1] > references.shape[1]: predictions = predictions[:,:references.shape[1]]\n",
    "    if predictions.shape[1] < references.shape[1]: references = references[:,:predictions.shape[1]]\n",
    "    if verbose > 0: \n",
    "        ut.print_flush(f\"Eval | After reshape | preds~{predictions.shape}\", print_to_path = print_to_path, print_path = print_path, print_mode = 'a', verbose = verbose, print_time = print_to_path)\n",
    "        ut.print_flush(f\"Eval | After reshape | refs~{references.shape}\", print_to_path = print_to_path, print_path = print_path, print_mode = 'a', verbose = verbose, print_time = print_to_path)\n",
    "        \n",
    "    nan_mask = torch.isnan(predictions) | torch.isnan(references)\n",
    "    predictions = torch.where(nan_mask, torch.tensor(0.0), predictions)\n",
    "    references = torch.where(nan_mask, torch.tensor(0.0), references)\n",
    "    if verbose > 0: \n",
    "        ut.print_flush(f\"Eval | After NaN | preds~{predictions.shape}\", print_to_path = print_to_path, print_path = print_path, print_mode = 'a', verbose = verbose, print_time = print_to_path)\n",
    "        ut.print_flush(f\"Eval | After NaN | refs~{references.shape}\", print_to_path = print_to_path, print_path = print_path, print_mode = 'a', verbose = verbose, print_time = print_to_path)\n",
    "    return predictions, references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "20e68c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def fine_tune_moment_eval_step_(\n",
    "    enc_learn : Learner,\n",
    "    batch,\n",
    "    mse_metric, \n",
    "    rmse_metric,\n",
    "    mae_metric,\n",
    "    smape_metric,\n",
    "    cpu             : bool = False,\n",
    "    verbose         : int = 0,\n",
    "    # Print options\n",
    "    print_to_path   : bool          = False,\n",
    "    print_path      : str           = \"~/data/logs/logs.txt\",\n",
    "    print_mode      : str           = 'a'\n",
    "):\n",
    "    with torch.no_grad():\n",
    "        output, enc_learn = sure_eval_moment(\n",
    "            enc_learn = enc_learn, \n",
    "            cpu = cpu,\n",
    "            verbose = verbose,                     \n",
    "            y = batch, \n",
    "            input_mask = None,\n",
    "            mask = None,\n",
    "            padd_step = 100, \n",
    "            max_trials = 5, \n",
    "            acts_indices = None,\n",
    "            print_to_path = print_to_path, print_path = print_path, print_mode = print_mode\n",
    "        )\n",
    "        predictions = output.reconstruction\n",
    "        references = batch\n",
    "        predictions = predictions.to(device)\n",
    "        references = references.to(device)\n",
    "        predictions, references = fine_tune_moment_eval_preprocess(predictions = predictions, references = references, verbose = verbose, print_to_path = print_to_path, print_path = print_path, print_mode = print_mode)\n",
    "        mse_metric.add_batch(predictions=predictions, references = references)\n",
    "        rmse_metric.add_batch(predictions=predictions, references = references)\n",
    "        mae_metric.add_batch(predictions=predictions, references = references)\n",
    "        smape_metric.add_batch(predictions=predictions, references = references)\n",
    "        return mse_metric, rmse_metric, mae_metric, smape_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0cc18bf9-fd6f-4758-9862-e0e73206c9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def fine_tune_moment_eval_(\n",
    "    enc_learn : Learner,\n",
    "    dl_eval   : DataLoader,\n",
    "    num_epochs: int = 1,\n",
    "    cpu       : bool = False,\n",
    "    verbose   : int = 0,\n",
    "    # Print options\n",
    "    print_to_path   : bool          = False,\n",
    "    print_path      : str           = \"~/data/logs/logs.txt\",\n",
    "    print_mode      : str           = 'a'\n",
    "):\n",
    "    # Select device\n",
    "    device = \"cpu\" if cpu else torch.cuda.current_device()\n",
    "    # Load metrics\n",
    "    mse_metric = evaluate.load('mse', \"multilist\")\n",
    "    rmse_metric = evaluate.load('mse', \"multilist\")\n",
    "    mae_metric = evaluate.load('mae', \"multilist\")\n",
    "    smape_metric = evaluate.load(\"smape\", \"multilist\")\n",
    "    num_evaluation_steps = len(dl_eval)\n",
    "    enc_learn = enc_learn.to(device)\n",
    "    enc_learn.eval()\n",
    "    #if print_to_path:\n",
    "    #    pf = open(os.path.expanduser(print_path + \"_progress\"), \"w\")\n",
    "    #    progress_bar = tqdm(range(num_evaluation_steps), file = pf)\n",
    "    #    # Predict evaluation dataset\n",
    "    #    for batch in dl_eval:\n",
    "    #        batch = batch.to(device)\n",
    "    #        mse_metric, rmse_metric, mae_metric, smape_metric = fine_tune_moment_eval_step_(\n",
    "    #            enc_learn = enc_learn, \n",
    "    #            batch = batch, \n",
    "    #            mse_metric = mse_metric, \n",
    "    #            rmse_metric = rmse_metric,\n",
    "    #            mae_metric = mae_metric,\n",
    "    #            smape_metric = smape_metric,\n",
    "    #        )\n",
    "    #        progress_bar.update(1)\n",
    "    #    progress_bar.close()\n",
    "    #else:\n",
    "    progress_bar = tqdm(range(num_evaluation_steps))\n",
    "    for batch in dl_eval:\n",
    "        batch = batch.to(device)\n",
    "        mse_metric, rmse_metric, mae_metric, smape_metric = fine_tune_moment_eval_step_(\n",
    "            enc_learn = enc_learn, \n",
    "            batch = batch, \n",
    "            mse_metric = mse_metric, \n",
    "            rmse_metric = rmse_metric,\n",
    "            mae_metric = mae_metric,\n",
    "            smape_metric = smape_metric,\n",
    "        )\n",
    "        progress_bar.update(1)\n",
    "    progress_bar.close()\n",
    "    mse   = mse_metric.compute(squared = False)\n",
    "    rmse  = rmse_metric.compute(squared = True)\n",
    "    mae   = mae_metric.compute()\n",
    "    smape = smape_metric.compute()\n",
    "    eval_results = {\n",
    "        \"mse\": mse,\n",
    "        \"rmse\": rmse,\n",
    "        \"mae\": mae,\n",
    "        \"smape\": smape\n",
    "    }\n",
    "    enc_learn.train()\n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "38f59b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def fine_tune_moment_train_loop_step_(\n",
    "    enc_learn,\n",
    "    batch, \n",
    "    batch_masks,\n",
    "    criterion                               = torch.nn.MSELoss, \n",
    "    window_mask_percent             : float = 0.3,\n",
    "    cpu                             : bool  = False,\n",
    "    verbose                         : int   = 0,\n",
    "    print_to_path                   : bool  = False,\n",
    "    print_path                      : str   = \"~/data/logs/logs.txt\",\n",
    "    print_mode                      : str   = 'a',\n",
    "    use_moment_masks                : bool  = False,\n",
    "    mask_stateful                   : bool  = False,\n",
    "    mask_future                     : bool  = False,\n",
    "    mask_sync                       : bool  = False\n",
    "): \n",
    "    device = torch.cuda.current_device() if not cpu else \"cpu\"\n",
    "    bms = batch_masks\n",
    "    if use_moment_masks:\n",
    "        mask_generator = Masking(mask_ratio = window_mask_percent)\n",
    "    \n",
    "    if batch.shape[0] < batch_masks.shape[0]:  \n",
    "        bms = batch_masks[:batch.shape[0]]\n",
    "    if verbose > 1: \n",
    "        ut.print_flush(\n",
    "            f\"fine_tune_moment_train_loop_step_ | Fine tune loop | batch ~ {batch.shape} | batch_masks ~ {bms.shape}\",\n",
    "            print_to_path = print_to_path, print_path = print_path, print_mode = 'a', verbose = verbose\n",
    "        )\n",
    "    \n",
    "    batch   = batch.to(device)\n",
    "    bms     = bms.to(device) \n",
    "\n",
    "    if bms.shape[0] > batch.shape[0]: bms = bms[:batch.shape[0]]\n",
    "    if verbose > 0: \n",
    "        ut.print_flush(\n",
    "            f\"fine_tune_moment_train_loop_step_ | Fine tune loop | window_mask_percent {window_mask_percent} | batch ~ {batch.shape}\",\n",
    "            print_to_path=print_to_path, print_path=print_path,\n",
    "            print_mode = 'a', verbose = verbose\n",
    "        )\n",
    "    if use_moment_masks:\n",
    "        mask = mask_generator.generate_mask(\n",
    "            x = batch,\n",
    "            input_mask = bms\n",
    "        )\n",
    "    else: \n",
    "        o   = torch.zeros(batch.shape[0], batch.shape[2])\n",
    "        if verbose > 0: \n",
    "            ut.print_flush(\n",
    "                f\"fine_tune_moment_train_loop_step_ | Fine tune loop | o ~ {o.shape} | stateful = {mask_stateful} | sync = {mask_sync} | r = {window_mask_percent}\",\n",
    "                print_to_path=print_to_path, print_path=print_path,\n",
    "                print_mode = 'a', verbose = verbose\n",
    "            )\n",
    "        if mask_future:\n",
    "            mask = create_future_mask(\n",
    "                o       = o, \n",
    "                r       = window_mask_percent, \n",
    "                sync    = mask_sync\n",
    "            )[0,:,:].int() # As there is only 1 variable/variables are flattened, an extra dim is created by the masking function\n",
    "        else:\n",
    "            mask = create_subsequence_mask(\n",
    "                o       = o,\n",
    "                r       = window_mask_percent,\n",
    "                stateful= mask_stateful,\n",
    "                sync    = mask_sync\n",
    "            )[0,:,:].int() # As there is only 1 variable/variables are flattened, an extra dim is created by the masking function\n",
    "        if verbose > 0:\n",
    "            ut.print_flush(\n",
    "                f\"fine_tune_moment_train_loop_step_ | Fine tune loop | Before shape adjustment | batch ~ {batch.shape} | batch_masks ~ {bms.shape} | mask ~ {mask.shape}\",\n",
    "                print_to_path = print_to_path, print_path = print_path, print_mode = 'a', verbose = verbose\n",
    "            )\n",
    "\n",
    "    if mask.shape[0] < bms.shape[0]:  bms = batch_masks[:mask.shape[0]]\n",
    "    if mask.shape[1]  < batch_masks.shape[1] :\n",
    "        mask = torch.nn.functional.pad(mask,(0,batch_masks.shape[1]-mask.shape[1]))\n",
    "    \n",
    "    batch = batch.to(device)\n",
    "    mask = mask.to(device)\n",
    "    bms = bms.to(device)\n",
    "    #ut.print_flush(f\"fine_tune_moment_train_loop_step_ | Enc_learn Before sure_eval_moment {enc_learn.__class__}\")\n",
    "    enc_learn = enc_learn.to(device)\n",
    "    if verbose > 1: \n",
    "        ut.print_flush(\n",
    "            f\"fine_tune_moment_train_loop_step_ | Fine tune loop | batch ~ {batch.shape} | batch_masks ~ {bms.shape} | mask ~ {mask.shape}\",\n",
    "            print_to_path = print_to_path, print_path = print_path, print_mode = 'a', verbose = verbose\n",
    "        )\n",
    "    for param in enc_learn.parameters():\n",
    "        param = param.to(device)\n",
    "    if verbose > 1: \n",
    "        ut.print_flush(\n",
    "            f\"fine_tune_moment_train_loop_step_ | sure_eval_moment | b{batch.device} | m{mask.device} | bm{bms.device}\",\n",
    "            print_to_path = print_to_path, print_path = print_path, print_mode = 'a', verbose = verbose\n",
    "        )\n",
    "    output, enc_learn = sure_eval_moment(\n",
    "        enc_learn = enc_learn, \n",
    "        cpu = cpu,\n",
    "        verbose = verbose, \n",
    "        y = batch, \n",
    "        input_mask = bms, # None\n",
    "        mask = mask, # None\n",
    "        padd_step = 100, \n",
    "        max_trials = 5, \n",
    "        acts_indices = None,\n",
    "        print_to_path = print_to_path, print_path = print_path, print_mode = 'a',\n",
    "        continue_if_fail = True\n",
    "    )\n",
    "    #ut.print_flush(f\"fine_tune_moment_train_loop_step_ | Enc_learn After sure_eval_moment {enc_learn.__class__}\")\n",
    "    # Compute output loss\n",
    "    if output is None:\n",
    "        ut.print_flush(\n",
    "            f\"fine_tune_moment_train_loop_step_ | Execution failed | Output none \",\n",
    "            print_to_path = print_to_path, print_path = print_path, print_mode = 'a', verbose = verbose\n",
    "        )\n",
    "        loss = 0\n",
    "    else: \n",
    "        loss = fine_tune_moment_compute_loss(batch, output, criterion, verbose = verbose, input_mask = bms, mask = mask, print_to_path = print_to_path, print_path = print_path, print_mode = 'a')\n",
    "        #ut.print_flush(f\"fine_tune_moment_train_loop_step_ | Enc_learn After compute loss {enc_learn.__class__} | -->\")\n",
    "    return loss, enc_learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8f06b0ec-8c25-470c-a315-fb89c40e2e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def fine_tune_moment_train_(\n",
    "    enc_learn                       : Learner, \n",
    "    dl_train                        : DataLoader,\n",
    "    ds_train                        : pd.DataFrame,\n",
    "    window_mask_percent             : float = 0.3,\n",
    "    batch_size                      : int   = 1,\n",
    "    num_epochs                      : int   = 1,\n",
    "    criterion                               = torch.nn.MSELoss, \n",
    "    optimizer                               = None, \n",
    "    lr                              : float = 5e-5,  #1 e -4\n",
    "    lr_scheduler_flag               : bool  = False, \n",
    "    lr_scheduler_name               : str   = \"linear\",\n",
    "    lr_scheduler_num_warmup_steps   : int   = None,\n",
    "    cpu                             : bool  = False,\n",
    "    verbose                         : int   = 0,\n",
    "    print_to_path                   : bool  = False,\n",
    "    print_path                      : str   = \"~/data/logs/logs.txt\",\n",
    "    print_mode                      : str   = 'a',\n",
    "    use_moment_masks                : bool  = False,\n",
    "    mask_stateful                   : bool  = False,\n",
    "    mask_future                     : bool  = False,\n",
    "    mask_sync                       : bool  = False\n",
    "):\n",
    "    # Select device\n",
    "    device = \"cpu\" if cpu else torch.cuda.current_device()\n",
    "    # Optimizer and learning rate scheduler\n",
    "    if optimizer is None: \n",
    "        optimizer = torch.optim.AdamW(enc_learn.parameters(), lr)\n",
    "    num_training_steps = num_epochs * len(dl_train)\n",
    "    losses = []\n",
    "    if lr_scheduler_flag:\n",
    "        lr_scheduler = setup_scheduler(\n",
    "            dl_train=dl_train, lr_scheduler_flag=lr_scheduler_flag, lr_scheduler_name=lr_scheduler_name,\n",
    "            optimizer=optimizer, num_epochs = num_epochs, lr_scheduler_num_warmup_steps = lr_scheduler_num_warmup_steps, \n",
    "            num_training_steps= num_training_steps, lr = lr\n",
    "        )\n",
    "    else:\n",
    "        lr_scheduler = None\n",
    "        \n",
    "    # Training loop\n",
    "    if verbose > 1: ut.print_flush(\"fine_tune_moment_train_ | Training loop\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose, print_time = print_to_path)\n",
    "    # Masks\n",
    "    n_samples, n_channels, window_size = ds_train.shape\n",
    "    batch_masks = torch.ones(\n",
    "        (batch_size, window_size), \n",
    "        device = device\n",
    "    ).long()\n",
    "    if verbose > 1: ut.print_flush(f\"fine_tune_moment_train | Fine tune loop | print_to_path {print_to_path} | batch_masks~{batch_masks}\", print_to_path = print_to_path, print_path = print_path, print_mode = 'a', verbose = verbose, print_time = print_to_path)     \n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "    if verbose > 0: ut.print_flush(f\"fine_tune_moment_train | num_epochs {num_epochs} | n_batches {len(dl_train)}\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose, print_time = print_to_path)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, batch in enumerate(dl_train):\n",
    "            if verbose > 0: \n",
    "                #ut.print_flush(f\"fine_tune_moment_train | batch {i} ~ {batch.shape} | epoch {epoch} | train {i+epoch} of {num_training_steps} | Before loop step | Enc_learn {enc_learn.__class__}\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose, print_time = print_to_path, print_both = False)\n",
    "                ut.print_flush(f\"fine_tune_moment_train | batch {i} ~ {batch.shape} | epoch {epoch} | train {i+epoch} of {num_training_steps} | Before loop step\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose, print_time = print_to_path, print_both = False)\n",
    "            loss, enc_learn = fine_tune_moment_train_loop_step_(\n",
    "                    enc_learn                       = enc_learn,\n",
    "                    batch                           = batch,\n",
    "                    batch_masks                     = batch_masks, \n",
    "                    window_mask_percent             = window_mask_percent,\n",
    "                    verbose                         = verbose,\n",
    "                    print_to_path                   = print_to_path,\n",
    "                    print_mode                      = 'a',\n",
    "                    use_moment_masks                = use_moment_masks,\n",
    "                    mask_stateful                   = mask_stateful,\n",
    "                    mask_future                     = mask_future,\n",
    "                    mask_sync                       = mask_sync\n",
    "                )\n",
    "            try: \n",
    "                if verbose > 0: ut.print_flush(\n",
    "                    #f\"fine_tune_moment_train | batch {i} ~ {batch.shape} | epoch {epoch} | train {i+epoch} of {num_training_steps} | Loss backward | After loop step | Enc_learn {enc_learn.__class__}\", \n",
    "                    f\"fine_tune_moment_train | batch {i} ~ {batch.shape} | epoch {epoch} | train {i+epoch} of {num_training_steps} | Loss backward | After loop step \", \n",
    "                    print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose, print_time = print_to_path, print_both = False\n",
    "                )\n",
    "                if isinstance(loss, int):\n",
    "                    losses.append(loss)    \n",
    "                else:\n",
    "                    losses.append(loss.item())\n",
    "                    loss.backward()\n",
    "                optimizer.zero_grad()  \n",
    "                optimizer.step()\n",
    "            except Exception as e: \n",
    "                ut.print_flush(f\"fine_tune_moment_train | batch {i} ~ {batch.shape} | epoch {epoch} | train {i+epoch} of {num_training_steps} | Loss backward failed: {e}\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose, print_time = print_to_path)\n",
    "                if isinstance(loss, int):\n",
    "                    losses.append(loss)                    \n",
    "                else:\n",
    "                    losses.append(np.nan)\n",
    "                optimizer.zero_grad()\n",
    "                optimizer.step()\n",
    "            \n",
    "            if lr_scheduler_flag: lr_scheduler.step()\n",
    "            progress_bar.update(1)\n",
    "    progress_bar.close()\n",
    "    if verbose > 0:\n",
    "        #ut.print_flush(f\"fine_tune_moment_train | enc_learn {enc_learn.__class__} | -->\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose, print_time = print_to_path)\n",
    "        ut.print_flush(f\"fine_tune_moment_train | -->\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, verbose = verbose, print_time = print_to_path)\n",
    "    return losses, enc_learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "93677275",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def fine_tune_moment_single_(\n",
    "    self                : Encoder,\n",
    "    eval_pre            : bool = False,\n",
    "    eval_post           : bool = False,\n",
    "    shot                : bool = True,\n",
    "    sample_id           : int  = 0,\n",
    "    use_moment_masks    : bool = False\n",
    "):\n",
    "    self.mssg.initial_(\"fine_tune_moment_single\")\n",
    "    t_shot              = 0\n",
    "    t_eval_1            = 0\n",
    "    t_eval_2            = 0\n",
    "    losses              = []\n",
    "    eval_results_pre    = \"\"\n",
    "    eval_results_post   = \"\"\n",
    "\n",
    "    if self.time_flag: timer = ut.Time(mssg = self.mssg)\n",
    "    self.mssg.print(f\"fine_tune_moment_single | Prepare the dataset | X ~ {self.input.data[sample_id].shape}\")\n",
    "    # Prepare the dataset\n",
    "    dl_eval, dl_train, ds_train = prepare_train_and_eval_dataloaders(\n",
    "        X                   = self.input.data[sample_id], \n",
    "        batch_size          = self.input.batch_size, \n",
    "        n_windows           = self.input.n_windows, \n",
    "        n_windows_percent   = self.input.n_windows_percent,\n",
    "        training_percent    = self.input.training_percent, \n",
    "        validation_percent  = self.input.validation_percent, \n",
    "        shot                = shot, \n",
    "        eval_pre            = eval_pre, \n",
    "        eval_post           = eval_post,\n",
    "        mssg                = deepcopy(self.mssg)\n",
    "    )\n",
    "    if eval_pre:\n",
    "        self.mssg.print(f\"fine_tune_moment_single | Eval Pre | wlen {self.input.data[sample_id].shape[2]}\")\n",
    "        if self.time_flag: timer.start()\n",
    "        eval_results_pre    = fine_tune_moment_eval_(\n",
    "            enc_learn       = self.model,\n",
    "            dl_eval         = dl_eval,\n",
    "            num_epochs      = self.num_epochs,\n",
    "            cpu             = self.cpu,\n",
    "            verbose         = self.mssg.verbose-1,\n",
    "            print_to_path   = self.mssg.to_path, \n",
    "            print_path      = self.mssg.path, \n",
    "            print_mode      = self.mssg.mode\n",
    "        )\n",
    "        if self.time_flag: \n",
    "            timer.end()\n",
    "            t_eval_1 = timer.duration()\n",
    "            timer.show(verbose = self.mssg.verbose)\n",
    "    if shot:\n",
    "        if self.time_flag: timer.start()\n",
    "        self.mssg.print(f\"fine_tune_moment_single | Train | wlen {self.input.data[sample_id].shape[2]}\")\n",
    "        try:\n",
    "            if self.time_flag: timer.start()\n",
    "            losses, self.model                  = fine_tune_moment_train_(\n",
    "                enc_learn                       = self.model,\n",
    "                dl_train                        = dl_train,\n",
    "                ds_train                        = ds_train,\n",
    "                window_mask_percent             = self.input.window_mask_percent,\n",
    "                batch_size                      = self.input.batch_size,\n",
    "                num_epochs                      = self.num_epochs,\n",
    "                criterion                       = self.optim.criterion, \n",
    "                optimizer                       = self.optim.optimizer, \n",
    "                lr                              = self.optim.lr.lr      if isinstance(self.optim.lr, LRScheduler) else self.optim.lr, \n",
    "                lr_scheduler_flag               = self.optim.lr.flag    if isinstance(self.optim.lr, LRScheduler) else False, \n",
    "                lr_scheduler_name               = self.optim.lr.name    if isinstance(self.optim.lr, LRScheduler) else False,\n",
    "                lr_scheduler_num_warmup_steps   = self.optim.lr.num_warmup_steps if isinstance(self.optim.lr, LRScheduler) else 0,\n",
    "                cpu                             = self.cpu,\n",
    "                verbose                         = self.mssg.verbose-1,\n",
    "                print_to_path                   = self.mssg.to_path, \n",
    "                print_path                      = self.mssg.path, \n",
    "                print_mode                      = self.mssg.mode,\n",
    "                use_moment_masks                = use_moment_masks,\n",
    "                mask_stateful                   = self.mask_stateful,\n",
    "                mask_future                     = self.mask_future,\n",
    "                mask_sync                       = self.mask_sync\n",
    "            )\n",
    "            if self.time_flag:\n",
    "                timer.end()\n",
    "                t_shot = timer.duration()\n",
    "                timer.show()\n",
    "        except Exception as e:\n",
    "            self.mssg.print(f\"fine_tune_moment_single | Train | Window {self.input.shape[2]} not valid | {e}\")\n",
    "            traceback.print_exc()\n",
    "    if eval_post:    \n",
    "        self.mssg.print(f\"fine_tune_moment_single | Eval Post | wlen {self.input.shape[2]}\")\n",
    "        if self.time_flag: timer.start()\n",
    "        eval_results_post = fine_tune_moment_eval_(\n",
    "            enc_learn       = self.model,\n",
    "            dl_eval         = dl_eval,\n",
    "            num_epochs      = self.num_epochs,\n",
    "            cpu             = self.cpu,\n",
    "            verbose         = self.mssg.verbose-1,\n",
    "            print_to_path   = self.mssg.to_path, \n",
    "            print_path      = self.mssg.path, \n",
    "            print_mode      = 'a'\n",
    "        )\n",
    "        if self.time_flag:\n",
    "            timer.end()\n",
    "            t_eval_2 = timer.duration()\n",
    "            if self.mssg.verbose > 0: \n",
    "                timer.show()\n",
    "            if self.mssg.verbose > 0: \n",
    "                self.show_eval_stats(\n",
    "                    # Wether computed or not pre & post errors\n",
    "                    eval_pre        = eval_pre, \n",
    "                    eval_post       = eval_post, \n",
    "                    # Results\n",
    "                    eval_stats_pre  = eval_results_pre,\n",
    "                    eval_stats_post = eval_results_post,\n",
    "                    # Function name\n",
    "                    func_name       = ut.funcname()\n",
    "                )\n",
    "    self.mssg.final(ut.funcname())\n",
    "    return losses, eval_results_pre, eval_results_post, t_shot, t_eval_1, t_eval_2, self.model\n",
    "\n",
    "Encoder.fine_tune_moment_single_ = fine_tune_moment_single_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21133e9-63d4-41e6-8fa0-42265ede334c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def fine_tune_moment_(\n",
    "        self                : Encoder, \n",
    "        eval_pre            : bool = False, \n",
    "        eval_post           : bool = False, \n",
    "        shot                : bool = False,\n",
    "        time_flag           : bool = None,\n",
    "        use_moment_masks    : bool = None\n",
    "):   \n",
    "    self.mssg.initial(ut.funcname())\n",
    "    self.time_flag = self.time_flag if time_flag is None else time_flag\n",
    "    self.use_moment_masks = self.use_moment_masks if use_moment_masks is None else use_moment_masks\n",
    "    # Return values\n",
    "    lossess             = []\n",
    "    eval_results_pre    = []\n",
    "    eval_results_post   = []\n",
    "    t_shots             = []\n",
    "    t_shot              = 0\n",
    "    t_evals             = []\n",
    "    t_eval              = 0\n",
    "    if self.input.size is None:\n",
    "        self.mssg.print(f\"Windows: {len(self.input._data)}\")\n",
    "        raise ValueError(f\"Invalid number of windows: {self.input.size}\")\n",
    "    self.mssg.print(f\"Processing {self.input.size} datasets : {self.input.shape}\")\n",
    "    # Build optimizer\n",
    "    if self.optim.optimizer is None: \n",
    "        self.mssg.print(f\"Setting up optimizer as AdamW\")\n",
    "        self.optim.optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.optim.lr.lr)\n",
    "    # Compute model for each window in the windowed dataset\n",
    "    for i in range(self.input.size):\n",
    "        self.mssg.print(f\"Processing wlen {self.input.shape[2]}\")\n",
    "        ( \n",
    "            losses, eval_results_pre_, eval_results_post_, t_shot_, t_eval_1, t_eval_2, self.model\n",
    "        ) =  self.fine_tune_moment_single_(eval_pre, eval_post, shot, i, use_moment_masks)\n",
    "        lossess.append(losses)\n",
    "        if (eval_pre): eval_results_pre = eval_results_pre_\n",
    "        eval_results_post.append(eval_results_post_)\n",
    "        t_shots.append(t_shot_)\n",
    "        if eval_pre: t_evals.append(t_eval_1)\n",
    "        if eval_post: t_evals.append(t_eval_2)\n",
    "        eval_pre = False\n",
    "    t_shot = sum(t_shots)\n",
    "    t_eval = sum(t_evals)\n",
    "    self.mssg.final(ut.funcname())\n",
    "    return lossess, eval_results_pre, eval_results_post, t_shots, t_shot, t_evals, t_eval, self.model\n",
    "\n",
    "Encoder.fine_tune_moment_ = fine_tune_moment_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7455264d-4c8c-4c4f-9197-ea522878ecc9",
   "metadata": {},
   "source": [
    "### MVP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36653fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastai.metrics import mae\n",
    "def rmse(preds, targets):\n",
    "    res = torch.sqrt(torch.nn.functional.mse_loss(preds, targets))\n",
    "    return res\n",
    "\n",
    "def smape(preds, targets):\n",
    "    res = 100 * torch.mean(2 * torch.abs(preds - targets) / (torch.abs(preds) + torch.abs(targets)))\n",
    "    return res\n",
    "\n",
    "def rmse_flat(preds, targets):\n",
    "    \"\"\"\n",
    "    Computes RMSE while flattening the tensors to ensure compatibility with MSELossFlat.\n",
    "    \"\"\"\n",
    "    preds, targets = preds.view(-1), targets.view(-1)  # Flatten tensors\n",
    "    return torch.sqrt(torch.nn.functional.mse_loss(preds, targets))\n",
    "\n",
    "def smape_flat(preds, targets):\n",
    "    \"\"\"\n",
    "    Computes SMAPE while flattening the tensors to ensure compatibility with MSELossFlat.\n",
    "    \"\"\"\n",
    "    preds, targets = preds.view(-1), targets.view(-1)  # Flatten tensors\n",
    "    denominator = (torch.abs(preds) + torch.abs(targets))\n",
    "    return 100 * torch.mean(2 * torch.abs(preds - targets) / torch.clamp(denominator, min=1e-7))\n",
    "def mae_flat(preds, targets):\n",
    "    \"\"\"\n",
    "    Computes Mean Absolute Error (MAE) while flattening the tensors to ensure compatibility.\n",
    "    \"\"\"\n",
    "    preds, targets = preds.view(-1), targets.view(-1)  # Flatten tensors\n",
    "    return torch.mean(torch.abs(preds - targets))\n",
    "\n",
    "def mse_loss_flat(preds, targets):\n",
    "    \"\"\"\n",
    "    Computes Mean Squared Error (MSE) while flattening the tensors to ensure compatibility.\n",
    "    \"\"\"\n",
    "    preds, targets = preds.view(-1), targets.view(-1)  # Flatten tensors\n",
    "    return torch.mean((preds - targets) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba903b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastai.losses import BaseLoss\n",
    "from fastai.losses import MSELossFlat\n",
    "from fastai.losses import L1LossFlat\n",
    "\n",
    "class RMSELoss(_Loss):\n",
    "    __constants__ = [\"reduction\"]\n",
    "    def __init__(self, size_average = None, reduce = None, reduction: str = \"mean\") -> None:\n",
    "        super().__init__(size_average, reduce, reduction)\n",
    "    \n",
    "    def forward(self, input: Tensor, target:Tensor) -> Tensor:\n",
    "        return torch.nn.functional.mse_loss(input, target, reduction = self.reduction)\n",
    "\n",
    "@use_kwargs_dict(reduction='mean')\n",
    "def RMSELossFlat(\n",
    "    *args,\n",
    "    axis:int = -1,\n",
    "    floatify: bool = True, \n",
    "    **kwargs\n",
    "):\n",
    "    \"Computes RMSE with flattening, similar to MSELossFlat.\"\n",
    "    return BaseLoss(RMSELoss, *args, axis = axis, floatify = floatify, is_2d = False, **kwargs)\n",
    "\n",
    "class SMAPELoss(_Loss):\n",
    "    __constants__ = [\"reduction\"]\n",
    "    \n",
    "    def __init__(self, size_average=None, reduce=None, reduction: str = \"mean\") -> None:\n",
    "        \"\"\"\n",
    "        Initializes the SMAPE Loss.\n",
    "        \n",
    "        Args:\n",
    "            size_average (bool, optional): Deprecated (use reduction).\n",
    "            reduce (bool, optional): Deprecated (use reduction).\n",
    "            reduction (str): Specifies the reduction to apply to the output ('none', 'mean', 'sum').\n",
    "        \"\"\"\n",
    "        super().__init__(size_average, reduce, reduction)\n",
    "    \n",
    "    def forward(self, input: Tensor, target: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Computes the SMAPE loss.\n",
    "        \n",
    "        Args:\n",
    "            input (Tensor): Predicted values.\n",
    "            target (Tensor): Ground truth values.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Computed SMAPE loss.\n",
    "        \"\"\"\n",
    "        return self.smape_loss(input, target)\n",
    "    \n",
    "    @staticmethod\n",
    "    def smape_loss(preds: Tensor, targets: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Computes the SMAPE loss for the given predictions and targets.\n",
    "        \n",
    "        Args:\n",
    "            preds (Tensor): Predicted values.\n",
    "            targets (Tensor): Ground truth values.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: SMAPE loss.\n",
    "        \"\"\"\n",
    "        denominator = (torch.abs(preds) + torch.abs(targets))\n",
    "        smape = 100 * torch.mean(2 * torch.abs(preds - targets) / torch.clamp(denominator, min=1e-7))\n",
    "        return smape\n",
    "\n",
    "\n",
    "@use_kwargs_dict(reduction=\"mean\")\n",
    "def SMAPELossFlat(\n",
    "    *args,\n",
    "    axis: int = -1,\n",
    "    floatify: bool = True,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes SMAPE with flattening, similar to MSELossFlat.\n",
    "    \n",
    "    Args:\n",
    "        axis (int): Axis to flatten. Default is -1.\n",
    "        floatify (bool): Convert target to float. Default is True.\n",
    "        **kwargs: Additional arguments.\n",
    "    \"\"\"\n",
    "    return BaseLoss(SMAPELoss, *args, axis=axis, floatify=floatify, is_2d=False, **kwargs)\n",
    "\n",
    "# Class alias for clarity\n",
    "MAELossFlat = L1LossFlat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e99761",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#TODO: Check. Adding lr_scheduler & optimizer to mvp\n",
    "from fastai.callback.core import Callback\n",
    "\n",
    "class CustomOptimizerCallback(Callback):\n",
    "    def __init__(self, optimizer, scheduler):\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "\n",
    "    def before_fit(self):\n",
    "        # Reemplazar el optimizador de FastAI con el personalizado\n",
    "        self.learn.opt = self.optimizer\n",
    "\n",
    "    def after_batch(self):\n",
    "        # Actualizar el scheduler después de cada batch\n",
    "        if self.scheduler is not None:\n",
    "            self.scheduler.step()\n",
    "\n",
    "    def after_fit(self):\n",
    "        # Restaurar el optimizador original si es necesario\n",
    "        del self.learn.opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f54b205",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def validate_with_metrics(learner, metrics):\n",
    "    results = []\n",
    "    for metric in metrics:\n",
    "        learner.crit = metric\n",
    "        result = learner.validate()\n",
    "        results.append(result.item() if hasattr(result, 'item') else result)\n",
    "    learner.crit=MSELossFlat\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a832a490",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mvp_format_results(results):\n",
    "    return {\n",
    "        \"mse\"   : results[0],\n",
    "        \"rmse\"  : results[1],\n",
    "        \"mae\"   : results[2],\n",
    "        \"smape\" : results[3]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8140ec07-2b42-4b9c-ac57-32d868b551ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def fine_tune_mvp_single_(\n",
    "    self            : Encoder,\n",
    "    eval_pre        : bool  = False,\n",
    "    eval_post       : bool  = False,\n",
    "    shot            : bool  = False,\n",
    "    show_plot       : bool  = False,\n",
    "    sample_id       : int   = 0\n",
    "):\n",
    "    self.show_plot = self.show_plot if show_plot is None else show_plot\n",
    "    t_shot = 0\n",
    "    t_eval_1 = 0\n",
    "    t_eval_2 = 0\n",
    "    losses = []\n",
    "    eval_results_pre = \"\",\n",
    "    eval_results_post = \"\"\n",
    "    if self.time_flag : timer = ut.Time(mssg = self.mssg)\n",
    "    self.mssg.initial(\"fine_tune_mvp_single_\")   \n",
    "    X = self.get_splits_(sample_id)\n",
    "    self.mssg.print(\"About to set callbacks\", func_name = ut.funcname())\n",
    "    cbs = L(WandbCallback(log_preds=False)) if self.use_wandb else L()\n",
    "    cbs2 = [\n",
    "        EarlyStoppingCallback(\n",
    "            monitor='valid_loss', \n",
    "            min_delta=0.000001, \n",
    "            patience=10\n",
    "        ),\n",
    "        #SaveModelCallback(\n",
    "        #    monitor = 'valid_loss', \n",
    "        #    fname = 'best_model'\n",
    "        #),\n",
    "    ]\n",
    "    \n",
    "\n",
    "    self.mssg.print(\"About to set batch tfms\")\n",
    "    tfms = [ToFloat(), None]\n",
    "    batch_tfms = [\n",
    "        TSStandardize(\n",
    "            by_sample       = self.norm_by_sample, \n",
    "            use_single_batch= self.norm_use_single_batch\n",
    "        )\n",
    "    ]\n",
    "    dls = get_ts_dls(X, splits = self.splits, tfms = tfms, bs = self.input.batch_size, batch_tfms = batch_tfms)\n",
    "\n",
    "    ### \n",
    "    # Optimizer  ### TODO: CHECK\n",
    "    #### if not ( isinstance(self.optim.lr, float) or isinstance(self.optim.lr, int)):\n",
    "    ####     if self.optim.lr.flag:\n",
    "    ####         scheduler = setup_scheduler(\n",
    "    ####             dl_train = dls.train,\n",
    "    ####             lr_scheduler_flag = True,\n",
    "    ####             lr_scheduler_name = self.optim.lr.name,\n",
    "    ####             optimizer         = self.optim.optimizer,\n",
    "    ####             num_epochs        = self.num_epochs,\n",
    "    ####             lr_scheduler_num_warmup_steps = self.optim.lr.num_warmup_steps,\n",
    "    ####             lr_scheduler_max_lr = None, #TODO: Think\n",
    "    ####             lr                  = self.optim.lr.lr\n",
    "    ####         )\n",
    "    ####         custom_opt_cb = CustomOptimizerCallback(optimizer = self.optim.optimizer, scheduler = scheduler)\n",
    "    ####         cbs2 += [custom_opt_cb]\n",
    "    ###\n",
    "    if self.show_plot: \n",
    "        self.mssg.print(\"Show plot\")\n",
    "        display(dls.show_at(0))\n",
    "        sgc = ShowGraphCallback2()\n",
    "        self.model = ts_learner(\n",
    "            dls, \n",
    "            InceptionTimePlus,\n",
    "            cbs = cbs + sgc + MVP(\n",
    "                r           = self.optim.lr if isinstance(self.optim.lr, float) else self.optim.lr.lr,\n",
    "                window_size = X.shape[2]-1,\n",
    "                future_mask = self.mask_future,\n",
    "                target_dir  = './models',\n",
    "                sync        = self.mask_sync,\n",
    "                stateful    = self.mask_stateful,\n",
    "                fname       = f'encoder_MVP'\n",
    "            ),\n",
    "            y_range = [X.min(), X.max()]\n",
    "        )\n",
    "    else:\n",
    "        self.mssg.print(\"Don't show plot\")\n",
    "        self.model = ts_learner(\n",
    "            dls, \n",
    "            InceptionTimePlus,\n",
    "            cbs = cbs + MVP(\n",
    "                r           = self.optim.lr if ( isinstance(self.optim.lr, float) or isinstance(self.optim.lr, int)) else self.optim.lr.lr,\n",
    "                window_size = X.shape[2]-1,\n",
    "                future_mask = self.mask_future,\n",
    "                target_dir  = './models',\n",
    "                sync        = self.mask_sync,\n",
    "                stateful    = self.mask_stateful,\n",
    "                fname       = f'encoder_MVP'\n",
    "            ),\n",
    "            y_range = [X.min(), X.max()]\n",
    "            #metrics = [torch.nn.functional.mse_loss, rmse, mae, smape]\n",
    "        )\n",
    "        self.mssg.print(f\"Model Class {self.model.__class__} | Type: {type(self.model)}\")\n",
    "\n",
    "    device = \"cpu\" if self.cpu else torch.cuda.current_device()\n",
    "    self.model.to(device)\n",
    "    self.mssg.print(f\"Model Class {self.model.__class__} | Type: {type(self.model)}\")\n",
    "\n",
    "    # Eval - pre \n",
    "    if eval_pre:\n",
    "        if self.time_flag: timer.start()\n",
    "        self.mssg.print(f\"Eval Pre | wlen {X.shape[2]} | Model: {self.model.__class__} | {type(self.model)} \")\n",
    "        self.model.eval()\n",
    "        results = validate_with_metrics(self.model, self.metrics)\n",
    "        eval_results_pre = mvp_format_results(results)\n",
    "        if self.time_flag:\n",
    "            timer.end()\n",
    "            t_eval_1 = timer.duration()\n",
    "            timer.show(verbose = self.mssg.verbose)\n",
    "    # Train \n",
    "    if shot:\n",
    "        if self.time_flag: timer.start()\n",
    "        self.model.train()\n",
    "        self.mssg.print(f\"Training the model | window size {X.shape[2]} | X ~ {X.shape}\")\n",
    "        lr_valley, lr_steep = self.model.lr_find(suggest_funcs=(valley, steep), show_plot=show_plot)\n",
    "        self.model.fit_one_cycle(\n",
    "            n_epoch = self.num_epochs, \n",
    "            lr_max  = lr_valley,  \n",
    "            cbs     = cbs2\n",
    "        )\n",
    "        losses = self.model.recorder.losses\n",
    "        if self.time_flag:\n",
    "            timer.end()\n",
    "            t_shot= timer.duration()\n",
    "            timer.show(verbose = self.mssg.verbose)\n",
    "\n",
    "    # Eval - post\n",
    "    if eval_post:\n",
    "        if self.time_flag: timer.start()\n",
    "        self.mssg.print(f\"Eval Pre | wlen {X.shape[2]}\")\n",
    "        self.model.eval()\n",
    "        results = validate_with_metrics(self.model, self.metrics)\n",
    "        self.mssg.print(f\"Format results | results~{len(results)}\")\n",
    "        eval_results_post = mvp_format_results(results)\n",
    "        if self.time_flag:\n",
    "            timer.end()\n",
    "            t_eval_2 = timer.duration()\n",
    "            timer.show(verbose = self.mssg.verbose)\n",
    "    self.mssg.final()\n",
    "    return losses, eval_results_pre, eval_results_post, t_shot, t_eval_1, t_eval_2, self.model\n",
    "Encoder.fine_tune_mvp_single_ = fine_tune_mvp_single_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94967aec-fa42-415a-8188-403d8fa53875",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# TODO: Revisar inclusion del optimizer en fine_tune_mvp_\n",
    "\n",
    "def fine_tune_mvp_(\n",
    "    self                    : Encoder,\n",
    "    eval_pre                : bool  = True,\n",
    "    eval_post               : bool  = True,\n",
    "    shot                    : bool  = False,\n",
    "    time_flag               : bool  = None,\n",
    "    use_wandb               : bool  = None,\n",
    "    analysis_mode           : str   = None,\n",
    "    norm_by_sample          : bool  = None,\n",
    "    norm_use_single_batch   : bool  = None,\n",
    "    show_plot               : bool  = None\n",
    "):\n",
    "    self.mssg.initial_(\"fine_tune_mvp_\")\n",
    "    self.time_flag      = self.time_flag if time_flag is None else time_flag\n",
    "    self.use_wandb      = self.use_wandb if use_wandb is None else use_wandb\n",
    "    self.analysis_mode  = self.analysis_mode if analysis_mode is None else analysis_mode\n",
    "    self.norm_by_sample = self.norm_by_sample if norm_by_sample is None else norm_by_sample\n",
    "    self.norm_use_single_batch = self.norm_use_single_batch if norm_use_single_batch is None else norm_use_single_batch\n",
    "    # Return values\n",
    "    lossess             = []\n",
    "    eval_results_pre    = []\n",
    "    eval_results_post   = []\n",
    "    t_shots             = []\n",
    "    t_shot              = 0\n",
    "    t_evals             = []\n",
    "    t_eval              = 0\n",
    "\n",
    "    if self.input.size is None:\n",
    "        self.mssg.print(f\"Windows: {len(self.input._data)}\")\n",
    "        raise ValueError(f\"Invalid number of windows: {self.input.size}\")\n",
    "    self.mssg.print(f\"Processing {self.input.size} datasets: {self.input.shapes}\")\n",
    "    # Build optimizer\n",
    "    if self.optim.optimizer is None: \n",
    "        self.mssg.print(f\"Setting up optimizer as AdamW\")\n",
    "        if (not isinstance(self.optim.lr, float)):\n",
    "            self.optim.optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.optim.lr.lr)\n",
    "        else:\n",
    "            self.optim.optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr)\n",
    "    # Compute model for each window in the windowed dataset\n",
    "    for i in range(self.input.size):\n",
    "        self.mssg.print(f\"Processing wlen {self.input.shape[2]}\")\n",
    "        ( \n",
    "            losses, eval_results_pre_, eval_results_post_, t_shot_, t_eval_1, t_eval_2, self.model\n",
    "        ) =  self.fine_tune_mvp_single_(eval_pre, eval_post, shot, sample_id = i, show_plot = show_plot)\n",
    "        lossess.append(losses)\n",
    "        if (eval_pre): eval_results_pre = eval_results_pre_\n",
    "        eval_results_post.append(eval_results_post_)\n",
    "        t_shots.append(t_shot_)\n",
    "        if eval_pre: t_evals.append(t_eval_1)\n",
    "        if eval_post: t_evals.append(t_eval_2)\n",
    "        eval_pre = False\n",
    "    t_shot = sum(t_shots)\n",
    "    t_eval = sum(t_evals)\n",
    "    self.mssg.final(ut.funcname())\n",
    "    return lossess, eval_results_pre, eval_results_post, t_shots, t_shot, t_evals, t_eval, self.model\n",
    "\n",
    "Encoder.fine_tune_mvp_ = fine_tune_mvp_ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc806e5c-f799-470a-a925-04de33240a05",
   "metadata": {},
   "source": [
    "### Moirai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7756b3e-a227-4790-bd1a-5f9c21ce0880",
   "metadata": {},
   "source": [
    "### Global method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8e4e43-acbd-4706-b18c-b5a9ba709c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def fine_tune__old(\n",
    "    X                               : Union [ List [ List [ List [ float ]]], List [ float ], pd.DataFrame ],\n",
    "    enc_learn                       : Learner, \n",
    "    stride                          : int           = 1,      \n",
    "    batch_size                      : int           = 32,\n",
    "    cpu                             : bool          = False,\n",
    "    to_numpy                        : bool          = True, \n",
    "    verbose                         : int           = 0, \n",
    "    time_flag                       : bool          = False,\n",
    "    n_windows                       : int           = None,\n",
    "    n_windows_percent               : float         = None,\n",
    "    validation_percent              : float         = 0.2, \n",
    "    training_percent                : float         = 0.2,\n",
    "    window_mask_percent             : float         = 0.3,\n",
    "    num_epochs                      : int           = 3,\n",
    "    shot                            : bool          = True,\n",
    "    eval_pre                        : bool          = True,\n",
    "    eval_post                       : bool          = True,\n",
    "    criterion                       : _Loss         = torch.nn.MSELoss, \n",
    "    optimizer                                       = None, \n",
    "    lr                              : float         = 5e-5, #1e-4, \n",
    "    lr_scheduler_flag               : bool          = False, \n",
    "    lr_scheduler_name               : str           = \"linear\",\n",
    "    lr_scheduler_num_warmup_steps   : int           = None,\n",
    "    window_sizes                    : List [int]    = None,\n",
    "    n_window_sizes                  : int           = 1,\n",
    "    window_sizes_offset             : int           = 0.05,\n",
    "    windows_min_distance            : int           = 1,\n",
    "    full_dataset                    : bool          = False,\n",
    "    #- Printing options for debugging\n",
    "    print_to_path                   : bool          = False,\n",
    "    print_path                      : str           = \"~/data/logs/logs.txt\",\n",
    "    print_mode                      : str           = 'a',\n",
    "    #- Only for moment\n",
    "    use_moment_masks                : bool          = False,\n",
    "    #- Masking options\n",
    "    mask_stateful                   : bool          = False,\n",
    "    mask_future                     : bool          = False,\n",
    "    mask_sync                       : bool          = False\n",
    "): \n",
    "    mssg = ut.Mssg(\n",
    "            to_path=print_to_path,\n",
    "            path=print_path,\n",
    "            mode=print_mode,\n",
    "            verbose=verbose\n",
    "        ) \n",
    "    mssg.initial()\n",
    "    \n",
    "    lossess, eval_results_pre, eval_results_post, t_shots, t_shot, t_evals, t_eval = ( None, None, None, None, None, None, None )\n",
    "    \n",
    "    enc_input = windowed_dataset(\n",
    "        X, stride, window_sizes, \n",
    "        n_window_sizes, window_sizes_offset, \n",
    "        windows_min_distance, full_dataset, \n",
    "        mssg\n",
    "    )\n",
    "    enc_input = EncoderInput(\n",
    "        _data               = enc_input, \n",
    "        stride              = stride,\n",
    "        batch_size          = batch_size,\n",
    "        n_windows           = n_windows,\n",
    "        n_windows_percent   = n_windows_percent,\n",
    "        validation_percent  = validation_percent,\n",
    "        training_percent    = training_percent,\n",
    "        window_mask_percent = window_mask_percent,\n",
    "    )\n",
    "    optim = EncoderOptimizer(\n",
    "        criterion   = criterion,\n",
    "        optimizer   = optimizer,\n",
    "        lr          = LRScheduler (\n",
    "                        flag            = lr_scheduler_flag,\n",
    "                        name            = lr_scheduler_name,\n",
    "                        num_warmup_steps= lr_scheduler_num_warmup_steps\n",
    "        ),\n",
    "    )\n",
    "    enc = Encoder(\n",
    "        model           = enc_learn,\n",
    "        input           = enc_input,\n",
    "        mssg            = mssg,\n",
    "        cpu             = cpu,\n",
    "        to_numpy        = to_numpy, \n",
    "        num_epochs      = num_epochs, \n",
    "        optim           = optim,\n",
    "        mask_stateful   = mask_stateful,\n",
    "        mask_future     = mask_future,\n",
    "        mask_sync       = mask_sync,\n",
    "        eval_stats_pre  = eval_results_pre,\n",
    "        eval_stats_post = eval_results_post\n",
    "    )\n",
    "    enc.set_fine_tune_()\n",
    "    match enc.fine_tune_.__name__:\n",
    "        case \"fine_tune_moment_\":\n",
    "            ( \n",
    "                lossess, eval_results_pre, eval_results_post, \n",
    "                t_shots, t_shot, t_evals, t_eval, enc.model \n",
    "            ) = enc.fine_tune_(\n",
    "                eval_pre, eval_post, shot, time_flag, use_moment_masks\n",
    "            )\n",
    "        case _:\n",
    "            ( \n",
    "                lossess, eval_results_pre, eval_results_post, \n",
    "                t_shots, t_shot, t_evals, t_eval, enc.model \n",
    "            ) = enc.fine_tune_(eval_pre, eval_post, shot, time_flag)\n",
    "    return lossess, eval_results_pre, eval_results_post, t_shots, t_shot, t_evals, t_eval, enc.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353eddb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _get_mssg(\n",
    "    mssg : ut.Mssg = None,\n",
    "    verbose                         : int           = 0, \n",
    "    print_to_path                   : bool          = False,\n",
    "    print_path                      : str           = \"~/data/logs/logs.txt\",\n",
    "    print_mode                      : str           = 'a',\n",
    "):\n",
    "    mssg,_ = ut._check_value(mssg, None, \"mssg\", ut.Mssg)\n",
    "    if mssg is None:\n",
    "        mssg = ut.Mssg(\n",
    "            to_path = print_to_path,\n",
    "            path    = print_path,\n",
    "            mode    = print_mode,\n",
    "            verbose = verbose\n",
    "        ) \n",
    "    return mssg\n",
    "\n",
    "def _get_enc_input(\n",
    "    mssg                            : ut.Mssg,\n",
    "    # Encoder Input\n",
    "    ## -- Using all parammeters\n",
    "    X                               : Optional [ Union [ List [ List [ List [ float ]]], List [ float ], pd.DataFrame ] ],\n",
    "    stride                          : Optional [ int ]          = None,\n",
    "    batch_size                      : Optional [ int ]          = None,\n",
    "    n_windows                       : Optional [ int ]          = None,\n",
    "    n_windows_percent               : Optional [ float ]        = None,\n",
    "    validation_percent              : Optional [ float ]        = None, \n",
    "    training_percent                : Optional [ float ]        = None,\n",
    "    window_mask_percent             : Optional [ float ]        = None,\n",
    "    window_sizes                    : Optional [ List [int] ]   = None,\n",
    "    n_window_sizes                  : Optional [ int ]          = 1,\n",
    "    window_sizes_offset             : Optional [ int ]          = 0.05,\n",
    "    windows_min_distance            : Optional [ int ]          = 1,\n",
    "    full_dataset                    : Optional [ bool ]         = False,\n",
    "    ## -- Using Type\n",
    "    enc_input                       : Optional [ EncoderInput ] = None\n",
    "): \n",
    "    mssg.initial_(func_name = ut.funcname())\n",
    "    enc_input, _ = ut._check_value(enc_input, None, \"enc_input\", EncoderInput, True, False, False)\n",
    "    mssg.print(f\"is none enc_input? {enc_input is None}\")\n",
    "    if enc_input is None:\n",
    "        mssg.print(f\"About to get the windows\")\n",
    "        enc_input = windowed_dataset(\n",
    "            X                       = X,\n",
    "            stride                  = stride,\n",
    "            window_sizes            = window_sizes,\n",
    "            n_window_sizes          = n_window_sizes,\n",
    "            window_sizes_offset     = window_sizes_offset,\n",
    "            windows_min_distance    = windows_min_distance,\n",
    "            full_dataset            = full_dataset,\n",
    "            mssg                    = mssg\n",
    "        )\n",
    "        mssg.print(f\"About to get the encoder input | windows~{len(enc_input)}\", func_name = ut.funcname())\n",
    "        enc_input = EncoderInput(\n",
    "            _data               = enc_input, \n",
    "            stride              = stride,\n",
    "            batch_size          = batch_size,\n",
    "            n_windows           = n_windows,\n",
    "            n_windows_percent   = n_windows_percent,\n",
    "            validation_percent  = validation_percent,\n",
    "            training_percent    = training_percent,\n",
    "            window_mask_percent = window_mask_percent,\n",
    "        )\n",
    "        mssg.print(f\"Enc input obtained | enc_input~{enc_input.shape}\")\n",
    "    mssg.final()\n",
    "    return enc_input\n",
    "\n",
    "def _get_optimizer(\n",
    "    mssg                            : ut.Mssg,\n",
    "    optim                           : EncoderOptimizer = None,\n",
    "    criterion                       : _Loss         = torch.nn.MSELoss, \n",
    "    optimizer                                       = None, \n",
    "    lr                              : float         = 5e-5, #1e-4, \n",
    "    lr_scheduler_flag               : bool          = False, \n",
    "    lr_scheduler_name               : str           = \"linear\",\n",
    "    lr_scheduler_num_warmup_steps   : int           = None\n",
    "):\n",
    "    mssg.initial(ut.funcname())\n",
    "    optim,_ = ut._check_value(optim, None, \"optim\", EncoderOptimizer, True)\n",
    "    if optim is None:\n",
    "        optim = EncoderOptimizer(\n",
    "            criterion   = criterion,\n",
    "            optimizer   = optimizer,\n",
    "            lr          = LRScheduler (\n",
    "                            lr              = lr,\n",
    "                            flag            = lr_scheduler_flag,\n",
    "                            name            = lr_scheduler_name,\n",
    "                            num_warmup_steps= lr_scheduler_num_warmup_steps\n",
    "            ),\n",
    "        )\n",
    "    mssg.final()\n",
    "    return optim\n",
    "\n",
    "def _get_encoder(\n",
    "    ## -- Using all parammeters\n",
    "    X                               : Optional [ Union [ List [ List [ List [ float ]]], List [ float ], pd.DataFrame ] ],\n",
    "    stride                          : Optional [ int ]          = None,\n",
    "    batch_size                      : Optional [ int ]          = None,\n",
    "    n_windows                       : Optional [ int ]          = None,\n",
    "    n_windows_percent               : Optional [ float ]        = None,\n",
    "    validation_percent              : Optional [ float ]        = None, \n",
    "    training_percent                : Optional [ float ]        = None,\n",
    "    window_mask_percent             : Optional [ float ]        = None,\n",
    "    window_sizes                    : Optional [ List [int] ]   = None,\n",
    "    n_window_sizes                  : Optional [ int ]          = 1,\n",
    "    window_sizes_offset             : Optional [ int ]          = 0.05,\n",
    "    windows_min_distance            : Optional [ int ]          = 1,\n",
    "    full_dataset                    : Optional [ bool ]         = False,\n",
    "    ##-- Given by Type \n",
    "    enc_input                       : Optional [ EncoderInput ] = None,\n",
    "    # Optimizer\n",
    "    optim                           : Optional [ EncoderOptimizer ] = None,\n",
    "    ## -- Using all parameters\n",
    "    criterion                       : Optional [ _Loss ]            = torch.nn.MSELoss, \n",
    "    optimizer                                                       = None, \n",
    "    lr                              : Optional [ float ]            = 5e-5, #1e-4, \n",
    "    lr_scheduler_flag               : Optional [ bool ]             = False, \n",
    "    lr_scheduler_name               : Optional [ str ]              = \"linear\",\n",
    "    lr_scheduler_num_warmup_steps   : Optional [ int ]              = None,\n",
    "    # Mssg\n",
    "    ## -- Using all parameters\n",
    "    verbose                         : Optional[ int ]               = 0, \n",
    "    print_to_path                   : Optional[ bool ]              = False,\n",
    "    print_path                      : Optional[ str ]               = \"~/data/logs/logs.txt\",\n",
    "    print_mode                      : Optional[ str ]               = 'a',\n",
    "    ## -- Using Type\n",
    "    mssg                            : Optional [ ut.Mssg ]          = None,\n",
    "    ## Encoder \n",
    "    enc                             : Optional [ Encoder ]          = None,\n",
    "    ## -- Using all parameters\n",
    "    num_epochs                      : Optional [ int]               = 3,\n",
    "    enc_learn                       : Optional [Learner]            = None, \n",
    "    cpu                             : Optional [ bool ]             = False,\n",
    "    to_numpy                        : Optional [ bool ]             = True,\n",
    "    #- Masking options\n",
    "    mask_stateful                   : Optional [ bool ]             = False,\n",
    "    mask_future                     : Optional [ bool ]             = False,\n",
    "    mask_sync                       : Optional [ bool ]             = False,\n",
    "    #- Loss criterions\n",
    "    metrics                         : Optional [ List [ Callable ]] = None\n",
    "):\n",
    "    enc,_ = ut._check_value(enc, None, \"enc\", Encoder, True)\n",
    "    \n",
    "    if enc is None: \n",
    "        mssg = _get_mssg(mssg, verbose, print_to_path, print_path, print_mode)\n",
    "        mssg.initial(ut.funcname())\n",
    "        mssg.print(\"About to exec _get_enc_input\")\n",
    "        enc_input = _get_enc_input(mssg, X, stride, batch_size, n_windows, n_windows_percent, validation_percent, training_percent, window_mask_percent, window_sizes, n_window_sizes, window_sizes_offset, windows_min_distance, full_dataset, enc_input)\n",
    "        mssg.print(f\"enc_input~{enc_input.shape}\")\n",
    "        mssg.print(\"About to exec _get_optimizer\")\n",
    "        optim = _get_optimizer(mssg, optim, criterion, optimizer, lr, lr_scheduler_flag, lr_scheduler_name, lr_scheduler_num_warmup_steps)\n",
    "        enc = Encoder(\n",
    "            model           = enc_learn,\n",
    "            input           = enc_input,\n",
    "            mssg            = mssg,\n",
    "            cpu             = cpu,\n",
    "            to_numpy        = to_numpy, \n",
    "            num_epochs      = num_epochs, \n",
    "            optim           = optim,\n",
    "            mask_stateful   = mask_stateful,\n",
    "            mask_future     = mask_future,\n",
    "            mask_sync       = mask_sync,\n",
    "            eval_stats_pre  = None,\n",
    "            eval_stats_post = None,\n",
    "            metrics         = metrics\n",
    "        )\n",
    "    enc.mssg.final(ut.funcname())\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc4c6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def fine_tune(\n",
    "    # Optional parameters\n",
    "    ## Encoder Input\n",
    "    ## -- Using all parammeters\n",
    "    X                               : Optional [ Union [ List [ List [ List [ float ]]], List [ float ], pd.DataFrame ] ],\n",
    "    stride                          : Optional [ int ]          = None,\n",
    "    batch_size                      : Optional [ int ]          = None,\n",
    "    n_windows                       : Optional [ int ]          = None,\n",
    "    n_windows_percent               : Optional [ float ]        = None,\n",
    "    validation_percent              : Optional [ float ]        = None, \n",
    "    training_percent                : Optional [ float ]        = None,\n",
    "    window_mask_percent             : Optional [ float ]        = None,\n",
    "    window_sizes                    : Optional [ List [int] ]   = None,\n",
    "    n_window_sizes                  : Optional [ int ]          = 1,\n",
    "    window_sizes_offset             : Optional [ int ]          = 0.05,\n",
    "    windows_min_distance            : Optional [ int ]          = 1,\n",
    "    full_dataset                    : Optional [ bool ]         = False,\n",
    "    ##-- Given by Type \n",
    "    enc_input                       : Optional [ EncoderInput ] = None,\n",
    "    # Optimizer\n",
    "    optim                           : Optional [ EncoderOptimizer ] = None,\n",
    "    ## -- Using all parameters\n",
    "    criterion                       : Optional [ _Loss ]            = torch.nn.MSELoss, \n",
    "    optimizer                                                       = None, \n",
    "    lr                              : Optional [ float ]            = 5e-5, #1e-4, \n",
    "    lr_scheduler_flag               : Optional [ bool ]             = False, \n",
    "    lr_scheduler_name               : Optional [ str ]              = \"linear\",\n",
    "    lr_scheduler_num_warmup_steps   : Optional [ int ]              = None,\n",
    "    # Mssg\n",
    "    ## -- Using all parameters\n",
    "    verbose                         : Optional[ int ]               = 0, \n",
    "    print_to_path                   : Optional[ bool ]              = False,\n",
    "    print_path                      : Optional[ str ]               = \"~/data/logs/logs.txt\",\n",
    "    print_mode                      : Optional[ str ]               = 'a',\n",
    "    ## -- Using Type\n",
    "    mssg                            : Optional [ ut.Mssg ]          = None,\n",
    "    \n",
    "    ## Encoder \n",
    "    enc                             : Optional [ Encoder ]          = None,\n",
    "    ## -- Using all parameters\n",
    "    num_epochs                      : Optional [ int]               = 3,\n",
    "    enc_learn                       : Optional [Learner]            = None, \n",
    "    cpu                             : Optional [ bool ]             = False,\n",
    "    to_numpy                        : Optional [ bool ]             = True,\n",
    "    #- Only for moment\n",
    "    use_moment_masks                : Optional [ bool ]             = False,\n",
    "    #- Masking options\n",
    "    mask_stateful                   : Optional [ bool ]             = False,\n",
    "    mask_future                     : Optional [ bool ]             = False,\n",
    "    mask_sync                       : Optional [ bool ]             = False,\n",
    "    # Non-Optional parameters\n",
    "    time_flag                       : bool          = False,\n",
    "    shot                            : bool          = True, \n",
    "    eval_pre                        : bool          = True, \n",
    "    eval_post                       : bool          = True,\n",
    "    use_wandb                       : bool          = None,\n",
    "    analysis_mode                   : str           = None,\n",
    "    norm_by_sample                  : bool          = None,\n",
    "    norm_use_single_batch           : bool          = None,\n",
    "    show_plot                       : bool          = False,\n",
    "    metrics                                         = None\n",
    "): \n",
    "    enc = _get_encoder(\n",
    "        X                               = X,\n",
    "        stride                          = stride,\n",
    "        batch_size                      = batch_size,\n",
    "        n_windows                       = n_windows,\n",
    "        n_windows_percent               = n_windows_percent,\n",
    "        validation_percent              = validation_percent,\n",
    "        training_percent                = training_percent,\n",
    "        window_mask_percent             = window_mask_percent,\n",
    "        window_sizes                    = window_sizes,\n",
    "        n_window_sizes                  = n_window_sizes,\n",
    "        window_sizes_offset             = window_sizes_offset,\n",
    "        windows_min_distance            = windows_min_distance,\n",
    "        full_dataset                    = full_dataset,\n",
    "        enc_input                       = enc_input,\n",
    "        optim                           = optim,\n",
    "        criterion                       = criterion,\n",
    "        optimizer                       = optimizer,\n",
    "        lr                              = lr,\n",
    "        lr_scheduler_flag               = lr_scheduler_flag,\n",
    "        lr_scheduler_name               = lr_scheduler_name,\n",
    "        lr_scheduler_num_warmup_steps   = lr_scheduler_num_warmup_steps,\n",
    "        verbose                         = verbose,\n",
    "        print_to_path                   = print_to_path,\n",
    "        print_path                      = print_path,\n",
    "        print_mode                      = print_mode,\n",
    "        mssg                            = mssg,\n",
    "        enc                             = enc,\n",
    "        num_epochs                      = num_epochs,\n",
    "        enc_learn                       = enc_learn,\n",
    "        cpu                             = cpu,\n",
    "        to_numpy                        = to_numpy,\n",
    "        mask_stateful                   = mask_stateful,\n",
    "        mask_future                     = mask_future,\n",
    "        mask_sync                       = mask_sync,\n",
    "        metrics                         = metrics        \n",
    ")\n",
    "    enc.mssg.initial_(\"fine_tune\")\n",
    "    enc.mssg.print(f\"Original enc_learn { enc_learn }  | Final model { enc.model }\")\n",
    "    lossess, eval_results_pre, eval_results_post, t_shots, t_shot, t_evals, t_eval = ( None, None, None, None, None, None, None )\n",
    "    enc.set_fine_tune_()\n",
    "    match enc.fine_tune_.__name__:\n",
    "        case \"fine_tune_moment_\":\n",
    "            enc.mssg.print(\"Use fine_tune_moment parameters\")\n",
    "            ( \n",
    "                lossess, eval_results_pre, eval_results_post, \n",
    "                t_shots, t_shot, t_evals, t_eval, enc.model \n",
    "            ) = enc.fine_tune_(\n",
    "                eval_pre, eval_post, shot, time_flag, use_moment_masks\n",
    "            )\n",
    "        case \"fine_tune_mvp_\":\n",
    "            enc.mssg.print(\"Use fine_tune_mvp parameters\")\n",
    "            ( \n",
    "                lossess, eval_results_pre, eval_results_post, \n",
    "                t_shots, t_shot, t_evals, t_eval, enc.model \n",
    "            ) = enc.fine_tune_(eval_pre, eval_post, shot, time_flag, use_wandb = use_wandb, analysis_mode = analysis_mode, norm_by_sample = norm_by_sample, norm_use_single_batch = norm_use_single_batch, show_plot = show_plot)\n",
    "        case _:\n",
    "            enc.mssg.print(\"Use generic fine_tune parameters\")\n",
    "            ( \n",
    "                lossess, eval_results_pre, eval_results_post, \n",
    "                t_shots, t_shot, t_evals, t_eval, enc.model \n",
    "            ) = enc.fine_tune_(eval_pre, eval_post, shot, time_flag)\n",
    "    enc.mssg.final()\n",
    "    return lossess, eval_results_pre, eval_results_post, t_shots, t_shot, t_evals, t_eval, enc.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa9a46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune__(\n",
    "    self                            : Encoder,\n",
    "    # Optional parameters\n",
    "    ## Encoder Input\n",
    "    ## -- Using all parammeters\n",
    "    X                               : Optional [ Union [ List [ List [ List [ float ]]], List [ float ], pd.DataFrame ] ],\n",
    "    stride                          : Optional [ int ]          = None,\n",
    "    batch_size                      : Optional [ int ]          = None,\n",
    "    n_windows                       : Optional [ int ]          = None,\n",
    "    n_windows_percent               : Optional [ float ]        = None,\n",
    "    validation_percent              : Optional [ float ]        = None, \n",
    "    training_percent                : Optional [ float ]        = None,\n",
    "    window_mask_percent             : Optional [ float ]        = None,\n",
    "    window_sizes                    : Optional [ List [int] ]   = None,\n",
    "    n_window_sizes                  : Optional [ int ]          = 1,\n",
    "    window_sizes_offset             : Optional [ int ]          = 0.05,\n",
    "    windows_min_distance            : Optional [ int ]          = 1,\n",
    "    full_dataset                    : Optional [ bool ]         = False,\n",
    "    ##-- Given by Type \n",
    "    enc_input                       : Optional [ EncoderInput ] = None,\n",
    "    # Optimizer\n",
    "    optim                           : Optional [ EncoderOptimizer ] = None,\n",
    "    ## -- Using all parameters\n",
    "    criterion                       : Optional [ _Loss ]            = torch.nn.MSELoss, \n",
    "    optimizer                                                       = None, \n",
    "    lr                              : Optional [ float ]            = 5e-5, #1e-4, \n",
    "    lr_scheduler_flag               : Optional [ bool ]             = False, \n",
    "    lr_scheduler_name               : Optional [ str ]              = \"linear\",\n",
    "    lr_scheduler_num_warmup_steps   : Optional [ int ]              = None,\n",
    "    # Mssg\n",
    "    ## -- Using all parameters\n",
    "    verbose                         : Optional[ int ]               = 0, \n",
    "    print_to_path                   : Optional[ bool ]              = False,\n",
    "    print_path                      : Optional[ str ]               = \"~/data/logs/logs.txt\",\n",
    "    print_mode                      : Optional[ str ]               = 'a',\n",
    "    ## -- Using Type\n",
    "    mssg                            : Optional [ ut.Mssg ]          = None,\n",
    "    # Non-Optional parameters\n",
    "    use_moment_masks                : bool          = True,\n",
    "    time_flag                       : bool          = False,\n",
    "    shot                            : bool          = True, \n",
    "    eval_pre                        : bool          = True, \n",
    "    eval_post                       : bool          = True,\n",
    "    # MVP\n",
    "    use_wandb                       : bool          = None,\n",
    "    norm_by_sample                  : bool          = None,\n",
    "    norm_use_single_batch           : bool          = None\n",
    "):\n",
    "    if self.mssg == ut.Mssg():\n",
    "        mssg = _get_mssg(mssg, verbose, print_to_path, print_path, print_mode)\n",
    "    if self.input == EncoderInput():\n",
    "        enc_input = _get_enc_input(mssg, X, stride, batch_size, n_windows, n_windows_percent, validation_percent, training_percent, window_mask_percent, window_sizes, n_window_sizes, window_sizes_offset, windows_min_distance, full_dataset, enc_input)\n",
    "    if self.optim == EncoderOptimizer():\n",
    "        optim = _get_optimizer(mssg, optim, criterion, optimizer, lr, lr_scheduler_flag, lr_scheduler_name, lr_scheduler_num_warmup_steps)\n",
    "    \n",
    "    self.input  = enc_input\n",
    "    self.mssg   = mssg \n",
    "    self.optim  = optim\n",
    "    lossess, eval_results_pre, eval_results_post, t_shots, t_shot, t_evals, t_eval = ( None, None, None, None, None, None, None )\n",
    "    self.set_fine_tune_()\n",
    "    \n",
    "    if self.fine_tune_ == fine_tune_moment_:\n",
    "        ( \n",
    "            lossess, eval_results_pre, eval_results_post, \n",
    "            t_shots, t_shot, t_evals, t_eval, self.model \n",
    "        ) = self.fine_tune_(\n",
    "            eval_pre, eval_post, shot, time_flag, use_moment_masks\n",
    "        )\n",
    "    elif self.fine_tune_ == fine_tune_mvp_:\n",
    "        ( \n",
    "            lossess, eval_results_pre, eval_results_post, \n",
    "            t_shots, t_shot, t_evals, t_eval, self.model\n",
    "        ) = self.fine_tune_(\n",
    "            eval_pre, eval_post, shot, time_flag, use_wandb, norm_by_sample, norm_use_single_batch\n",
    "        )\n",
    "    else:\n",
    "        ( \n",
    "            lossess, eval_results_pre, eval_results_post, \n",
    "            t_shots, t_shot, t_evals, t_eval, self.model\n",
    "        ) = self.fine_tune_(eval_pre, eval_post, shot, time_flag)\n",
    "    mssg.final()\n",
    "    return lossess, eval_results_pre, eval_results_post, t_shots, t_shot, t_evals, t_eval, self.model\n",
    "Encoder.fine_tune = fine_tune__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a5c9b646-2a6c-460b-bf56-e10fb740ac17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#import wandb\n",
    "#from dvats.utils import *\n",
    "#wandb_api = wandb.Api()\n",
    "#enc_artifact = wandb_api.artifact('deepvats/mvp-SWV:latest')\n",
    "#enc_learner = enc_artifact.to_obj()\n",
    "#X = torch.rand(9, 1, 48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c8101184-0680-416f-a378-998c8fd00502",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#%time\n",
    "#embs = get_enc_embs(X, enc_learner, cpu=True)\n",
    "#test_eq(embs.shape[0], X.shape[0])\n",
    "#embs.shape, embs.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e04ade89-a3ad-4358-960d-5d21806ba01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#%%time #TODO dont work with nb2py\n",
    "#embs = get_enc_embs(X, enc_learner, cpu=False, to_numpy=False)\n",
    "#test_eq(embs.shape[0], X.shape[0])\n",
    "#embs.shape, embs.__class__, embs.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e63662-c5d3-47ea-a43f-5db9a2437745",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "#%%time #TODO --> dont works with nb2py\n",
    "#embs = get_enc_embs(X, enc_learner, cpu=False, to_numpy=True)\n",
    "#test_eq(embs.shape[0], X.shape[0])\n",
    "#embs.shape, embs.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "85e0b3a8-62ee-4009-bc18-7080a61c21bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| hide\n",
    "from dvats.imports import beep\n",
    "beep(1)\n",
    "beep(1)\n",
    "beep(1)\n",
    "beep(1)\n",
    "beep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
