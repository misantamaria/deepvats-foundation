{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9ed6e6a-b0f6-40af-a4ad-3c0f071f190b",
   "metadata": {},
   "source": [
    "# MVP model fine-tune analysis using mixed windows for guessing the validation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4c450ecd-78c5-43a8-9b6f-6a52f6f791c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input parameters\n",
    "model_patch_size = 8\n",
    "verbose          = 0\n",
    "reset_kernel     = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e9204916-1bf8-4e7d-ba91-5a96ef37a503",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install --no-deps ydata_profiling\n",
    "#! pip install --no-deps dacite\n",
    "#! pip install --no-deps multimethod\n",
    "#! pip install --no-deps visions\n",
    "#! pip install --no-deps wordcloud\n",
    "#! pip install --no-deps imagehash\n",
    "#! pip install --no-deps htmlmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0844f7eb-9ef5-4a3f-927e-6a9ca512ac5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.set_device(0)\n",
    "#torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "af03fc5a-b69c-4bbf-85be-391588feff97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from dvats.all import *\n",
    "from tsai.data.preparation import SlidingWindow\n",
    "from fastcore.all import *\n",
    "import wandb\n",
    "wandb_api = wandb.Api()\n",
    "from yaml import load, FullLoader\n",
    "import dvats.utils as ut\n",
    "from dvats.imports import beep\n",
    "import pandas as pdz\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import ydata_profiling as ydp\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c246288c-b2a9-4da3-9bc2-4f10c7ed605e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User and project\n",
    "entity = os.environ.get(\"WANDB_ENTITY\")\n",
    "project = os.environ.get(\"WANDB_PROJECT\")\n",
    "folder = entity+'/'+project+'/'\n",
    "\n",
    "# Dataset\n",
    "dataset = 'gtrends_kohls'\n",
    "dataset_version = 'v2'\n",
    "\n",
    "#dataset = 'S1'\n",
    "#dataset_version = 'v8'\n",
    "\n",
    "enc_artifact_dataset = folder + dataset + ':' + dataset_version\n",
    "\n",
    "enc_artifact_name = \"mvp:v194\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5ef4b029-cc5f-4c07-8346-74aa67c6eb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting dataset artifact:  mi-santamaria/deepvats/gtrends_kohls:v2\n"
     ]
    }
   ],
   "source": [
    "print(\"Getting dataset artifact: \", enc_artifact_dataset)\n",
    "df_artifact = wandb_api.artifact(enc_artifact_dataset, type = 'dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "08ebdf53-b6f2-477a-a4ec-ffc7c11a53e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gtrends_kohls:v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2004-01-01</th>\n",
       "      <td>0.010417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-08</th>\n",
       "      <td>0.010417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-15</th>\n",
       "      <td>0.010417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-22</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-29</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              volume\n",
       "2004-01-01  0.010417\n",
       "2004-01-08  0.010417\n",
       "2004-01-15  0.010417\n",
       "2004-01-22  0.000000\n",
       "2004-01-29  0.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(440, 1)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_artifact.name)\n",
    "df = df_artifact.to_df()\n",
    "display(df.head())\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c0861ab3-cfd1-4e2c-9e1f-1efe33782d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_input = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9293d580-8121-497a-9de7-beb97ca61bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dvats.config as cfg_\n",
    "user, project, version, data, config, job_type = cfg_.get_artifact_config_MVP(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "626867e3-47a6-4f9b-bfb4-e152468a3d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cases: 72\n"
     ]
    }
   ],
   "source": [
    "n_epochs_list     = [5, 10, 20]\n",
    "dataset_percents  = [0.25, 0.5, 0.75, 1] #1 No tendrÃ­a sentido porque serÃ­a como hacer lo mismo que con mvp. entrenar con todo el dataset.\n",
    "masked_percents = [0.25, 0.5, 0.75]\n",
    "sizes             = [1, 5]\n",
    "total_cases = len(n_epochs_list)*len(dataset_percents)*len(masked_percents)*len(sizes)\n",
    "print(f\"Total cases: {total_cases}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f3ddbb29-7618-4ac2-972f-60b8a99ef071",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_results_small = 'results_mvp_1'\n",
    "file_errors_small = 'errors_mvp_1'\n",
    "file_errors_small = file_errors_small+dataset+version+\".csv\"\n",
    "file_results_small = file_results_small+dataset+version+\".csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "235ec64a-827b-4912-88c3-b4cdabd46a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dvats.encoder import _set_enc_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd6197b-c666-445e-a8cb-c288668374cb",
   "metadata": {},
   "source": [
    "## Preparing the DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a70021d4-ec15-4880-8369-ef8d0f23ad72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.data.core import DataLoaders\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "50c7a6a6-bb95-4b4e-ae9d-d3b737858582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_input.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "641b1282-ba3d-4a4c-a23b-037141121661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8] [ --> _set_enc_input ]\n",
      "[8] \u001b[91m [ _set_enc_input ] is none enc_input? True\u001b[0m\n",
      "[8] \u001b[91m [ _set_enc_input ] About to get the windows\u001b[0m\n",
      "[8] [ --> windowed_dataset ]\n",
      "Initialize Windowed Dataset\n",
      "[8]  [ _set_enc_input ] Train size: 308\n",
      "[8]  [ _set_enc_input ] Valid size: 8\n",
      "[8]  [ _set_enc_input ] X is a DataFrame, X~(440, 1) | window_sizes 1, n_window_sizes 1\n",
      "[8]  [ windowed_dataset ] X is a DataFrame | Window sizes: 1\n",
      "[8]  [ windowed_dataset ] Building the windows\n",
      "[8] [windowed_dataset --> ]\n",
      "[8] \u001b[91m [ _set_enc_input ] About to get the encoder input\u001b[0m\n",
      "[8] [_set_enc_input --> ]\n"
     ]
    }
   ],
   "source": [
    "mssg = ut.Mssg(verbose=8)\n",
    "enc_input, window_sizes = _set_enc_input(\n",
    "    mssg = mssg, \n",
    "    X = df, \n",
    "    stride = 1,\n",
    "    batch_size = 16,\n",
    "    validation_percent = 0.3,\n",
    "    training_percent = 0.7,\n",
    "    window_mask_percent = 0.75,\n",
    "    window_sizes = [17],\n",
    "    n_window_sizes = 1,\n",
    "    full_dataset = True,\n",
    "    mix_windows = True,\n",
    "    cpu = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "66bf296f-7481-45bd-ab1a-91deb318e144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([16, 1, 17])\n",
      "Shape: torch.Size([16, 1, 17])\n",
      "Shape: torch.Size([16, 1, 17])\n",
      "Shape: torch.Size([16, 1, 17])\n",
      "Shape: torch.Size([16, 1, 17])\n",
      "Shape: torch.Size([16, 1, 17])\n",
      "Shape: torch.Size([16, 1, 17])\n",
      "Shape: torch.Size([4, 1, 17])\n"
     ]
    }
   ],
   "source": [
    "for batch in enc_input.data.valid_batches():\n",
    "    print(f\"Shape: {batch.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "0273bfe0-b8c8-4c66-9654-05e64bb0d935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dl_shapes(dl):\n",
    "    for i, batch in enumerate(dl):\n",
    "        print(f\"Batch {i+1}: {type(batch)}\")\n",
    "        if isinstance(batch, (list, tuple)):  # Para comprobar si es una tupla de (inputs, labels)\n",
    "            print(f\"  Input shape: {batch[0].shape}\")\n",
    "            if len(batch) > 1:\n",
    "                print(f\"  Target shape: {batch[1].shape}\")\n",
    "        else:\n",
    "            print(f\"  Shape: {batch.shape}\")  # Si batch no es una tupla/lista\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "8aef8874-0b39-4c82-bd22-e4a0bb8b9114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class IndexedDataset(Dataset):\n",
    "    def __init__(self, batches):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # ðŸ”¹ Verificar si `batches` no estÃ¡ vacÃ­o\n",
    "        if not batches:\n",
    "            raise ValueError(\"âš  Error: `batches` estÃ¡ vacÃ­o. No se puede crear un dataset sin datos.\")\n",
    "\n",
    "        # ðŸ”¹ Convertir los batches a `cuda` o `cpu`\n",
    "        self.batches = [batch.to(self.device, dtype=torch.float32) for batch in batches]\n",
    "\n",
    "        # ðŸ”¹ Verificar el primer batch\n",
    "        print(f\"\\nðŸ“Œ IndexedDataset creado con {len(self.batches)} batches\")\n",
    "        for i, batch in enumerate(self.batches):\n",
    "            print(f\"ðŸ”¹ IndexedDataset Batch {i}: Shape {batch.shape}, Device {batch.device}\")\n",
    "\n",
    "\n",
    "        # ðŸ”¹ Verificar si el Ãºltimo batch es mÃ¡s pequeÃ±o y eliminarlo si es necesario\n",
    "        if len(self.batches) > 1 and self.batches[-1].shape[0] < self.batches[0].shape[0]:\n",
    "            print(\"âš  Ãšltimo batch mÃ¡s pequeÃ±o. Eliminando para evitar errores.\")\n",
    "            self.batches = self.batches[:-1]\n",
    "\n",
    "        # ðŸ”¹ Confirmar si los tensores estÃ¡n en CUDA\n",
    "        if self.batches[0].is_cuda:\n",
    "            print(\"âœ… Los tensores estÃ¡n correctamente en CUDA.\")\n",
    "        else:\n",
    "            print(\"âŒ ERROR: Los tensores no estÃ¡n en CUDA.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batches)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch = self.batches[idx]\n",
    "\n",
    "        # ðŸ”¹ Solo devolver el batch (sin `None`)\n",
    "        print(f\"\\nðŸ“Œ Batch {idx} - Shape: {batch.shape}, Dispositivo: {batch.device}\")\n",
    "        return batch  # ðŸš€ Fastai espera solo el tensor, no una tupla\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "4f677ae7-b8a1-47ec-b094-1b6ceafcb9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class FlattenedDataset(Dataset):\n",
    "    def __init__(self, batches):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        if not batches:\n",
    "            raise ValueError(\"âš  Error: `batches` estÃ¡ vacÃ­o. No se puede crear un dataset sin datos.\")\n",
    "\n",
    "        self.batches = [batch.to(self.device, dtype=torch.float32) for batch in batches]\n",
    "\n",
    "        if len(self.batches) > 1 and self.batches[-1].shape[0] < self.batches[0].shape[0]:\n",
    "            print(\"âš  Ãšltimo batch mÃ¡s pequeÃ±o. Eliminando para evitar errores.\")\n",
    "            self.batches = self.batches[:-1]\n",
    "\n",
    "        # ðŸ”¹ Aplanar manteniendo la estructura original\n",
    "        self.samples = [(x,) for batch in self.batches for x in batch]  # ðŸ”¹ DEVOLVER TUPLAS\n",
    "\n",
    "        if self.samples[0][0].is_cuda:\n",
    "            print(\"âœ… Los tensores estÃ¡n correctamente en CUDA.\")\n",
    "        else:\n",
    "            print(\"âŒ ERROR: Los tensores no estÃ¡n en CUDA.\")\n",
    "\n",
    "        print(f\"\\nðŸ“Œ FlattenedDataset creado con {len(self.samples)} muestras\")\n",
    "        print(f\"ðŸ“Œ Shape de una muestra: {self.samples[0][0].shape}\")  \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]  # ðŸ”¹ DEVOLVER TUPLAS DIRECTAMENTE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "b4510846-4f18-4996-b1ef-0659b65bd59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------ START INSPECTION ----------------------------------------\n",
      "--- Get batches ---\n",
      "--- Check valid batches ...\n",
      "8\n",
      "torch.Size([16, 1, 17])\n",
      "--- Convert to IndexedDataset ---\n",
      "âš  Ãšltimo batch mÃ¡s pequeÃ±o. Eliminando para evitar errores.\n",
      "âœ… Los tensores estÃ¡n correctamente en CUDA.\n",
      "\n",
      "ðŸ“Œ FlattenedDataset creado con 288 muestras\n",
      "ðŸ“Œ Shape de una muestra: torch.Size([1, 1, 17])\n",
      "âš  Ãšltimo batch mÃ¡s pequeÃ±o. Eliminando para evitar errores.\n",
      "âœ… Los tensores estÃ¡n correctamente en CUDA.\n",
      "\n",
      "ðŸ“Œ FlattenedDataset creado con 112 muestras\n",
      "ðŸ“Œ Shape de una muestra: torch.Size([1, 1, 17])\n",
      "---- Create dataloader ---\n",
      "\n",
      "---- Verificando FastaiDataLoader ----\n",
      "---- Train---\n",
      "Batch 1: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "Batch 2: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "Batch 3: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "Batch 4: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "Batch 5: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "Batch 6: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "Batch 7: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "Batch 8: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "Batch 9: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "Batch 10: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "Batch 11: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "Batch 12: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "Batch 13: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "Batch 14: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "Batch 15: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "Batch 16: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "Batch 17: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "Batch 18: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "---- Valid---\n",
      "Batch 1: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "Batch 2: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "Batch 3: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "Batch 4: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "Batch 5: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "Batch 6: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "Batch 7: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "---- Create dataloaders ---\n",
      "---- Dataloaders valid -----\n",
      "Batch 1: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "Batch 2: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "Batch 3: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "Batch 4: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "Batch 5: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "Batch 6: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "Batch 7: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "---- Dataloaders train -----\n",
      "Batch 1: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "Batch 2: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "Batch 3: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "Batch 4: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "Batch 5: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "Batch 6: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "Batch 7: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "Batch 8: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "Batch 9: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "Batch 10: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "Batch 11: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "Batch 12: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "Batch 13: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "Batch 14: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "Batch 15: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "Batch 16: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "Batch 17: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "Batch 18: <class 'tuple'>\n",
      "  Input shape: torch.Size([16, 1, 1, 17])\n",
      "------------------------------------------ END OF INSPECTION ----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------------------------------ START INSPECTION ----------------------------------------\")\n",
    "print(\"--- Get batches ---\")\n",
    "# ðŸ”¹ Convertir el generador en una lista de tensores y asegurarse de que estÃ¡n en `cuda:0`\n",
    "train_batches = [batch.to(device, dtype=torch.float32) for batch in enc_input.data.train_batches()]\n",
    "valid_batches = [batch.to(device, dtype=torch.float32) for batch in enc_input.data.valid_batches()]\n",
    "print(\"--- Check valid batches ...\")\n",
    "print(len(valid_batches))\n",
    "print(valid_batches[0].shape)\n",
    "print(\"--- Convert to IndexedDataset ---\")\n",
    "# ðŸ”¹ Crear datasets indexables con datos en CUDA/CPU segÃºn corresponda\n",
    "#train_dataset = IndexedDataset(train_batches)\n",
    "#valid_dataset = IndexedDataset(valid_batches)\n",
    "train_dataset = FlattenedDataset(train_batches)\n",
    "valid_dataset = FlattenedDataset(valid_batches)\n",
    "\n",
    "# ðŸ”¹ Crear DataLoaders compatibles con fastai\n",
    "print(\"---- Create dataloader ---\")\n",
    "train_loader = FastaiDataLoader(\n",
    "    train_dataset, \n",
    "    bs=16,  \n",
    "    shuffle=True#, \n",
    "    #collate_fn=lambda x: tuple(zip(*x))  # ðŸ”¹ Esto mantiene la estructura de tupla\n",
    ")\n",
    "\n",
    "valid_loader = FastaiDataLoader(\n",
    "    valid_dataset, \n",
    "    bs=16,  \n",
    "    shuffle=False#, \n",
    "    #collate_fn=lambda x: tuple(zip(*x))  # ðŸ”¹ Igual para validaciÃ³n\n",
    ")\n",
    "print(\"\\n---- Verificando FastaiDataLoader ----\")\n",
    "print(\"---- Train---\")\n",
    "dl_shapes(train_loader)\n",
    "\n",
    "print(\"---- Valid---\")\n",
    "dl_shapes(valid_loader)\n",
    "\n",
    "\n",
    "print(\"---- Create dataloaders ---\")\n",
    "# ðŸ”¹ Crear el DataLoaders final en fastai\n",
    "dls = DataLoaders(to_device(train_loader, device), to_device(valid_loader, device))\n",
    "print(\"---- Dataloaders valid -----\")\n",
    "dl_shapes(dls.valid)\n",
    "print(\"---- Dataloaders train -----\")\n",
    "dl_shapes(dls.train)\n",
    "print(\"------------------------------------------ END OF INSPECTION ----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a391fe5d-db0b-4d6e-bb25-2dd23df0c95e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7fa9944a2260>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls.valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf212ab4-12ee-4542-923c-5c5f594fc717",
   "metadata": {},
   "source": [
    "## Validate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0ea9e9b1-6fc1-47a4-89ae-c3c9977ac24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    }
   ],
   "source": [
    "enc_artifact = wandb_api.artifact(enc_artifact_name, type = 'learner')\n",
    "enc = enc_artifact.to_obj()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "bbc5457c-ad11-4be7-9c2f-2e3818d6a1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_bkup = deepcopy(enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "7e3c8c02-4674-4829-9dc5-e5a4b0f7d8d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from fastai.data.core import DataLoaders\n",
    "from fastai.data.load import DataLoader as FastaiDataLoader\n",
    "def validate_in_device():\n",
    "    # ðŸ”¹ Obtener el dispositivo del modelo\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # ðŸ”¹ Mover explÃ­citamente `enc.model` a CUDA, asegurando que no lo regrese a CPU en validaciÃ³n\n",
    "    enc.model.to(device)\n",
    "    for module in enc.model.modules():\n",
    "        module.to(device)\n",
    "    for name, param in enc.model.named_parameters():\n",
    "        param.data = param.data.to(device, dtype=torch.float32)\n",
    "    for name, buffer in enc.model.named_buffers():\n",
    "        buffer.data = buffer.data.to(device, dtype=torch.float32)\n",
    "\n",
    "    # ðŸ”¹ Verificar los pesos y buffers del modelo para asegurar que estÃ¡n en CUDA y dtype `torch.float32`\n",
    "    print(\"\\nðŸ“Œ **Verificando pesos del modelo**\")\n",
    "    for name, param in enc.model.named_parameters():\n",
    "        print(f\"{name}: {param.device}, dtype={param.dtype}\")\n",
    "\n",
    "    print(\"\\nðŸ“Œ **Verificando buffers del modelo**\")\n",
    "    for name, buffer in enc.model.named_buffers():\n",
    "        print(f\"{name}: {buffer.device}, dtype={buffer.dtype}\")\n",
    "\n",
    "    # ðŸ”¹ Verificar los datos del DataLoader\n",
    "    for xb in dls.valid:\n",
    "        xb = xb[0] if isinstance(xb, tuple) else xb  # ðŸ”¹ Desempaquetar si es una tupla\n",
    "        xb = xb.to(device, dtype=torch.float32)  # ðŸ”¹ Asegurar que el tensor estÃ¡ en CUDA con el tipo correcto\n",
    "        print(f\"\\nðŸ“Œ **Verificando los datos del DataLoader:** Tipo de tensor: {xb.dtype}, Dispositivo: {xb.device}\")\n",
    "        break  # Solo mostrar un batch para prueba\n",
    "\n",
    "    # ðŸ”¹ Ejecutar validaciÃ³n asegurando que **todo** estÃ¡ en CUDA con `torch.float32`\n",
    "    enc.model.to(device)  # ðŸš¨ Asegurar que `enc.model` sigue en CUDA antes de validar\n",
    "    with torch.no_grad():\n",
    "        for xb in dls.valid:\n",
    "            xb = xb[0] if isinstance(xb, tuple) else xb  # ðŸ”¹ Desempaquetar si es necesario\n",
    "            xb = xb.to(device, dtype=torch.float32)  # ðŸ”¹ Mover a `cuda:0` antes de la validaciÃ³n\n",
    "            print(f\"\\nðŸ“Œ Pasando xb al modelo con tipo: {xb.dtype}, dispositivo: {xb.device}\")\n",
    "\n",
    "            result = enc.validate(1, dls.valid, None)  # ðŸš€ Ejecutar validaciÃ³n\n",
    "            print(\"\\nâœ… **ValidaciÃ³n completada correctamente:**\", result)\n",
    "#validate_in_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cc4a78-f3e6-4394-8b92-2ea871dc5c6b",
   "metadata": {},
   "source": [
    "### Move to CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "c032f4d0-22a5-429d-897f-1b01d1d3f993",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Todo ha sido movido a CPU correctamente.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "check = False\n",
    "def move_to_cpu(model, dataloaders):\n",
    "    \"\"\" Mueve el modelo y los datos en los DataLoaders a CPU sin asumir targets. \"\"\"\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "    # ðŸ”¹ Mover modelo a CPU\n",
    "    model.to(device)\n",
    "    \n",
    "    # ðŸ”¹ Mover pesos y buffers explÃ­citamente\n",
    "    for name, param in model.named_parameters():\n",
    "        param.data = param.data.to(device)\n",
    "    for name, buffer in model.named_buffers():\n",
    "        buffer.data = buffer.data.to(device)\n",
    "\n",
    "    # ðŸ”¹ Mover los datos en los DataLoaders a CPU\n",
    "    def to_cpu_batch(batch):\n",
    "        if isinstance(batch, (list, tuple)):\n",
    "            return [x.to(device) if isinstance(x, torch.Tensor) else x for x in batch]\n",
    "        elif isinstance(batch, torch.Tensor):\n",
    "            return batch.to(device)\n",
    "        return batch\n",
    "\n",
    "    # ðŸ”¹ Modificar DataLoader sin asumir `y`\n",
    "    dataloaders.train = [to_cpu_batch(x) for x in dataloaders.train]\n",
    "    dataloaders.valid = [to_cpu_batch(x) for x in dataloaders.valid]\n",
    "\n",
    "    print(\"âœ… Todo ha sido movido a CPU correctamente.\")\n",
    "\n",
    "    return model, dataloaders\n",
    "\n",
    "# ðŸ”¹ Aplicar la funciÃ³n\n",
    "enc.model, dls = move_to_cpu(enc.model, dls)\n",
    "if check:\n",
    "    # ðŸ”¹ Comprobar\n",
    "    for name, param in enc.model.named_parameters():\n",
    "        print(f\"{name}: {param.device}, {param.dtype}\")\n",
    "    for xb in dls.valid:\n",
    "        print(f\"Tipo de tensor en DataLoader: {xb.device}, {xb.dtype}\")\n",
    "        break  # Solo imprimir una muestra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6f2f26-f29c-440a-9a5b-7d7890aecb2b",
   "metadata": {},
   "source": [
    "### Validate ensuring determinism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f37bd2e7-cb75-4251-b621-a4ad84de22f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "58860ce7-0f45-4faf-b773-476d4ff80e84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m  \u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m Validate on `dl` with potential new `cbs`.\n",
       "\u001b[0;31mFile:\u001b[0m      /usr/local/share/miniconda3/envs/env/lib/python3.10/site-packages/fastai/learner.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "? enc.validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "cb21af0b-32a4-47ba-880b-7592d3379c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Todo ha sido movido a CPU correctamente.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/2 00:00&lt;?]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not enough values to plot a chart\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "MVP._loss() takes 3 positional arguments but 19 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[182], line 62\u001b[0m\n\u001b[1;32m     60\u001b[0m learn \u001b[38;5;241m=\u001b[39m deepcopy(learn_bkup)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 62\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mmvp_validate_determinist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdls\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteraciÃ³n \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Loss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, result)\n",
      "Cell \u001b[0;32mIn[182], line 39\u001b[0m, in \u001b[0;36mmvp_validate_determinist\u001b[0;34m(learn, idx, cbs, dl)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# ðŸš€ Ejecutar validaciÃ³n en modo determinista\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m---> 39\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mlearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# ðŸ”„ Restaurar la variable de entorno original\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m old_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/share/miniconda3/envs/env/lib/python3.10/site-packages/fastai/learner.py:278\u001b[0m, in \u001b[0;36mLearner.validate\u001b[0;34m(self, ds_idx, dl, cbs)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate\u001b[39m(\u001b[38;5;28mself\u001b[39m, ds_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cbs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: dl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdls[ds_idx]\n\u001b[0;32m--> 278\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_context(cbs\u001b[38;5;241m=\u001b[39mcbs): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_epoch_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_record\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/share/miniconda3/envs/env/lib/python3.10/site-packages/fastai/learner.py:244\u001b[0m, in \u001b[0;36mLearner._do_epoch_validate\u001b[0;34m(self, ds_idx, dl)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: dl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdls[ds_idx]\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl \u001b[38;5;241m=\u001b[39m dl\n\u001b[0;32m--> 244\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalidate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelValidException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/share/miniconda3/envs/env/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m/usr/local/share/miniconda3/envs/env/lib/python3.10/site-packages/fastai/learner.py:205\u001b[0m, in \u001b[0;36mLearner.all_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mall_batches\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl)\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/share/miniconda3/envs/env/lib/python3.10/site-packages/tsai/learner.py:40\u001b[0m, in \u001b[0;36mone_batch\u001b[0;34m(self, i, b)\u001b[0m\n\u001b[1;32m     38\u001b[0m b_on_device \u001b[38;5;241m=\u001b[39m to_device(b, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdls\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdls\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m b\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split(b_on_device)\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_one_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelBatchException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/share/miniconda3/envs/env/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m/usr/local/share/miniconda3/envs/env/lib/python3.10/site-packages/fastai/learner.py:219\u001b[0m, in \u001b[0;36mLearner._do_one_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_pred\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myb):\n\u001b[0;32m--> 219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43myb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_grad\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: MVP._loss() takes 3 positional arguments but 19 were given"
     ]
    }
   ],
   "source": [
    "def mvp_validate_determinist(learn, idx=1, cbs=None, dl = None):\n",
    "    \"\"\"Ejecuta una validaciÃ³n determinista en CUBLAS y restaura el modelo despuÃ©s.\"\"\"\n",
    "\n",
    "    # ðŸ”„ Guardar estado inicial del modelo sin `inference_mode()`\n",
    "    initial_state = {k: v.clone().detach() for k, v in learn.model.state_dict().items()}\n",
    "\n",
    "    # ðŸ”„ Guardar especÃ­ficamente `head.1` para restaurarlo manualmente\n",
    "    head1_weight_before = learn.model.head[1].weight.clone().detach()\n",
    "    head1_bias_before = learn.model.head[1].bias.clone().detach()\n",
    "\n",
    "    # ðŸ”„ Fijar semillas para asegurar reproducibilidad\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "\n",
    "    # ðŸ”„ Desactivar BatchNorm completamente (evita updates en `running_mean`)\n",
    "    for module in learn.model.modules():\n",
    "        if isinstance(module, (torch.nn.BatchNorm1d, torch.nn.BatchNorm2d)):\n",
    "            module.track_running_stats = False  # No acumular stats\n",
    "            module.momentum = 0  # Asegurar que no haya drift en estadÃ­sticas\n",
    "            module.eval()\n",
    "\n",
    "    # ðŸ”„ Asegurar que el `valid_dl` no tenga shuffle ni drop_last\n",
    "    learn.dls.valid.shuffle_fn = lambda x: x  # Desactivar mezcla de datos\n",
    "    learn.dls.valid.drop_last = False\n",
    "\n",
    "    # ðŸ”„ Poner el modelo en modo evaluaciÃ³n\n",
    "    learn.model.eval()\n",
    "\n",
    "    # ðŸ”„ Guardar configuraciÃ³n de CUBLAS antes de modificarla\n",
    "    old_value = os.environ.get(\"CUBLAS_WORKSPACE_CONFIG\")\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"  # Activar determinismo\n",
    "\n",
    "    try:\n",
    "        # ðŸš€ Ejecutar validaciÃ³n en modo determinista\n",
    "        with torch.no_grad(), torch.inference_mode():\n",
    "            result = learn.validate(ds_idx = idx, cbs = cbs, dl = dl)\n",
    "    finally:\n",
    "        # ðŸ”„ Restaurar la variable de entorno original\n",
    "        if old_value is not None:\n",
    "            os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = old_value\n",
    "        else:\n",
    "            del os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]\n",
    "\n",
    "        # ðŸ”„ Restaurar `head.1` manualmente evitando problemas con `inference_mode`\n",
    "        for name, param in learn.model.named_parameters():\n",
    "            if name == \"head.1.weight\":\n",
    "                param.data = head1_weight_before.clone().to(param.device)\n",
    "            elif name == \"head.1.bias\":\n",
    "                param.data = head1_bias_before.clone().to(param.device)\n",
    "            elif name in initial_state:\n",
    "                param.data = initial_state[name].clone().to(param.device)\n",
    "\n",
    "    return result  # ðŸ“Š Devolver resultado de la validaciÃ³n\n",
    "\n",
    "enc.model, dls = move_to_cpu(enc.model, dls)\n",
    "# ðŸ”„ Probar la validaciÃ³n varias veces\n",
    "learn = deepcopy(learn_bkup)\n",
    "for i in range(10):\n",
    "    result = mvp_validate_determinist(learn, 1, None, dls)\n",
    "    print(f\"IteraciÃ³n {i+1} - Loss:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1963b85e-baed-4351-805a-14bcfe0bea08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_layers(learn):\n",
    "    # ANSI escape codes para color rojo y negrita en la consola\n",
    "    RED = \"\\033[91m\"\n",
    "    BOLD = \"\\033[1m\"\n",
    "    RESET = \"\\033[0m\"\n",
    "\n",
    "    seen_layers = set()\n",
    "    param_dict = {name: param.shape for name, param in learn.model.named_parameters()}\n",
    "\n",
    "    def recurse_layers(module, prefix=\"\", depth=0):\n",
    "        \"\"\"Recorre todas las capas del modelo y muestra solo sus parÃ¡metros directos.\"\"\"\n",
    "        for name, layer in module.named_children():  # Obtiene todos los submÃ³dulos\n",
    "            full_name = f\"{prefix}.{name}\" if prefix else name\n",
    "\n",
    "            # Aplicar negrita solo la primera vez que aparece un nombre de capa\n",
    "            formatted_name = f\"{BOLD}{name}{RESET}\" if full_name not in seen_layers else name\n",
    "            seen_layers.add(full_name)\n",
    "\n",
    "            indentation = \"\\t\" * depth  # Agregar tabulaciÃ³n segÃºn la profundidad\n",
    "            \n",
    "            # Filtrar solo los parÃ¡metros **directamente dentro** de esta capa\n",
    "            param_info = [\n",
    "                f\"{n.split('.')[-1]}: {param_dict[n]}\"\n",
    "                for n in param_dict\n",
    "                if n.startswith(full_name) and n.count('.') == full_name.count('.') + 1\n",
    "            ]\n",
    "            param_text = f\" | ParÃ¡metros: {', '.join(param_info)}\" if param_info else \"\"\n",
    "\n",
    "            print(f\"{indentation}[{depth}] Capa {formatted_name} {RED}[{type(layer).__name__}]{RESET}{param_text}\")\n",
    "\n",
    "            # Recursivamente mostrar las subcapas\n",
    "            recurse_layers(layer, full_name, depth + 1)\n",
    "\n",
    "    # Iniciar el recorrido desde el modelo completo\n",
    "    recurse_layers(learn.model)\n",
    "\n",
    "check_layers(learn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49af55ae-2793-4d6c-b455-9e564d5e35fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4996361c-cb3b-4335-b9a4-d6a45598fe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.dls.valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717cf48f-5df1-4331-aa21-04bbb70a75e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14af730-d7f2-4715-be3c-acc064d5b57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cases_loop(\n",
    "    model, \n",
    "    n_epochs_list, \n",
    "    dataset_percents, \n",
    "    masked_percents, \n",
    "    n_sizes_list, \n",
    "    summarized = True, \n",
    "    do_beep = True, \n",
    "    verbose = 1,\n",
    "    save = True,\n",
    "    file_errors = \"\",\n",
    "    file_results = \"\"\n",
    "):\n",
    "    mssg = ut.Mssg(verbose = verbose, level = -1)\n",
    "    result_columns = [\n",
    "        'model_size','n_epochs','dataset_percent','masked_percent','n_windows', \n",
    "        'time',\n",
    "        'first_train_loss','first_mse','first_rmse','first_mae','first_smape', \n",
    "        'last_train_loss','last_mse','last_rmse','last_mae','last_smape'\n",
    "    ]\n",
    "    result_columns = result_columns if summarized else result_columns + ['losses','eval_results_pre','eval_results_post']\n",
    "    results = pd.DataFrame(columns = result_columns)\n",
    "    \n",
    "    errors = pd.DataFrame(\n",
    "        columns = [\n",
    "            'model_size',\n",
    "            'n_epochs',\n",
    "            'dataset_percent',\n",
    "            'masked_percent',\n",
    "            'n_windows',\n",
    "            'windows',\n",
    "            'error'\n",
    "        ]\n",
    "    )\n",
    "    model_backup = deepcopy(model)\n",
    "    i = 0\n",
    "    for n_epochs in n_epochs_list:\n",
    "        for dataset_percent in dataset_percents:\n",
    "            print(dataset_percent)\n",
    "            for masked_percent in masked_percents:\n",
    "                model.mask_generator = Masking(mask_ratio = masked_percent)\n",
    "                for sizes in n_sizes_list:\n",
    "                    print(f\"--> epoch {n_epochs}, dataset_percent {dataset_percent}, mask {masked_percent}\")\n",
    "                    redmssg = f\" sizes {sizes}\"\n",
    "                    redmssg = f\"\\033[91m{redmssg}\\033[0m\"\n",
    "                    print(redmssg)\n",
    "\n",
    "                    print(f\"Cuda memmory allocated: {torch.cuda.memory_allocated()}\")\n",
    "                    model_case = deepcopy(model_backup)\n",
    "                    case = {\n",
    "                            'model_size': \"small\",\n",
    "                            'n_epochs': n_epochs,\n",
    "                            'dataset_percent': dataset_percent,\n",
    "                            'masked_percent': masked_percent,\n",
    "                            'n_windows': sizes,\n",
    "                            'windows': None\n",
    "                           }\n",
    "                    result_dict = deepcopy(case)\n",
    "                    error_dict = deepcopy(case)\n",
    "                    error = False\n",
    "                    print(1-dataset_percent)\n",
    "                    torch.cuda.synchronize()\n",
    "                    result = fine_tune(\n",
    "                        enc_learn           = model_case,\n",
    "                        window_mask_percent = masked_percent,\n",
    "                        training_percent    = dataset_percent,\n",
    "                        validation_percent  = 0.3,\n",
    "                        num_epochs          = n_epochs,\n",
    "                        n_window_sizes      = sizes,\n",
    "                        verbose             = verbose,\n",
    "                        register_errors     = True,\n",
    "                        save_best_or_last   = True, # only available for moment,\n",
    "                        #force_best_lr       = True,\n",
    "                        **common_args    \n",
    "                    )\n",
    "                    common_args['print_mode']='a'\n",
    "\n",
    "                    default_error = pd.DataFrame([{\n",
    "                        'model_size': case['model_size'], \n",
    "                        'n_epochs': case['n_epochs'],\n",
    "                        'dataset_percent': case['dataset_percent'],\n",
    "                        'masked_percent': case['masked_percent'],\n",
    "                        'n_windows': sizes,\n",
    "                        'windows': \"Unknown\",\n",
    "                        'error': 'Non registered error',\n",
    "                        'window': \"Unknown\"\n",
    "                    }])\n",
    "\n",
    "                    try:\n",
    "                        print(\"---- Returned internal errors ---\")\n",
    "                        internal_errors = result[10]\n",
    "                        \n",
    "                    except:\n",
    "                        internal_errors = default_error\n",
    "            \n",
    "                    print(\"Check:\", result[0])\n",
    "                    if len(result[0]) > 0:\n",
    "                    \n",
    "                        result_dict.update({\n",
    "                            'time'             : result[4],\n",
    "                            'windows'          : result[8].cpu() if isinstance(result[8], torch.Tensor) else result[8],\n",
    "                            'first_train_loss' : result[0][0][0].cpu().item() if torch.is_tensor(result[0][0][0]) else result[0][0][0],\n",
    "                            'last_train_loss'  : result[0][-1][-1].cpu().item() if torch.is_tensor(result[0][-1][-1]) else result[0][-1][-1],\n",
    "                            'best_epochs'       : result[9],\n",
    "                            'train_losses'      : result[0][0],\n",
    "                            'eval_pre'          : result[1],\n",
    "                            'eval_post'         : result[2],\n",
    "                            'full_result'       : result\n",
    "                        })\n",
    "                        if result[1] == {}:\n",
    "                            result_dict.update({\n",
    "                                'first_eval_loss'  : np.nan,\n",
    "                                'first_mse'        : np.nan,\n",
    "                                'first_rmse'       : np.nan,\n",
    "                                'first_mae'        : np.nan\n",
    "                            })\n",
    "                        else:\n",
    "                            print(\"N windows: \", len(result[8]))\n",
    "                            print(\"Loss: \", result[1]['loss'])\n",
    "                            result_dict.update({\n",
    "                                'first_eval_loss'  : result[1]['loss'][-1].cpu().item() if torch.is_tensor(result[1]['loss']) else result[1]['loss'][-1],\n",
    "                                'first_mse'        : result[1]['mse'][-1].cpu().item() if torch.is_tensor(result[1]['mse']) else result[1]['mse'][-1],    \n",
    "                                'first_rmse'       : result[1]['rmse'][-1].cpu().item() if torch.is_tensor(result[1]['rmse']) else result[1]['rmse'][-1],\n",
    "                                'first_mae'        : result[1]['mae'][-1].cpu().item() if torch.is_tensor(result[1]['mae']) else result[1]['mae'][-1],                                \n",
    "                                'first_smape'      : result[1]['smape'].cpu().item() if torch.is_tensor(result[1]['smape']) else result[1]['smape']\n",
    "                            })\n",
    "                        if result[2] == {}:\n",
    "                            result_dict.update({\n",
    "                                'last_eval_loss'  : np.nan,\n",
    "                                'last_mse'        : np.nan,\n",
    "                                'last_rmse'       : np.nan,\n",
    "                                'last_mae'        : np.nan\n",
    "                            })\n",
    "                        else:\n",
    "                            result_dict.update({\n",
    "                                'last_eval_loss'   : result[2]['loss'][-1].cpu().item() if torch.is_tensor(result[2]['loss'][-1]) else result[2]['loss'][-1],\n",
    "                                'last_mse'         : result[2]['mse'][-1].cpu().item() if torch.is_tensor(result[2]['mse'][-1]) else result[2]['mse'][-1],\n",
    "                                'last_rmse'        : result[2]['rmse'][-1].cpu().item() if torch.is_tensor(result[2]['rmse'][-1]) else result[2]['rmse'][-1],\n",
    "                                'last_mae'         : result[2]['mae'][-1].cpu().item() if torch.is_tensor(result[2]['mae'][-1]) else result[2]['mae'][-1],\n",
    "                                'last_smape'       : result[2]['smape'][-1].cpu().item() if torch.is_tensor(result[2]['smape'][-1]) else result[2]['smape'][-1]\n",
    "                            })\n",
    "                            \n",
    "        \n",
    "                        if not summarized:\n",
    "                            result_dict.update({\n",
    "                                'losses'           : [[v.cpu().item() if torch.is_tensor(v) else v for v in loss] for loss in result[0]],\n",
    "                                'eval_results_pre' : {k: v.cpu().item() if torch.is_tensor(v) else v for k, v in result[1].items()},\n",
    "                                'eval_results_post': {k: v.cpu().item() if torch.is_tensor(v) else v for k, v in result[2].items()},\n",
    "                                })  \n",
    "                        results = pd.concat([results, pd.DataFrame([result_dict])], ignore_index=True)\n",
    "                    else:\n",
    "                        print(f\"Failed case | N Errors {errors.shape[0]} | N Results { results.shape[0] }\")\n",
    "                        # Attach possible errors\n",
    "                        internal_errors['model_size'] = case['model_size']\n",
    "                        internal_errors['n_epochs'] = case['n_epochs']\n",
    "                        internal_errors['dataset_percent'] = case['dataset_percent']\n",
    "                        internal_errors['masked_percent'] = case['masked_percent']\n",
    "                        internal_errors['windows'] = [result[8]]*len(internal_errors)\n",
    "                        print(\"--- Internal ---\")\n",
    "                        if (internal_errors.empty):\n",
    "                            print(\"Returned errors empty\")\n",
    "                            internal_errors = default_error\n",
    "                        display(internal_errors)\n",
    "                        errors = pd.concat([errors, internal_errors])\n",
    "                        print(\"--- Concatenated ---\")\n",
    "                        display(errors)\n",
    "                        print(f\"Failed case | N Errors {errors.shape[0]} | N Results { results.shape[0] } \")\n",
    "                    if not error: mssg.print_error(f\" case {case} | time: {result[4]}\")\n",
    "                    before = torch.cuda.memory_allocated()\n",
    "                    model_case = None\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache()\n",
    "                    display(results)\n",
    "                    if do_beep:\n",
    "                        beep(1)\n",
    "                    mssg.print(f\"epoch {n_epochs}, dataset_percent {dataset_percent}, mask {masked_percent}, sizes {sizes} -->\")\n",
    "                if save:\n",
    "                    mssg.print(f\"Update results into {file_results}\")\n",
    "                    results.to_csv(file_results, index = False, header = True)\n",
    "                    mssg.print(f\"Update errors into {file_errors}\")\n",
    "                    errors.to_csv(file_errors, index = False, header = True)\n",
    "                if do_beep:\n",
    "                    beep(2)\n",
    "                    beep(2)\n",
    "                mssg.print(f\"epoch {n_epochs}, dataset_percent {dataset_percent}, mask {masked_percent} -->\")\n",
    "            if do_beep:\n",
    "                beep(3)\n",
    "                beep(3)\n",
    "                beep(3)\n",
    "            mssg.print(f\"epoch {n_epochs}, dataset_percent {dataset_percent}-->\")\n",
    "        if do_beep:\n",
    "            beep(4)\n",
    "            beep(4)\n",
    "            beep(4)\n",
    "            beep(4)\n",
    "        mssg.print(f\"epoch {n_epochs}-->\")\n",
    "    if do_beep:\n",
    "        beep(1000)\n",
    "        beep(1000)\n",
    "        beep(1000)\n",
    "        beep(1000)\n",
    "        beep(1000)\n",
    "    model_backup = None\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return results, errors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
