[8] [ --> _get_encoder ]
[8]  [ _get_encoder ] About to exec _get_enc_input
[8] [ --> _get_enc_input ]
[8]  [ _get_enc_input ] is none enc_input? True
[8]  [ _get_enc_input ] About to get the windows
[8] [ --> windowed_dataset ]
[8]  [ _get_enc_input ] X is a DataFrame, X~(440, 1) | window_sizes 0, n_window_sizes 5
[8]  [ _get_enc_input ] X is a DataFrame | Selecting Fourier's dominant frequences
[8] [ --> Find_dominant_window_sizes_list ]
[8]  [ Find_dominant_window_sizes_list ] X ~ (440, 1)
[8]  [ Find_dominant_window_sizes_list ] Get sizes for var 0
[8] [ --> find_dominant_window_sizes_list_single ]
[8]  [ find_dominant_window_sizes_list_single ] X ~ (440,)
[8]  [ find_dominant_window_sizes_list_single ] Looking for - at most - the best 5 window sizes
[8]  [ find_dominant_window_sizes_list_single ] Offset 0.05 max size: 22.0
[8]  [ find_dominant_window_sizes_list_single ] --> Freqs
[8]  [ find_dominant_window_sizes_list_single ] Freqs [ 0.          0.00227273  0.00454545  0.00681818  0.00909091  0.01136364
  0.01363636  0.01590909  0.01818182  0.02045455  0.02272727  0.025
  0.02727273  0.02954545  0.03181818  0.03409091  0.03636364  0.03863636
  0.04090909  0.04318182  0.04545455  0.04772727  0.05        0.05227273
  0.05454545  0.05681818  0.05909091  0.06136364  0.06363636  0.06590909
  0.06818182  0.07045455  0.07272727  0.075       0.07727273  0.07954545
  0.08181818  0.08409091  0.08636364  0.08863636  0.09090909  0.09318182
  0.09545455  0.09772727  0.1         0.10227273  0.10454545  0.10681818
  0.10909091  0.11136364  0.11363636  0.11590909  0.11818182  0.12045455
  0.12272727  0.125       0.12727273  0.12954545  0.13181818  0.13409091
  0.13636364  0.13863636  0.14090909  0.14318182  0.14545455  0.14772727
  0.15        0.15227273  0.15454545  0.15681818  0.15909091  0.16136364
  0.16363636  0.16590909  0.16818182  0.17045455  0.17272727  0.175
  0.17727273  0.17954545  0.18181818  0.18409091  0.18636364  0.18863636
  0.19090909  0.19318182  0.19545455  0.19772727  0.2         0.20227273
  0.20454545  0.20681818  0.20909091  0.21136364  0.21363636  0.21590909
  0.21818182  0.22045455  0.22272727  0.225       0.22727273  0.22954545
  0.23181818  0.23409091  0.23636364  0.23863636  0.24090909  0.24318182
  0.24545455  0.24772727  0.25        0.25227273  0.25454545  0.25681818
  0.25909091  0.26136364  0.26363636  0.26590909  0.26818182  0.27045455
  0.27272727  0.275       0.27727273  0.27954545  0.28181818  0.28409091
  0.28636364  0.28863636  0.29090909  0.29318182  0.29545455  0.29772727
  0.3         0.30227273  0.30454545  0.30681818  0.30909091  0.31136364
  0.31363636  0.31590909  0.31818182  0.32045455  0.32272727  0.325
  0.32727273  0.32954545  0.33181818  0.33409091  0.33636364  0.33863636
  0.34090909  0.34318182  0.34545455  0.34772727  0.35        0.35227273
  0.35454545  0.35681818  0.35909091  0.36136364  0.36363636  0.36590909
  0.36818182  0.37045455  0.37272727  0.375       0.37727273  0.37954545
  0.38181818  0.38409091  0.38636364  0.38863636  0.39090909  0.39318182
  0.39545455  0.39772727  0.4         0.40227273  0.40454545  0.40681818
  0.40909091  0.41136364  0.41363636  0.41590909  0.41818182  0.42045455
  0.42272727  0.425       0.42727273  0.42954545  0.43181818  0.43409091
  0.43636364  0.43863636  0.44090909  0.44318182  0.44545455  0.44772727
  0.45        0.45227273  0.45454545  0.45681818  0.45909091  0.46136364
  0.46363636  0.46590909  0.46818182  0.47045455  0.47272727  0.475
  0.47727273  0.47954545  0.48181818  0.48409091  0.48636364  0.48863636
  0.49090909  0.49318182  0.49545455  0.49772727 -0.5        -0.49772727
 -0.49545455 -0.49318182 -0.49090909 -0.48863636 -0.48636364 -0.48409091
 -0.48181818 -0.47954545 -0.47727273 -0.475      -0.47272727 -0.47045455
 -0.46818182 -0.46590909 -0.46363636 -0.46136364 -0.45909091 -0.45681818
 -0.45454545 -0.45227273 -0.45       -0.44772727 -0.44545455 -0.44318182
 -0.44090909 -0.43863636 -0.43636364 -0.43409091 -0.43181818 -0.42954545
 -0.42727273 -0.425      -0.42272727 -0.42045455 -0.41818182 -0.41590909
 -0.41363636 -0.41136364 -0.40909091 -0.40681818 -0.40454545 -0.40227273
 -0.4        -0.39772727 -0.39545455 -0.39318182 -0.39090909 -0.38863636
 -0.38636364 -0.38409091 -0.38181818 -0.37954545 -0.37727273 -0.375
 -0.37272727 -0.37045455 -0.36818182 -0.36590909 -0.36363636 -0.36136364
 -0.35909091 -0.35681818 -0.35454545 -0.35227273 -0.35       -0.34772727
 -0.34545455 -0.34318182 -0.34090909 -0.33863636 -0.33636364 -0.33409091
 -0.33181818 -0.32954545 -0.32727273 -0.325      -0.32272727 -0.32045455
 -0.31818182 -0.31590909 -0.31363636 -0.31136364 -0.30909091 -0.30681818
 -0.30454545 -0.30227273 -0.3        -0.29772727 -0.29545455 -0.29318182
 -0.29090909 -0.28863636 -0.28636364 -0.28409091 -0.28181818 -0.27954545
 -0.27727273 -0.275      -0.27272727 -0.27045455 -0.26818182 -0.26590909
 -0.26363636 -0.26136364 -0.25909091 -0.25681818 -0.25454545 -0.25227273
 -0.25       -0.24772727 -0.24545455 -0.24318182 -0.24090909 -0.23863636
 -0.23636364 -0.23409091 -0.23181818 -0.22954545 -0.22727273 -0.225
 -0.22272727 -0.22045455 -0.21818182 -0.21590909 -0.21363636 -0.21136364
 -0.20909091 -0.20681818 -0.20454545 -0.20227273 -0.2        -0.19772727
 -0.19545455 -0.19318182 -0.19090909 -0.18863636 -0.18636364 -0.18409091
 -0.18181818 -0.17954545 -0.17727273 -0.175      -0.17272727 -0.17045455
 -0.16818182 -0.16590909 -0.16363636 -0.16136364 -0.15909091 -0.15681818
 -0.15454545 -0.15227273 -0.15       -0.14772727 -0.14545455 -0.14318182
 -0.14090909 -0.13863636 -0.13636364 -0.13409091 -0.13181818 -0.12954545
 -0.12727273 -0.125      -0.12272727 -0.12045455 -0.11818182 -0.11590909
 -0.11363636 -0.11136364 -0.10909091 -0.10681818 -0.10454545 -0.10227273
 -0.1        -0.09772727 -0.09545455 -0.09318182 -0.09090909 -0.08863636
 -0.08636364 -0.08409091 -0.08181818 -0.07954545 -0.07727273 -0.075
 -0.07272727 -0.07045455 -0.06818182 -0.06590909 -0.06363636 -0.06136364
 -0.05909091 -0.05681818 -0.05454545 -0.05227273 -0.05       -0.04772727
 -0.04545455 -0.04318182 -0.04090909 -0.03863636 -0.03636364 -0.03409091
 -0.03181818 -0.02954545 -0.02727273 -0.025      -0.02272727 -0.02045455
 -0.01818182 -0.01590909 -0.01363636 -0.01136364 -0.00909091 -0.00681818
 -0.00454545 -0.00227273] -->
[8]  [ find_dominant_window_sizes_list_single ] coefs [7.05208450e+01 2.17522066e+01 1.51117073e+01 1.08917429e+01
 7.98043598e+00 4.85509881e+00 5.08770649e+00 4.61865603e+00
 1.05418881e+01 4.14979431e+00 4.76953214e-01 5.86317778e-01
 7.18337760e-01 1.07780909e+00 2.35498782e-01 7.20215333e-01
 1.96347247e+00 9.38784226e+00 2.99297830e+00 2.08189696e+00
 2.49385046e+00 1.71487275e+00 1.94407839e+00 1.96098531e+00
 1.89724827e+00 8.15590795e+00 3.52111673e+00 8.99907928e-01
 1.06066847e+00 8.28651027e-01 7.64079256e-01 9.36246469e-01
 5.81945408e-01 2.59914187e+00 5.88234408e+00 1.81839487e+00
 1.61762788e+00 1.39577649e+00 1.43216216e+00 1.49479629e+00
 1.19792146e+00 1.31164524e+00 3.67380021e+00 1.11452862e+00
 4.83337162e-01 1.82045706e-01 2.51375542e-01 8.59942592e-02
 5.14102525e-01 3.07583414e-01 7.76624319e-01 2.75514067e+00
 1.37824354e+00 1.46632219e-01 8.67086600e-01 5.33131622e-01
 5.37527585e-01 3.35078734e-01 3.15423069e-01 5.14284786e-01
 9.10117190e-01 7.90460275e-01 4.32711634e-01 3.93838231e-01
 4.18690901e-01 4.23629859e-01 7.50179324e-01 1.13851179e+00
 6.40861403e-01 1.26757880e-01 3.62668541e-01 1.03689586e-01
 4.53112006e-01 3.77366888e-01 8.20368247e-02 4.02314524e-01
 1.43060549e+00 5.70117079e-01 5.80550972e-01 2.43891425e-01
 5.09424523e-01 3.41242063e-01 5.75738081e-01 5.46758534e-01
 1.61731453e+00 1.83306734e-01 4.29781084e-01 1.72372519e-01
 9.96824171e-02 6.16629699e-02 3.83731539e-01 1.94739951e-01
 1.05264356e+00 2.19393275e+00 9.63156393e-01 6.26881821e-01
 5.32550231e-01 6.29025335e-01 6.22641311e-01 5.86583104e-01
 4.87077570e-01 1.32796671e+00 3.82040960e-01 4.77883625e-01
 1.24875209e-01 2.62341193e-01 1.43642833e-01 3.16621068e-01
 2.30396944e-01 1.27399937e+00 1.96126638e+00 1.19371022e+00
 8.59447880e-01 7.97778595e-01 5.27269828e-01 3.98809808e-01
 4.32951835e-01 7.52366345e-01 1.93323776e+00 7.28311360e-01
 4.15258679e-01 1.40513014e-01 3.91111899e-01 1.09665817e-01
 5.06474122e-01 2.67548244e-01 7.78454603e-01 1.28718009e+00
 9.37300430e-01 1.74514937e-01 4.80464220e-01 1.46025905e-01
 3.35117279e-01 1.05098177e-01 6.42178473e-01 4.91593538e-01
 1.83137275e-01 5.87990897e-01 9.85901654e-02 3.78025387e-01
 1.71697441e-01 5.45500234e-01 4.62014884e-01 8.06124751e-01
 6.32469073e-01 3.20604343e-01 3.13630825e-01 7.09827408e-01
 2.42198416e-01 4.83588209e-01 1.78252899e-01 4.82234424e-01
 2.84326895e-01 3.58662760e-01 5.60258251e-01 1.49563774e-01
 3.85001188e-01 1.86587731e-01 6.02123474e-01 1.71488932e-01
 9.20529001e-01 5.86678171e-01 4.16942363e-01 1.20759219e-01
 2.03270008e-01 2.56957857e-01 4.97482399e-01 2.21753783e-01
 7.77688688e-01 9.70753417e-01 9.64500032e-01 5.24903110e-01
 2.37929172e-01 2.68556750e-01 2.81549865e-01 4.23825895e-01
 7.07139444e-01 1.40628665e+00 7.23895052e-01 2.87939965e-01
 1.70589963e-01 1.86167939e-01 1.24306854e-01 4.02796030e-01
 3.43824952e-01 7.82722618e-01 9.60348143e-01 9.03734349e-01
 2.05860795e-01 5.30708958e-01 3.87088594e-01 3.65813128e-01
 3.92789116e-01 7.51960126e-01 9.55793288e-01 6.46895798e-01
 6.06062984e-01 1.06755334e-01 3.46925168e-01 1.18433818e-01
 4.65098476e-01 4.01014572e-01 3.82187726e-01 3.89927607e-01
 6.39409585e-01 1.16398430e-01 5.37536982e-01 2.60664188e-01
 5.94530536e-01 3.42324045e-01 5.92096914e-01 6.69123580e-01
 3.82960792e-01 3.88082937e-01 3.15541249e-01 5.47114371e-01
 1.58975566e-01 3.59959298e-01 3.04271830e-01 4.07639725e-01
 3.33335000e-01 4.07639725e-01 3.04271830e-01 3.59959298e-01
 1.58975566e-01 5.47114371e-01 3.15541249e-01 3.88082937e-01
 3.82960792e-01 6.69123580e-01 5.92096914e-01 3.42324045e-01
 5.94530536e-01 2.60664188e-01 5.37536982e-01 1.16398430e-01
 6.39409585e-01 3.89927607e-01 3.82187726e-01 4.01014572e-01
 4.65098476e-01 1.18433818e-01 3.46925168e-01 1.06755334e-01
 6.06062984e-01 6.46895798e-01 9.55793288e-01 7.51960126e-01
 3.92789116e-01 3.65813128e-01 3.87088594e-01 5.30708958e-01
 2.05860795e-01 9.03734349e-01 9.60348143e-01 7.82722618e-01
 3.43824952e-01 4.02796030e-01 1.24306854e-01 1.86167939e-01
 1.70589963e-01 2.87939965e-01 7.23895052e-01 1.40628665e+00
 7.07139444e-01 4.23825895e-01 2.81549865e-01 2.68556750e-01
 2.37929172e-01 5.24903110e-01 9.64500032e-01 9.70753417e-01
 7.77688688e-01 2.21753783e-01 4.97482399e-01 2.56957857e-01
 2.03270008e-01 1.20759219e-01 4.16942363e-01 5.86678171e-01
 9.20529001e-01 1.71488932e-01 6.02123474e-01 1.86587731e-01
 3.85001188e-01 1.49563774e-01 5.60258251e-01 3.58662760e-01
 2.84326895e-01 4.82234424e-01 1.78252899e-01 4.83588209e-01
 2.42198416e-01 7.09827408e-01 3.13630825e-01 3.20604343e-01
 6.32469073e-01 8.06124751e-01 4.62014884e-01 5.45500234e-01
 1.71697441e-01 3.78025387e-01 9.85901654e-02 5.87990897e-01
 1.83137275e-01 4.91593538e-01 6.42178473e-01 1.05098177e-01
 3.35117279e-01 1.46025905e-01 4.80464220e-01 1.74514937e-01
 9.37300430e-01 1.28718009e+00 7.78454603e-01 2.67548244e-01
 5.06474122e-01 1.09665817e-01 3.91111899e-01 1.40513014e-01
 4.15258679e-01 7.28311360e-01 1.93323776e+00 7.52366345e-01
 4.32951835e-01 3.98809808e-01 5.27269828e-01 7.97778595e-01
 8.59447880e-01 1.19371022e+00 1.96126638e+00 1.27399937e+00
 2.30396944e-01 3.16621068e-01 1.43642833e-01 2.62341193e-01
 1.24875209e-01 4.77883625e-01 3.82040960e-01 1.32796671e+00
 4.87077570e-01 5.86583104e-01 6.22641311e-01 6.29025335e-01
 5.32550231e-01 6.26881821e-01 9.63156393e-01 2.19393275e+00
 1.05264356e+00 1.94739951e-01 3.83731539e-01 6.16629699e-02
 9.96824171e-02 1.72372519e-01 4.29781084e-01 1.83306734e-01
 1.61731453e+00 5.46758534e-01 5.75738081e-01 3.41242063e-01
 5.09424523e-01 2.43891425e-01 5.80550972e-01 5.70117079e-01
 1.43060549e+00 4.02314524e-01 8.20368247e-02 3.77366888e-01
 4.53112006e-01 1.03689586e-01 3.62668541e-01 1.26757880e-01
 6.40861403e-01 1.13851179e+00 7.50179324e-01 4.23629859e-01
 4.18690901e-01 3.93838231e-01 4.32711634e-01 7.90460275e-01
 9.10117190e-01 5.14284786e-01 3.15423069e-01 3.35078734e-01
 5.37527585e-01 5.33131622e-01 8.67086600e-01 1.46632219e-01
 1.37824354e+00 2.75514067e+00 7.76624319e-01 3.07583414e-01
 5.14102525e-01 8.59942592e-02 2.51375542e-01 1.82045706e-01
 4.83337162e-01 1.11452862e+00 3.67380021e+00 1.31164524e+00
 1.19792146e+00 1.49479629e+00 1.43216216e+00 1.39577649e+00
 1.61762788e+00 1.81839487e+00 5.88234408e+00 2.59914187e+00
 5.81945408e-01 9.36246469e-01 7.64079256e-01 8.28651027e-01
 1.06066847e+00 8.99907928e-01 3.52111673e+00 8.15590795e+00
 1.89724827e+00 1.96098531e+00 1.94407839e+00 1.71487275e+00
 2.49385046e+00 2.08189696e+00 2.99297830e+00 9.38784226e+00
 1.96347247e+00 7.20215333e-01 2.35498782e-01 1.07780909e+00
 7.18337760e-01 5.86317778e-01 4.76953214e-01 4.14979431e+00
 1.05418881e+01 4.61865603e+00 5.08770649e+00 4.85509881e+00
 7.98043598e+00 1.08917429e+01 1.51117073e+01 2.17522066e+01] -->
[8]  [ find_dominant_window_sizes_list_single ] Freqs -->
[8]  [ find_dominant_window_sizes_list_single ] Coefs and window_sizes -->
[8]  [ find_dominant_window_sizes_list_single ] --> Find and return valid window_sizes
[8]  [ find_dominant_window_sizes_list_single ] Find and return valid window_sizes | ... 0 ... [  0   1   2   7  16  24   3  33   5   4   6   8  41  25  17  50  32  19
  92  18  15 109  22  21 117  23  34  20  35  83  38  37  75 176  36  51
 100  40 126 108  39 110  66  42  12  27  91 168 169  93 185 193 127  30
 159  59 186  26  53 111  28 142 112  60 184 125 167  49  29 116 192  65
 118 177  14  11 146 175 210 194 133  67 203 143  96  94  97 195 157 207
 209 136 160  98  10  31  77  81  76 153 214  82 140 205  55  54  95 188
 113 170  58  47  79 123 165 134  99 148  43 150 129 102   9 199 141  71
 115  61  85 174  64  63 161 119 218 182  74 200 114  62 191 121 202 212
 189 155  89 211 201 101 138  72 190  69 216 152 197 183 208  80 131  56
 144 106 213  57 145  48 217 178 151 173 172 124 104 206 164  45  78 147
 171  13 107 166 187 163  90 156 180  84 135  44 149 128  86 139 158 179
 215 154  52 130 105 120  68 103 181 162 198 204 122 196 132  70  87 137
  46  73  88]
[8]  [ find_dominant_window_sizes_list_single ] Find and return valid window_sizes | ... 1 ... [  0   1   2   7  16  24   3  33   5   4   6   8  41  25  17  50  32  19
  92  18  15 109  22  21 117  23  34  20  35  83  38  37  75 176  36  51
 100  40 126 108  39 110  66  42  12  27  91 168 169  93 185 193 127  30
 159  59 186  26  53 111  28 142 112  60 184 125 167  49  29 116 192  65
 118 177  14  11 146 175 210 194 133  67 203 143  96  94  97 195 157 207
 209 136 160  98  10  31  77  81  76 153 214  82 140 205  55  54  95 188
 113 170  58  47  79 123 165 134  99 148  43 150 129 102   9 199 141  71
 115  61  85 174  64  63 161 119 218 182  74 200 114  62 191 121 202 212
 189 155  89 211 201 101 138  72 190  69 216 152 197 183 208  80 131  56
 144 106 213  57 145  48 217 178 151 173 172 124 104 206 164  45  78 147
 171  13 107 166 187 163  90 156 180  84 135  44 149 128  86 139 158 179
 215 154  52 130 105 120  68 103 181 162 198 204 122 196 132  70  87 137
  46  73  88]
[8]  [ find_dominant_window_sizes_list_single ] Find and return valid window_sizes | ... 2 ... [  0   1   2   7  16  24   3  33   5   4   6   8  41  25  17  50  32  19
  92  18  15 109  22  21 117  23  34  20  35  83  38  37  75 176  36  51
 100  40 126 108  39 110  66  42  12  27  91 168 169  93 185 193 127  30
 159  59 186  26  53 111  28 142 112  60 184 125 167  49  29 116 192  65
 118 177  14  11 146 175 210 194 133  67 203 143  96  94  97 195 157 207
 209 136 160  98  10  31  77  81  76 153 214  82 140 205  55  54  95 188
 113 170  58  47  79 123 165 134  99 148  43 150 129 102   9 199 141  71
 115  61  85 174  64  63 161 119 218 182  74 200 114  62 191 121 202 212
 189 155  89 211 201 101 138  72 190  69 216 152 197 183 208  80 131  56
 144 106 213  57 145  48 217 178 151 173 172 124 104 206 164  45  78 147
 171  13 107 166 187 163  90 156 180  84 135  44 149 128  86 139 158 179
 215 154  52 130 105 120  68 103 181 162 198 204 122 196 132  70  87 137
  46  73  88]
[8]  [ find_dominant_window_sizes_list_single ] Find and return valid window_sizes | ... 2b ... 5
[8]  [ find_dominant_window_sizes_list_single ] Find and return valid window_sizes -->
[8]  [ find_dominant_window_sizes_list_single ] Sizes: [17, 12, 10, 16, 8]
[8] [find_dominant_window_sizes_list_single --> ]
[8]  [ Find_dominant_window_sizes_list ] Get sizes for var 0 | [17, 12, 10, 16, 8]
[8]  [ Find_dominant_window_sizes_list ] Grouping sizes
[8]  [ Find_dominant_window_sizes_list ] Final selected window sizes: [17, 12, 10, 16, 8]
[8] [Find_dominant_window_sizes_list --> ]
[8]  [ windowed_dataset ] X is a DataFrame | Window sizes: 5
[8]  [ windowed_dataset ] Building the windows
[8]  [ windowed_dataset ] w = 17
[8]  [ windowed_dataset ] w 17 | enc_input~(424, 1, 17) | dss~1
[8]  [ windowed_dataset ] w = 12
[8]  [ windowed_dataset ] w 12 | enc_input~(429, 1, 12) | dss~2
[8]  [ windowed_dataset ] w = 10
[8]  [ windowed_dataset ] w 10 | enc_input~(431, 1, 10) | dss~3
[8]  [ windowed_dataset ] w = 16
[8]  [ windowed_dataset ] w 16 | enc_input~(425, 1, 16) | dss~4
[8]  [ windowed_dataset ] w = 8
[8]  [ windowed_dataset ] w 8 | enc_input~(433, 1, 8) | dss~5
[8]  [ windowed_dataset ] Number of windows: 5
[8] [windowed_dataset --> ]
[8]  [ _get_enc_input ] About to get the encoder input | windows~5
[8]  [ _get_enc_input ] Enc input obtained | enc_input~(424, 1, 17)
[8] [_get_encoder --> ]
[8]  [ _get_encoder ] enc_input~(424, 1, 17)
[8]  [ _get_encoder ] About to exec _get_optimizer
[8] [ --> _get_optimizer ]
[8] [_get_encoder --> ]
[8] [_get_encoder --> ]
[8] [ --> set_fine_tune_ ]
[8]  [ set_fine_tune_ ] Model class: momentfm.models.moment.MOMENTPipeline
[8]  [ set_fine_tune_ ] Moment
[8] [set_fine_tune_ --> ]
[8] [set_fine_tune_ --> ]
[8] [set_fine_tune_ --> ]
[8] [set_fine_tune_ --> ]
[8] [set_fine_tune_ --> ]
[8] [set_fine_tune_ --> ]
[8] [set_fine_tune_ --> ]
[8] [set_fine_tune_ --> ]
[8] [set_fine_tune_ --> ]
[8] [set_fine_tune_ --> ]
[8] [set_fine_tune_ --> ]
[8] [set_fine_tune_ --> ]
[8] [set_fine_tune_ --> ]
[8] [_get_encoder --> ]
[8] [ --> fine_tune ]
[8]  [ fine_tune ] Original enc_learn MOMENTPipeline(
  (normalizer): RevIN()
  (tokenizer): Patching()
  (patch_embedding): PatchEmbedding(
    (value_embedding): Linear(in_features=8, out_features=512, bias=False)
    (position_embedding): PositionalEmbedding()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): T5Stack(
    (embed_tokens): Embedding(32128, 512)
    (block): ModuleList(
      (0): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
              (relative_attention_bias): Embedding(32, 6)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseGatedActDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
              (act): NewGELUActivation()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1-7): 7 x T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseGatedActDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
              (act): NewGELUActivation()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (final_layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (head): PretrainHead(
    (dropout): Dropout(p=0.1, inplace=False)
    (linear): Linear(in_features=512, out_features=8, bias=True)
  )
)  | Final model MOMENTPipeline(
  (normalizer): RevIN()
  (tokenizer): Patching()
  (patch_embedding): PatchEmbedding(
    (value_embedding): Linear(in_features=8, out_features=512, bias=False)
    (position_embedding): PositionalEmbedding()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): T5Stack(
    (embed_tokens): Embedding(32128, 512)
    (block): ModuleList(
      (0): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
              (relative_attention_bias): Embedding(32, 6)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseGatedActDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
              (act): NewGELUActivation()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1-7): 7 x T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseGatedActDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
              (act): NewGELUActivation()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (final_layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (head): PretrainHead(
    (dropout): Dropout(p=0.1, inplace=False)
    (linear): Linear(in_features=512, out_features=8, bias=True)
  )
)
[8] [ --> set_fine_tune_ ]
[8]  [ set_fine_tune_ ] Model class: momentfm.models.moment.MOMENTPipeline
[8]  [ set_fine_tune_ ] Moment
[8] [set_fine_tune_ --> ]
[8]  [ set_fine_tune_ ] Use fine_tune_moment parameters
[8] [ --> fine_tune_moment_ ]
[8]  [ set_fine_tune_ ] Processing 5 datasets : (424, 1, 17)
[8]  [ set_fine_tune_ ] Setting up optimizer as AdamW
[8] [91m [ set_fine_tune_ ] Processing wlen 17 | wlens [] | i 1/5[0m
[8] [ --> fine_tune_moment_single ]
[8]  [ fine_tune_moment_single ] fine_tune_moment_single | Prepare the dataset | X ~ (424, 1, 17)
[8]  [ fine_tune_moment_single | prepare_train_and_eval_dataloaders ] Selecting ds train | 106 windows
[8]  [ fine_tune_moment_single | prepare_train_and_eval_dataloaders ] Selecting validation train | 318 windows
[8]  [ fine_tune_moment_single | prepare_train_and_eval_dataloaders ] Train DataLoader | Random windows
[7] [ --> fine_tune_moment_single | prepare_train_and_eval_dataloaders | random_windows ]
[7]  [ fine_tune_moment_single | prepare_train_and_eval_dataloaders ] N windows: None
[7]  [ fine_tune_moment_single | prepare_train_and_eval_dataloaders ] windows~torch.Size([106, 1, 17])
[7] [fine_tune_moment_single | prepare_train_and_eval_dataloaders --> ]
[8]  [ fine_tune_moment_single | prepare_train_and_eval_dataloaders ] Train DataLoader | DataLoader
[8]  [ fine_tune_moment_single | prepare_train_and_eval_dataloaders ] Validation DataLoader
[8]  [ fine_tune_moment_single ] Processing wlen 17 | Lengths list: [17]
[8]  [ fine_tune_moment_single ] Eval Pre | wlen 17
[8]  [ fine_tune_moment_single ] Start timer
[8] [ --> fine_tune_moment_eval_step_ ]
[8] [ --> moment_build_masks ]
[8]  [ moment_build_masks ] moment_build_masks | batch ~ torch.Size([16, 1, 17]) | batch_masks ~ torch.Size([16, 17])
[8]  [ moment_build_masks ] window_mask_percent 1 | batch ~ torch.Size([16, 1, 17])
[8]  [ moment_build_masks ] Using MVP masking generation style
[8]  [ moment_build_masks ] o ~ torch.Size([16, 17]) | stateful = False | sync = False | r = 1
[8] [91m [ moment_build_masks ] Registering error in DataFrame | window: 17 | error: "binomial_cpu" not implemented for 'Long'[0m
[8] [fine_tune_moment_single_ --> ]
[8] [91m [ set_fine_tune_ ] Processing wlen 12 | wlens [17] | i 2/5[0m
[8] [ --> fine_tune_moment_single ]
[8]  [ fine_tune_moment_single ] fine_tune_moment_single | Prepare the dataset | X ~ (429, 1, 12)
[8]  [ fine_tune_moment_single | prepare_train_and_eval_dataloaders ] Selecting ds train | 108 windows
[8]  [ fine_tune_moment_single | prepare_train_and_eval_dataloaders ] Selecting validation train | 322 windows
[8]  [ fine_tune_moment_single | prepare_train_and_eval_dataloaders ] Train DataLoader | Random windows
[7] [ --> fine_tune_moment_single | prepare_train_and_eval_dataloaders | random_windows ]
[7]  [ fine_tune_moment_single | prepare_train_and_eval_dataloaders ] N windows: None
[7]  [ fine_tune_moment_single | prepare_train_and_eval_dataloaders ] windows~torch.Size([108, 1, 12])
[7] [fine_tune_moment_single | prepare_train_and_eval_dataloaders --> ]
[8]  [ fine_tune_moment_single | prepare_train_and_eval_dataloaders ] Train DataLoader | DataLoader
[8]  [ fine_tune_moment_single | prepare_train_and_eval_dataloaders ] Validation DataLoader
[8]  [ fine_tune_moment_single ] Processing wlen 12 | Lengths list: [17, 12]
[8]  [ fine_tune_moment_single ] Start timer
[8]  [ fine_tune_moment_single ] Train | wlen 12
[8] [ --> fine_tune_moment_train_ ]
[8]  [ fine_tune_moment_train_ ] Training loop
[8]  [ fine_tune_moment_train_ ] Fine tune loop | batch_masks~tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:1')
[8]  [ fine_tune_moment_train_ ] num_epochs 5 | n_batches 7
[8]  [ fine_tune_moment_train_ ] batch 0 ~ torch.Size([16, 1, 12]) | epoch 0 | train 0 of 35 | Before loop step
[8] [ --> fine_tune_moment_train_loop_step_ ]
[8]  [ fine_tune_moment_train_loop_step_ ] Get the masks
[8] [ --> moment_build_masks ]
[8]  [ moment_build_masks ] moment_build_masks | batch ~ torch.Size([16, 1, 12]) | batch_masks ~ torch.Size([16, 12])
[8]  [ moment_build_masks ] window_mask_percent 1 | batch ~ torch.Size([16, 1, 12])
[8]  [ moment_build_masks ] Using MVP masking generation style
[8]  [ moment_build_masks ] o ~ torch.Size([16, 12]) | stateful = False | sync = False | r = 1
[8]  [ moment_build_masks ] fine_tune_moment_single | Train | Window 17 not valid | "binomial_cpu" not implemented for 'Long'
[8] [91m [ moment_build_masks ] Registering error in DataFrame | window: 12 | error: "binomial_cpu" not implemented for 'Long'[0m
[8] [fine_tune_moment_single_ --> ]
[8] [91m [ set_fine_tune_ ] Processing wlen 10 | wlens [17, 12] | i 3/5[0m
[8] [ --> fine_tune_moment_single ]
[8]  [ fine_tune_moment_single ] fine_tune_moment_single | Prepare the dataset | X ~ (431, 1, 10)
[8]  [ fine_tune_moment_single | prepare_train_and_eval_dataloaders ] Selecting ds train | 108 windows
[8]  [ fine_tune_moment_single | prepare_train_and_eval_dataloaders ] Selecting validation train | 324 windows
[8]  [ fine_tune_moment_single | prepare_train_and_eval_dataloaders ] Train DataLoader | Random windows
[7] [ --> fine_tune_moment_single | prepare_train_and_eval_dataloaders | random_windows ]
[7]  [ fine_tune_moment_single | prepare_train_and_eval_dataloaders ] N windows: None
[7]  [ fine_tune_moment_single | prepare_train_and_eval_dataloaders ] windows~torch.Size([108, 1, 10])
[7] [fine_tune_moment_single | prepare_train_and_eval_dataloaders --> ]
[8]  [ fine_tune_moment_single | prepare_train_and_eval_dataloaders ] Train DataLoader | DataLoader
[8]  [ fine_tune_moment_single | prepare_train_and_eval_dataloaders ] Validation DataLoader
[8]  [ fine_tune_moment_single ] Processing wlen 10 | Lengths list: [17, 12, 10]
[8]  [ fine_tune_moment_single ] Start timer
[8]  [ fine_tune_moment_single ] Train | wlen 10
[8] [ --> fine_tune_moment_train_ ]
[8]  [ fine_tune_moment_train_ ] Training loop
[8]  [ fine_tune_moment_train_ ] Fine tune loop | batch_masks~tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:1')
[8]  [ fine_tune_moment_train_ ] num_epochs 5 | n_batches 7
[8]  [ fine_tune_moment_train_ ] batch 0 ~ torch.Size([16, 1, 10]) | epoch 0 | train 0 of 35 | Before loop step
[8] [ --> fine_tune_moment_train_loop_step_ ]
[8]  [ fine_tune_moment_train_loop_step_ ] Get the masks
[8] [ --> moment_build_masks ]
[8]  [ moment_build_masks ] moment_build_masks | batch ~ torch.Size([16, 1, 10]) | batch_masks ~ torch.Size([16, 10])
[8]  [ moment_build_masks ] window_mask_percent 1 | batch ~ torch.Size([16, 1, 10])
[8]  [ moment_build_masks ] Using MVP masking generation style
[8]  [ moment_build_masks ] o ~ torch.Size([16, 10]) | stateful = False | sync = False | r = 1
[8]  [ moment_build_masks ] fine_tune_moment_single | Train | Window 17 not valid | "binomial_cpu" not implemented for 'Long'
[8] [91m [ moment_build_masks ] Registering error in DataFrame | window: 10 | error: "binomial_cpu" not implemented for 'Long'[0m
[8] [fine_tune_moment_single_ --> ]
[8] [91m [ set_fine_tune_ ] Processing wlen 16 | wlens [17, 12, 10] | i 4/5[0m
[8] [ --> fine_tune_moment_single ]
[8]  [ fine_tune_moment_single ] fine_tune_moment_single | Prepare the dataset | X ~ (425, 1, 16)
[8]  [ fine_tune_moment_single | prepare_train_and_eval_dataloaders ] Selecting ds train | 107 windows
[8]  [ fine_tune_moment_single | prepare_train_and_eval_dataloaders ] Selecting validation train | 319 windows
[8]  [ fine_tune_moment_single | prepare_train_and_eval_dataloaders ] Train DataLoader | Random windows
[7] [ --> fine_tune_moment_single | prepare_train_and_eval_dataloaders | random_windows ]
[7]  [ fine_tune_moment_single | prepare_train_and_eval_dataloaders ] N windows: None
[7]  [ fine_tune_moment_single | prepare_train_and_eval_dataloaders ] windows~torch.Size([107, 1, 16])
[7] [fine_tune_moment_single | prepare_train_and_eval_dataloaders --> ]
[8]  [ fine_tune_moment_single | prepare_train_and_eval_dataloaders ] Train DataLoader | DataLoader
[8]  [ fine_tune_moment_single | prepare_train_and_eval_dataloaders ] Validation DataLoader
[8]  [ fine_tune_moment_single ] Processing wlen 16 | Lengths list: [17, 12, 10, 16]
[8]  [ fine_tune_moment_single ] Start timer
[8]  [ fine_tune_moment_single ] Train | wlen 16
[8] [ --> fine_tune_moment_train_ ]
[8]  [ fine_tune_moment_train_ ] Training loop
[8]  [ fine_tune_moment_train_ ] num_epochs 5 | n_batches 7
[8]  [ fine_tune_moment_train_ ] batch 0 ~ torch.Size([16, 1, 16]) | epoch 0 | train 0 of 35 | Before loop step
[8] [91m [ moment_build_masks ] Registering error in DataFrame | window: 16 | error: "binomial_cpu" not implemented for 'Long'[0m
[8] [91m [ set_fine_tune_ ] Processing wlen 8 | wlens [17, 12, 10, 16] | i 5/5[0m
[8] [91m [ moment_build_masks ] Registering error in DataFrame | window: 8 | error: "binomial_cpu" not implemented for 'Long'[0m
