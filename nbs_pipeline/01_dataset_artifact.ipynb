{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create artifact from time series dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbs_pipeline.utils.vscode  as vs\n",
    "vs.DisplayHandle.update = vs.update_patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fastcore.all import *\n",
    "import wandb\n",
    "from dvats.load import TSArtifact, infer_or_inject_freq\n",
    "import pickle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tsai.data.external import convert_tsf_to_dataframe\n",
    "from tsai.utils import stack_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_path = Path.home()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yml: ./config/base.yaml\n",
      "Getting content./config/base.yaml\n",
      "... About to replace includes with content\n",
      "Load content./config/base.yaml\n",
      "-----------Project configuration-----------\n",
      "user: mi-santamaria\n",
      "project: deepvats\n",
      "version: latest\n",
      "data: ETTh1:latest\n",
      "-----------Project configuration-----------\n",
      "Current: /home/macu/work/nbs_pipeline\n",
      "yml: ./config/base.yaml\n",
      "yml: ./config/base.yaml\n",
      "Getting content./config/base.yaml\n",
      "... About to replace includes with content\n",
      "Load content./config/base.yaml\n"
     ]
    },
    {
     "ename": "<class 'Exception'>",
     "evalue": "\u001b[91mPlease check .env and base.yml: project differs os test-project yaml deepvats\u001b[0m",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/macu/work/nbs_pipeline/01_dataset_artifact.ipynb Celda 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f64766174732d6a7570797465722d31227d@ssh-remote%2Bg4.etsisi.upm.es/home/macu/work/nbs_pipeline/01_dataset_artifact.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconfig\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mcfg\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f64766174732d6a7570797465722d31227d@ssh-remote%2Bg4.etsisi.upm.es/home/macu/work/nbs_pipeline/01_dataset_artifact.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m config \u001b[39m=\u001b[39m cfg\u001b[39m.\u001b[39mget_artifact_config_sd2a(\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/work/nbs_pipeline/utils/config.py:206\u001b[0m, in \u001b[0;36mget_artifact_config_sd2a\u001b[0;34m(print_flag)\u001b[0m\n\u001b[1;32m    184\u001b[0m user, project, version, data, use_wandb, wandb_path \u001b[39m=\u001b[39m get__artifact_config_sd2a_get_auxiliar_variables(print_flag)\n\u001b[1;32m    185\u001b[0m artifact_config \u001b[39m=\u001b[39m AttrDict(\n\u001b[1;32m    186\u001b[0m     artifact_name           \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39malias,\n\u001b[1;32m    187\u001b[0m     csv_config              \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mcsv_config,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    204\u001b[0m     wandb_artifacts_path    \u001b[39m=\u001b[39m wandb_path\n\u001b[1;32m    205\u001b[0m )\n\u001b[0;32m--> 206\u001b[0m get__artifact_config_sd2a_check_errors(use_wandb, artifact_config, user, project)    \n\u001b[1;32m    207\u001b[0m \u001b[39mreturn\u001b[39;00m artifact_config\n",
      "File \u001b[0;32m~/work/nbs_pipeline/utils/config.py:171\u001b[0m, in \u001b[0;36mget__artifact_config_sd2a_check_errors\u001b[0;34m(use_wandb, artifact_config, user, project)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget__artifact_config_sd2a_check_errors\u001b[39m(use_wandb, artifact_config, user, project):\n\u001b[0;32m--> 171\u001b[0m     check_project_and_entity(user, project)\n\u001b[1;32m    172\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    173\u001b[0m             use_wandb   \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39moffline\u001b[39m\u001b[39m\"\u001b[39m \n\u001b[1;32m    174\u001b[0m         \u001b[39mand\u001b[39;00m artifact_config\u001b[39m.\u001b[39mjoining_train_test  \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    175\u001b[0m     ):\n\u001b[1;32m    176\u001b[0m         custom_error(\u001b[39m\"\u001b[39m\u001b[39mIf you\u001b[39m\u001b[39m'\u001b[39m\u001b[39mre using deepvats in offline mode, set joining_train_test to False\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/work/nbs_pipeline/utils/config.py:121\u001b[0m, in \u001b[0;36mcheck_project_and_entity\u001b[0;34m(user, project)\u001b[0m\n\u001b[1;32m    119\u001b[0m     custom_error(\u001b[39m\"\u001b[39m\u001b[39mPlease check .env and base.yml: entity != user os \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m os_entity \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m yaml \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m user)\n\u001b[1;32m    120\u001b[0m \u001b[39mif\u001b[39;00m (os_project \u001b[39m!=\u001b[39m project):\n\u001b[0;32m--> 121\u001b[0m     custom_error(\u001b[39m\"\u001b[39;49m\u001b[39mPlease check .env and base.yml: project differs os \u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m os_project \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m yaml \u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m project)\n",
      "File \u001b[0;32m~/work/nbs_pipeline/utils/config.py:18\u001b[0m, in \u001b[0;36mcustom_error\u001b[0;34m(message)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39m# CÃ³digo ANSI para restablecer el color\u001b[39;00m\n\u001b[1;32m     17\u001b[0m reset \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[0m\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 18\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(red_start \u001b[39m+\u001b[39m message \u001b[39m+\u001b[39m reset)\n",
      "\u001b[0;31mException\u001b[0m: \u001b[91mPlease check .env and base.yml: project differs os test-project yaml deepvats\u001b[0m"
     ]
    }
   ],
   "source": [
    "import utils.config as cfg\n",
    "config = cfg.get_artifact_config_sd2a(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is assumed to come as a dataframe, either as a binarized  picke file or\n",
    "as a csv file. It can also come as a `.tsf` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ext = str(config.data_fpath).split('.')[-1]\n",
    "\n",
    "if ext == 'pickle':\n",
    "    df = pd.read_pickle(config.data_fpath)\n",
    "    \n",
    "elif ext in ['csv','txt']:\n",
    "    df = pd.read_csv(config.data_fpath, **config.csv_config)\n",
    "    \n",
    "elif ext == 'tsf':\n",
    "    data, _, _, _, _ = convert_tsf_to_dataframe(config.data_fpath)\n",
    "    config.update({'start_date': data.start_timestamp[0]}, allow_val_change=True)\n",
    "    date_format = config.date_format\n",
    "    df = pd.DataFrame(stack_pad(data.series_value).T)\n",
    "    \n",
    "else:\n",
    "    raise Exception('The data file path has an unsupported extension')\n",
    "    \n",
    "print(f'File loaded successfully')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the time columm (if any) as an index\n",
    "if config.time_col is not None:\n",
    "    print(\"time_col: \"+str(config.time_col))\n",
    "    if isinstance(config.time_col, int): \n",
    "        datetime = df.iloc[:, config.time_col]\n",
    "        \n",
    "    elif isinstance(config.time_col, list): \n",
    "        datetime = df.iloc[:, config.time_col].apply(lambda x: x.astype(str).str.cat(sep='-'), axis=1)\n",
    "        \n",
    "    index = pd.DatetimeIndex(datetime)\n",
    "    if config.date_offset:\n",
    "        index += config.date_offset\n",
    "    \n",
    "    df = df.set_index(index, drop=False)\n",
    "    \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set dataframe frequency\n",
    "df = infer_or_inject_freq(df, injected_freq=config.freq, \n",
    "                          start_date=config.start_date, format=config.date_format)\n",
    "df.index.freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Subset of variables\n",
    "if config.data_cols:\n",
    "    df = df.iloc[:, config.data_cols]\n",
    "\n",
    "print(f'Num. variables: {len(df.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Replace the default missing values by np.NaN\n",
    "if config.missing_values_constant:\n",
    "    df.replace(config.missing_values_constant, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Show time series plot\n",
    "fig, ax = plt.subplots(1, figsize=(15,5), )\n",
    "cmap = matplotlib.colormaps.get_cmap('viridis')\n",
    "df.plot(color=cmap(0.05), ax=ax) # or use colormap=cmap\n",
    "# rect = Rectangle((5000, -4.2), 3000, 8.4, facecolor='lightgrey', alpha=0.5)\n",
    "# ax.add_patch(rect)\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Handle Missing Values, Resample and Normalize__\n",
    "\n",
    "In this second part, Time Series Artifact (TSArtifact) object can be created and missing values handling techniques, resampling and normalization can be applied. This techniques should be applied on the three subsets that must be previously created: training, validation and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training data\n",
    "rg = config.range_training\n",
    "\n",
    "if isinstance(rg, list):\n",
    "    rg_training = rg\n",
    "    \n",
    "elif isinstance(rg, dict):\n",
    "    rg_training = pd.date_range(rg['start'], rg['end'], freq=rg['freq'])\n",
    "    \n",
    "elif config.test_split:\n",
    "    rg_training = df.index[:math.ceil(len(df) * (1-config.test_split))]\n",
    "\n",
    "else:\n",
    "    rg_training = None\n",
    "    \n",
    "df_training = df[df.index.isin(rg_training)] if rg_training is not None else df\n",
    "training_artifact = TSArtifact.from_df(df_training, \n",
    "                                       name=config.artifact_name, \n",
    "                                       missing_values_technique=config.missing_values_technique,\n",
    "                                       resampling_freq=config.resampling_freq, \n",
    "                                       normalize=config.normalize_training, \n",
    "                                       path=str(Path.home()/config.wandb_artifacts_path))\n",
    "display(training_artifact.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Testing data\n",
    "rg = config.range_testing\n",
    "\n",
    "if rg or config.test_split:\n",
    "    \n",
    "    if isinstance(rg, list):\n",
    "        rg_testing = rg\n",
    "\n",
    "    elif isinstance(rg, dict):\n",
    "        rg_testing = pd.date_range(rg['start'], rg['end'], freq=rg['freq'])\n",
    "\n",
    "    elif config.test_split:\n",
    "        rg_testing = df.index[math.ceil(len(df) * (1 - config.test_split)):]\n",
    "\n",
    "    else:\n",
    "        rg_testing = None\n",
    "    \n",
    "    df_testing = df[df.index.isin(rg_testing)]\n",
    "    testing_artifact = TSArtifact.from_df(df_testing,\n",
    "                                          name=config.artifact_name, \n",
    "                                          missing_values_technique=config.missing_values_technique,\n",
    "                                          resampling_freq=config.resampling_freq, \n",
    "                                          normalize=False,\n",
    "                                          path=str(Path.home()/config.wandb_artifacts_path))\n",
    "    display(testing_artifact.metadata)\n",
    "else:\n",
    "    print(\"rg \"+ str(rg) + \" | test_split \"+ str(config.test_split))\n",
    "    testing_artifact = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training + Testing data\n",
    "if(config.joining_train_test):\n",
    "    print(\"joining_train_test: \"+ str(config.joining_train_test))\n",
    "    df_train_test = pd.concat([df_training, df_testing])\n",
    "    train_test_artifact = TSArtifact.from_df(df_train_test,\n",
    "                                           name=config.artifact_name, \n",
    "                                           missing_values_technique=config.missing_values_technique,\n",
    "                                           resampling_freq=config.resampling_freq, \n",
    "                                           normalize=False,\n",
    "                                           path=str(Path.home()/config.wandb_artifacts_path))\n",
    "    display(train_test_artifact.metadata)\n",
    "else:\n",
    "    train_test_artifact = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the experiment tracking and hyperparameter we will use the tool **Weights & Biases**. \n",
    "\n",
    "Before running this notebook part, make sure you have the `$WANDB_API_KEY`, `$WANDB_ENTITY` and `$WANDB_PROJECT` environment varibales defined with your API_KEY and your ENTITY and PROJECT names (run in a terminal `echo $WANDB_API_KEY` to see it, same with the other variables). If not, run in a terminal `wandb login [API_KEY]` to set the first one. You can see your API_KEY [here](https://wandb.ai/authorize) or in the settings of your W&B account. Run in a terminal `export WANDB_ENTITY=entity_name` and/or `export WANDB_PROJECT=project_name` to set the other two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.path.expanduser(\"~/work/nbs_pipeline/\")\n",
    "name=\"01_dataset_artifact\"\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = path+name+\".ipynb\"\n",
    "runname=name\n",
    "print(\"runname: \"+runname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mode = 'online' if config.use_wandb else 'disabled'\n",
    "\n",
    "# Make the run that will produce the artifact\n",
    "with wandb.init(job_type='create_dataset', resume=True, mode=mode, config=config, name=runname) as run:\n",
    "    if testing_artifact: \n",
    "        run.log_artifact(training_artifact, aliases=['train'])\n",
    "        run.log_artifact(testing_artifact, aliases=['test'])\n",
    "        \n",
    "        if train_test_artifact:\n",
    "            run.log_artifact(train_test_artifact, aliases=['all'])\n",
    "    \n",
    "    else:\n",
    "        run.log_artifact(training_artifact, aliases=['all'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d45d555be0220b07bf61be557bfa0ebbf7a95015976aec9a23277863e1bd4593"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
