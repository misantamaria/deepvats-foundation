{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create artifact from time series dataframe\n",
    "> Gets a .tsf or .csv with a time serie, convert int to np.dataframe and loads it to W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if '--vscode' in sys.argv:\n",
    "    print(\"Executing inside vscode\")\n",
    "    import nbs_pipeline.utils.vscode  as vs\n",
    "    vs.DisplayHandle.update = vs.update_patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fastcore.all import *\n",
    "import wandb\n",
    "from dvats.load import TSArtifact, infer_or_inject_freq\n",
    "import pickle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tsai.data.external import convert_tsf_to_dataframe\n",
    "from tsai.utils import stack_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook config\n",
    "> This notebook gets configuration from 'config\\base.yaml' and 'config\\01-dataset_artifact.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_path = Path.home()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "<class 'NameError'>",
     "evalue": "name 'get_artifact_config_sd2a_get_auxiliar_variables' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcfg\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m config \u001b[38;5;241m=\u001b[39m cfg\u001b[38;5;241m.\u001b[39mget_artifact_config_sd2a(print_flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/work/nbs_pipeline/utils/config.py:202\u001b[0m, in \u001b[0;36mget_artifact_config_sd2a\u001b[0;34m(print_flag)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_artifact_config_sd2a\u001b[39m(print_flag: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AttrDict:\n\u001b[1;32m    198\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;124;03m    Constructs the configuration for the dataset artifact by retrieving auxiliary variables and setting up the artifact configuration.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03m    Validates the configuration to ensure it meets specific criteria and returns the final artifact configuration as an AttrDict.\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m     user, project, version, data, use_wandb, wandb_path \u001b[38;5;241m=\u001b[39m \u001b[43mget_artifact_config_sd2a_get_auxiliar_variables\u001b[49m(print_flag)\n\u001b[1;32m    203\u001b[0m     artifact_config \u001b[38;5;241m=\u001b[39m AttrDict(\n\u001b[1;32m    204\u001b[0m         artifact_name           \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39malias,\n\u001b[1;32m    205\u001b[0m         csv_config              \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcsv_config,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    222\u001b[0m         wandb_artifacts_path    \u001b[38;5;241m=\u001b[39m wandb_path\n\u001b[1;32m    223\u001b[0m     )\n\u001b[1;32m    224\u001b[0m     get_artifact_config_sd2a_check_errors(use_wandb, artifact_config, user, project)    \n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_artifact_config_sd2a_get_auxiliar_variables' is not defined"
     ]
    }
   ],
   "source": [
    "import utils.config as cfg\n",
    "config = cfg.get_artifact_config_sd2a(print_flag = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in config.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is assumed to come as a dataframe, either as a binarized  picke file or\n",
    "as a csv file. It can also come as a `.tsf` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just checking file content\n",
    "try: \n",
    "    fpath=os.path.expanduser(config.data_fpath)\n",
    "    print(fpath)\n",
    "    with open(fpath, 'r') as file:\n",
    "        for _ in range(13):\n",
    "            line = file.readline()\n",
    "            print(line, end='')\n",
    "    data, _, _, _, _ = convert_tsf_to_dataframe(fpath)\n",
    "    print(\"Timestamp\", data.start_timestamp)\n",
    "except Exception as e:\n",
    "    print(\"Error while converting file. Maybe not a tsf: \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ext = str(config.data_fpath).split('.')[-1]\n",
    "\n",
    "if ext == 'pickle':\n",
    "    df = pd.read_pickle(config.data_fpath)\n",
    "    \n",
    "elif ext in ['csv','txt']:\n",
    "    df = pd.read_csv(config.data_fpath, **config.csv_config)\n",
    "    \n",
    "elif ext == 'tsf':\n",
    "    data, _, _, _, _ = convert_tsf_to_dataframe(os.path.expanduser(config.data_fpath))\n",
    "    config.update({'start_date': data.start_timestamp[0]}, allow_val_change=True)\n",
    "    date_format = config.date_format\n",
    "    df = pd.DataFrame(stack_pad(data.series_value).T)\n",
    "    \n",
    "else:\n",
    "    raise Exception('The data file path has an unsupported extension')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'File loaded successfully')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the time columm (if any) as an index\n",
    "if config.time_col is not None:\n",
    "    print(\"time_col: \"+str(config.time_col))\n",
    "    if isinstance(config.time_col, int): \n",
    "        print(\"Op 1\")\n",
    "        datetime = df.iloc[:, config.time_col]\n",
    "    elif isinstance(config.time_col, list): \n",
    "        print(\"Op 2\")\n",
    "        datetime = df.iloc[:, config.time_col].apply(lambda x: x.astype(str).str.cat(sep='-'), axis=1)\n",
    "    index = pd.DatetimeIndex(datetime)\n",
    "    if config.date_offset:\n",
    "        index += config.date_offset\n",
    "    \n",
    "    df = df.set_index(index, drop=False)   \n",
    "    #Delete Timestamp col\n",
    "    col_name = df.columns[config.time_col]\n",
    "    print(\"... drop Timestamp col \" + str(col_name))\n",
    "    df = df.drop(col_name, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set dataframe frequency\n",
    "df = infer_or_inject_freq(df, injected_freq=config.freq, \n",
    "                          start_date=config.start_date, format=config.date_format)\n",
    "df.index.freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Subset of variables\n",
    "if config.data_cols:\n",
    "    df = df.iloc[:, config.data_cols]\n",
    "\n",
    "print(f'Num. variables: {len(df.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Duplicated rows\n",
    "print(\"df shape before dropping duplicates\", df.shape)\n",
    "df.drop_duplicates()\n",
    "print(\"df shape after dropping duplicates\", df.shape)\n",
    "# Verificar si hay duplicados en el Ã­ndice del dataframe\n",
    "if df.index.duplicated().any():\n",
    "    raise ValueError(\"Duplicated index names\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Replace the default missing values by np.NaN\n",
    "if config.missing_values_constant:\n",
    "    df.replace(config.missing_values_constant, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Show time series plot\n",
    "fig, ax = plt.subplots(1, figsize=(15,5), )\n",
    "cmap = matplotlib.colormaps.get_cmap('viridis')\n",
    "df.plot(color=cmap(0.05), ax=ax) # or use colormap=cmap\n",
    "# rect = Rectangle((5000, -4.2), 3000, 8.4, facecolor='lightgrey', alpha=0.5)\n",
    "# ax.add_patch(rect)\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Handle Missing Values, Resample and Normalize__\n",
    "\n",
    "> In this second part, Time Series Artifact (TSArtifact) object can be created and missing values handling techniques, resampling and normalization can be applied.\n",
    "> \n",
    "> This techniques should be applied on the three subsets that must be previously created: training, validation and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rg = config.range_training\n",
    "\n",
    "if isinstance(rg, list):\n",
    "    rg_training = rg\n",
    "    \n",
    "elif isinstance(rg, dict):\n",
    "    rg_training = pd.date_range(rg['start'], rg['end'], freq=rg['freq'])\n",
    "    \n",
    "elif config.test_split:\n",
    "    rg_training = df.index[:math.ceil(len(df) * (1-config.test_split))]\n",
    "\n",
    "else:\n",
    "    rg_training = None\n",
    "    \n",
    "df_training = df[df.index.isin(rg_training)] if rg_training is not None else df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build training artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_artifact = TSArtifact.from_df(\n",
    "    df_training, \n",
    "    name=config.artifact_name, \n",
    "    missing_values_technique=config.missing_values_technique,\n",
    "    resampling_freq=config.resampling_freq, \n",
    "    normalize=config.normalize_training, \n",
    "    path=str(Path.home()/config.wandb_artifacts_path)\n",
    ")\n",
    "display(training_artifact.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Debugging \n",
    "if df_training.index.duplicated().any():\n",
    "    raise ValueError(\"Duplicated index names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build dataframe & artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Testing data\n",
    "rg = config.range_testing\n",
    "\n",
    "if rg or config.test_split:\n",
    "    \n",
    "    if isinstance(rg, list):\n",
    "        rg_testing = rg\n",
    "\n",
    "    elif isinstance(rg, dict):\n",
    "        rg_testing = pd.date_range(rg['start'], rg['end'], freq=rg['freq'])\n",
    "\n",
    "    elif config.test_split:\n",
    "        rg_testing = df.index[math.ceil(len(df) * (1 - config.test_split)):]\n",
    "\n",
    "    else:\n",
    "        rg_testing = None\n",
    "    \n",
    "    df_testing = df[df.index.isin(rg_testing)]\n",
    "    testing_artifact = TSArtifact.from_df(df_testing,\n",
    "                                          name=config.artifact_name, \n",
    "                                          missing_values_technique=config.missing_values_technique,\n",
    "                                          resampling_freq=config.resampling_freq, \n",
    "                                          normalize=False,\n",
    "                                          path=str(Path.home()/config.wandb_artifacts_path))\n",
    "    display(testing_artifact.metadata)\n",
    "    if df_testing.index.duplicated().any():\n",
    "        print(\"Hay valores duplicados en el Ã­ndice del dataframe.\")\n",
    "    else:\n",
    "        print(\"No hay valores duplicados en el Ã­ndice del dataframe.\")\n",
    "else:\n",
    "    print(\"rg \"+ str(rg) + \" | test_split \"+ str(config.test_split))\n",
    "    testing_artifact = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training + Testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build dataframe & artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training + Testing data\n",
    "if(config.joining_train_test):\n",
    "    print(\"joining_train_test: \"+ str(config.joining_train_test))\n",
    "    df_train_test = pd.concat([df_training, df_testing])\n",
    "    train_test_artifact = TSArtifact.from_df(df_train_test,\n",
    "                                           name=config.artifact_name, \n",
    "                                           missing_values_technique=config.missing_values_technique,\n",
    "                                           resampling_freq=config.resampling_freq, \n",
    "                                           normalize=False,\n",
    "                                           path=str(Path.home()/config.wandb_artifacts_path))\n",
    "    if df_train_test.index.duplicated().any():\n",
    "        print(\"Hay valores duplicados en el Ã­ndice del dataframe.\")\n",
    "    else:\n",
    "        print(\"No hay valores duplicados en el Ã­ndice del dataframe.\")\n",
    "    display(train_test_artifact.metadata)\n",
    "else:\n",
    "    train_test_artifact = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the experiment tracking and hyperparameter we will use the tool **Weights & Biases**. \n",
    "\n",
    "Before running this notebook part, make sure you have the `$WANDB_API_KEY`, `$WANDB_ENTITY` and `$WANDB_PROJECT` environment varibales defined with your API_KEY and your ENTITY and PROJECT names (run in a terminal `echo $WANDB_API_KEY` to see it, same with the other variables). If not, run in a terminal `wandb login [API_KEY]` to set the first one. You can see your API_KEY [here](https://wandb.ai/authorize) or in the settings of your W&B account. Run in a terminal `export WANDB_ENTITY=entity_name` and/or `export WANDB_PROJECT=project_name` to set the other two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.path.expanduser(\"~/work/nbs_pipeline/\")\n",
    "name=\"01_dataset_artifact\"\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = path+name+\".ipynb\"\n",
    "runname=name\n",
    "print(\"runname: \"+runname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mode = 'online' if config.use_wandb else 'disabled'\n",
    "\n",
    "# Make the run that will produce the artifact\n",
    "with wandb.init(job_type='create_dataset', resume=True, mode=mode, config=config, name=runname) as run:\n",
    "    if testing_artifact: \n",
    "        run.log_artifact(training_artifact, aliases=['train'])\n",
    "        run.log_artifact(testing_artifact, aliases=['test'])\n",
    "        \n",
    "        if train_test_artifact:\n",
    "            run.log_artifact(train_test_artifact, aliases=['all'])\n",
    "    \n",
    "    else:\n",
    "        run.log_artifact(training_artifact, aliases=['all'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d45d555be0220b07bf61be557bfa0ebbf7a95015976aec9a23277863e1bd4593"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
