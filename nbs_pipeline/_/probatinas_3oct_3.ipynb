{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "296f19b6-1088-4873-bccf-431a2a47a293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from gluonts.dataset.pandas import PandasDataset\n",
    "from gluonts.dataset.split import split\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "from uni2ts.eval_util.plot import plot_single\n",
    "from uni2ts.model.moirai import MoiraiForecast, MoiraiModule\n",
    "import sys\n",
    "import dvats.utils as ut\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from gluonts.dataset.pandas import PandasDataset\n",
    "from gluonts.dataset.split import split\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "import uni2ts\n",
    "from uni2ts.eval_util.plot import plot_single\n",
    "from uni2ts.model.moirai import MoiraiForecast, MoiraiModule\n",
    "from uni2ts.eval_util.plot import plot_next_multi\n",
    "import pyarrow.feather as ft\n",
    "from gluonts.transform.split import TFTInstanceSplitter\n",
    "from gluonts.transform.sampler import TestSplitSampler\n",
    "import numpy as np\n",
    "import einops\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79b4b27f-0fd8-47fa-aff7-0963f04a39ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total length: 550\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url = \"https://zenodo.org/record/4328047/files/toy.csv?download=1\"\n",
    "df = pd.read_csv(url, parse_dates=True)\n",
    "#df = df[df[\"item_id\"] == \"A\"].drop(columns=[\"item_id\"])\n",
    "print(f\"total length: {df.shape[0]}\")\n",
    "df.plot(figsize=(12, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7525277-a00c-44ba-ba6b-c72475f58e2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(550, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb9b9b98-e8d0-4c23-9dc4-bc5b054393a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.dataset.pandas import PandasDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30acaab2-8b10-40fb-b04e-6cd529396411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a sample for the hourly data, using one week data as context window and predicting the next two days.\n",
    "inp = {\n",
    "    \"target\": df[[\"T1\", \"T2\", \"T3\"]].to_numpy()[:168],  # 168 = 24 * 7\n",
    "    \"start\": 0, #df.index[0].to_period(freq=\"H\"),\n",
    "}\n",
    "label = {\n",
    "    \"target\": df[[\"T1\", \"T2\", \"T3\"]].to_numpy()[168:216],  # 48 = 24 * 2\n",
    "    \"start\": 168, #df.index[168].to_period(freq=\"H\"),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81fe9097-d84b-4e61-96ac-40afeffac899",
   "metadata": {},
   "outputs": [],
   "source": [
    "module = MoiraiModule.from_pretrained(f\"Salesforce/moirai-1.1-R-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98f8a51b-bb59-483d-a87e-c321866c3e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp[\"target\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d801e134-d1a6-447c-b590-cdc6b7a7c1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model\n",
    "model = MoiraiForecast(\n",
    "    module=module,\n",
    "    prediction_length=48,\n",
    "    context_length=168,\n",
    "    patch_size=32,\n",
    "    num_samples=100,\n",
    "    target_dim=inp[\"target\"].shape[1],\n",
    "    feat_dynamic_real_dim=0,\n",
    "    past_feat_dynamic_real_dim=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fc55b79-0b12-42d7-b970-a4672a900219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series values. Shape: (batch, time, variate)\n",
    "past_target = einops.rearrange(\n",
    "    torch.as_tensor(inp[\"target\"], dtype=torch.float32), \"t v-> 1 t v\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d24e6958-e5e4-4631-8a15-aa92b8d11e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(168, 3)\n",
      "torch.Size([1, 168, 3])\n"
     ]
    }
   ],
   "source": [
    "print(inp[\"target\"].shape)\n",
    "print(past_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdbcef11-9a4c-476e-8396-7535612eb204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1s if the value is observed, 0s otherwise. Shape: (batch, time, variate)\n",
    "past_observed_target = torch.ones_like(past_target, dtype=torch.bool)\n",
    "# 1s if the value is padding, 0s otherwise. Shape: (batch, time)\n",
    "past_is_pad = torch.zeros_like(past_target, dtype=torch.bool)[...,:,-1] # Kill last dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80e69868-8462-4104-b253-86a3f4f1406a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 168, 3])\n",
      "torch.Size([1, 168, 3])\n",
      "torch.Size([1, 168])\n"
     ]
    }
   ],
   "source": [
    "print(past_target.shape)\n",
    "print(past_observed_target.shape)\n",
    "print(past_is_pad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "168c314c-8401-45ee-93b1-9d741ab37e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_convert | patch_size ~ 32\n",
      "_convert | past_target ~ torch.Size([1, 168, 3])\n",
      "_convert | past_observed_target ~ torch.Size([1, 168, 3])\n",
      "_convert | past_is_pad ~ torch.Size([1, 168])\n",
      "_convert | future_target ~  torch.Size([1, 48, 3])\n",
      "_convert | target before extend ~  0\n",
      "_convert | target extend ~  2\n",
      "_convert | future_observed_target ~  torch.Size([1, 48, 3])\n",
      "_convert | observed_mask before extend ~  0\n",
      "_convert | observed_mask extend ~  2\n",
      "_convert | future_is_pad ~  torch.Size([1, 48])\n",
      "_convert | sample_id before extend ~  0\n",
      "_convert | sample_id after extend ~  2\n",
      "_convert | time_id after extend ~  6\n",
      "_convert | variate_id after extend ~  2\n",
      "_convert --> |  target~torch.Size([1, 24, 128])\n",
      "_convert --> |  observed_mask~ torch.Size([1, 24, 128])\n",
      "_convert --> |  sample_id~torch.Size([1, 24])\n",
      "_convert --> |  time_id~torch.Size([1, 24])\n",
      "_convert --> |  variate_id~torch.Size([1, 24])\n",
      "_convert --> |  prediction_mask~torch.Size([1, 24])\n",
      "_convert | Target ~ torch.Size([1, 24, 128])\n",
      "_convert | observed_mask ~ torch.Size([1, 24, 128])\n",
      "_convert |prediction_mask ~ torch.Size([1, 24])\n",
      "_convert |sample_id ~ torch.Size([1, 24])\n",
      "_convert |time_id ~ torch.Size([1, 24])\n",
      "_convert |variate_id ~ torch.Size([1, 24])\n"
     ]
    }
   ],
   "source": [
    "forecast = model(\n",
    "    past_target=past_target,\n",
    "    past_observed_target=past_observed_target,\n",
    "    past_is_pad=past_is_pad,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "707908ab-9499-4e3a-bc7e-0485a718d4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 168, 3])\n",
      "torch.Size([1, 168, 3])\n",
      "torch.Size([1, 168])\n"
     ]
    }
   ],
   "source": [
    "print(past_target.shape)\n",
    "print(past_observed_target.shape)\n",
    "print(past_is_pad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18e56434-7ec7-491e-a078-1316fcae3284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(168//32)*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1773c24-1030-454c-aec8-14fbfd18eea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_convert | patch_size ~ 32\n",
      "_convert | past_target ~ torch.Size([1, 168, 3])\n",
      "_convert | past_observed_target ~ torch.Size([1, 168, 3])\n",
      "_convert | past_is_pad ~ torch.Size([1, 168])\n",
      "_convert | future_target ~  torch.Size([1, 48, 3])\n",
      "_convert | target before extend ~  0\n",
      "_convert | target extend ~  2\n",
      "_convert | future_observed_target ~  torch.Size([1, 48, 3])\n",
      "_convert | observed_mask before extend ~  0\n",
      "_convert | observed_mask extend ~  2\n",
      "_convert | future_is_pad ~  torch.Size([1, 48])\n",
      "_convert | sample_id before extend ~  0\n",
      "_convert | sample_id after extend ~  2\n",
      "_convert | time_id after extend ~  6\n",
      "_convert | variate_id after extend ~  2\n",
      "_convert --> |  target~torch.Size([1, 24, 128])\n",
      "_convert --> |  observed_mask~ torch.Size([1, 24, 128])\n",
      "_convert --> |  sample_id~torch.Size([1, 24])\n",
      "_convert --> |  time_id~torch.Size([1, 24])\n",
      "_convert --> |  variate_id~torch.Size([1, 24])\n",
      "_convert --> |  prediction_mask~torch.Size([1, 24])\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    target,\n",
    "    observed_mask,\n",
    "    sample_id,\n",
    "    time_id,\n",
    "    variate_id,\n",
    "    prediction_mask,\n",
    ") = model._convert(\n",
    "    32,\n",
    "    past_target,\n",
    "    past_observed_target,\n",
    "    past_is_pad\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1d75b65-9a28-4b73-86ab-8ff2eae03ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d76702f-b106-4f8f-a5fb-04cf7fe5d465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.ceil(48/32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4072999-b2b1-423f-a614-c9c9f48b21b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = module(\n",
    "    target = target,\n",
    "    observed_mask = observed_mask, \n",
    "    sample_id = sample_id,\n",
    "    time_id = time_id,\n",
    "    variate_id = variate_id,\n",
    "    prediction_mask = prediction_mask,\n",
    "    patch_size = torch.ones((target.shape[0], target.shape[1]))*32.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69619c6c-74fd-4f27-9c21-fdbeb4c8e9a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AffineTransformed()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6c80716-e12c-4f5d-840e-7018875fd1f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       " \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjaxtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'*batch seq_len max_patch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mobserved_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjaxtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBool\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'*batch seq_len max_patch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msample_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjaxtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'*batch seq_len'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtime_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjaxtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'*batch seq_len'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mvariate_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjaxtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'*batch seq_len'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mprediction_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjaxtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBool\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'*batch seq_len'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjaxtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'*batch seq_len'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Defines the forward pass of MoiraiModule.\n",
       "This method expects processed inputs.\n",
       "\n",
       "1. Apply scaling to observations\n",
       "2. Project from observations to representations\n",
       "3. Replace prediction window with learnable mask\n",
       "4. Apply transformer layers\n",
       "5. Project from representations to distribution parameters\n",
       "6. Return distribution object\n",
       "\n",
       ":param target: input data\n",
       ":param observed_mask: binary mask for missing values, 1 if observed, 0 otherwise\n",
       ":param sample_id: indices indicating the sample index (for packing)\n",
       ":param time_id: indices indicating the time index\n",
       ":param variate_id: indices indicating the variate index\n",
       ":param prediction_mask: binary mask for prediction horizon, 1 if part of the horizon, 0 otherwise\n",
       ":param patch_size: patch size for each token\n",
       ":return: predictive distribution\n",
       "\u001b[0;31mFile:\u001b[0m      ~/work/nbs_pipeline/uni2ts/src/uni2ts/model/moirai/module.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "? module.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b88b2bbd-6ade-48f1-9f60-b22cc5e3c8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerboseExecution(torch.nn.Module):\n",
    "    def __init__(self, model: torch.nn.Module):\n",
    "        try:\n",
    "            super().__init__()\n",
    "        except:\n",
    "            print(\"Asumming model has already been initialized\")\n",
    "        self.model = model\n",
    "\n",
    "        # Register a hook for each layer\n",
    "        for name, layer in self.model.named_children():\n",
    "            layer.__name__ = name\n",
    "            layer.register_forward_hook(\n",
    "                lambda layer, _, output: print(f\"{layer.__name__}: {output}\")\n",
    "            )\n",
    "\n",
    "    def forward(self, **module_kwargs):\n",
    "        return self.model(**module_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c1bd48e-f389-4fde-aabc-2d656abfe659",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerboseMoiraiForecast(torch.nn.Module):\n",
    "    def __init__(self, model: MoiraiForecast):\n",
    "        try:\n",
    "            super().__init__()\n",
    "        except:\n",
    "            print(\"Asumming model has already been initialized\")\n",
    "        self.model = model\n",
    "\n",
    "        # Register a hook for each layer\n",
    "        for name, layer in self.model.named_children():\n",
    "            layer.__name__ = name\n",
    "            layer.register_forward_hook(\n",
    "                lambda layer, _, output: print(f\"{layer.__name__}: {output}\")\n",
    "            )\n",
    "\n",
    "    def forward(self, **module_kwargs):\n",
    "        return self.model(**module_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2709c864-dacd-43d4-8e25-6ce6a20d1e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose_model = VerboseExecution(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34174ba5-70ad-4d33-af8b-cbe24b85f04c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       " \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjaxtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'*batch seq_len max_patch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mobserved_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjaxtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBool\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'*batch seq_len max_patch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msample_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjaxtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'*batch seq_len'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtime_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjaxtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'*batch seq_len'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mvariate_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjaxtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'*batch seq_len'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mprediction_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjaxtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBool\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'*batch seq_len'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjaxtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'*batch seq_len'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Defines the forward pass of MoiraiModule.\n",
       "This method expects processed inputs.\n",
       "\n",
       "1. Apply scaling to observations\n",
       "2. Project from observations to representations\n",
       "3. Replace prediction window with learnable mask\n",
       "4. Apply transformer layers\n",
       "5. Project from representations to distribution parameters\n",
       "6. Return distribution object\n",
       "\n",
       ":param target: input data\n",
       ":param observed_mask: binary mask for missing values, 1 if observed, 0 otherwise\n",
       ":param sample_id: indices indicating the sample index (for packing)\n",
       ":param time_id: indices indicating the time index\n",
       ":param variate_id: indices indicating the variate index\n",
       ":param prediction_mask: binary mask for prediction horizon, 1 if part of the horizon, 0 otherwise\n",
       ":param patch_size: patch size for each token\n",
       ":return: predictive distribution\n",
       "\u001b[0;31mFile:\u001b[0m      ~/work/nbs_pipeline/uni2ts/src/uni2ts/model/moirai/module.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "? module.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "781e2b19-939b-4a25-83f9-40c70d01af91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaler: (tensor([[[0.3261],\n",
      "         [0.3261],\n",
      "         [0.3261],\n",
      "         [0.3261],\n",
      "         [0.3261],\n",
      "         [0.3261],\n",
      "         [0.4834],\n",
      "         [0.4834],\n",
      "         [0.4834],\n",
      "         [0.4834],\n",
      "         [0.4834],\n",
      "         [0.4834],\n",
      "         [0.4883],\n",
      "         [0.4883],\n",
      "         [0.4883],\n",
      "         [0.4883],\n",
      "         [0.4883],\n",
      "         [0.4883],\n",
      "         [0.3261],\n",
      "         [0.3261],\n",
      "         [0.4834],\n",
      "         [0.4834],\n",
      "         [0.4883],\n",
      "         [0.4883]]]), tensor([[[0.1359],\n",
      "         [0.1359],\n",
      "         [0.1359],\n",
      "         [0.1359],\n",
      "         [0.1359],\n",
      "         [0.1359],\n",
      "         [0.1380],\n",
      "         [0.1380],\n",
      "         [0.1380],\n",
      "         [0.1380],\n",
      "         [0.1380],\n",
      "         [0.1380],\n",
      "         [0.2568],\n",
      "         [0.2568],\n",
      "         [0.2568],\n",
      "         [0.2568],\n",
      "         [0.2568],\n",
      "         [0.2568],\n",
      "         [0.1359],\n",
      "         [0.1359],\n",
      "         [0.1380],\n",
      "         [0.1380],\n",
      "         [0.2568],\n",
      "         [0.2568]]]))\n",
      "in_proj: tensor([[[ 0.1473,  0.3972,  0.3520,  ...,  0.2306, -0.1476,  0.2901],\n",
      "         [-0.0434,  0.0168,  0.2162,  ...,  0.0494, -0.0658,  0.2191],\n",
      "         [-0.0939,  0.0242,  0.2115,  ...,  0.0534, -0.0681,  0.1983],\n",
      "         ...,\n",
      "         [ 0.0314,  0.0286, -0.7216,  ...,  0.0284, -0.0995,  0.0419],\n",
      "         [ 0.0160,  0.0124, -0.3768,  ...,  0.0152, -0.0542,  0.0986],\n",
      "         [ 0.0160,  0.0124, -0.3768,  ...,  0.0152, -0.0542,  0.0986]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "encoder: tensor([[[-4.0408e-03, -7.2437e-02, -3.8505e-01,  ...,  3.4193e-02,\n",
      "          -1.3278e-02,  2.9317e-01],\n",
      "         [-7.7536e-04, -3.0185e-01, -3.1727e-01,  ..., -3.7425e-03,\n",
      "           3.6138e-02, -5.3705e+00],\n",
      "         [-5.8768e-04,  3.4007e-02,  4.2628e-01,  ..., -2.7119e-02,\n",
      "           7.7394e-03, -5.9764e+00],\n",
      "         ...,\n",
      "         [-1.4654e-03, -2.8225e-02, -1.5043e-01,  ...,  2.5066e-02,\n",
      "          -1.7496e-02,  1.3423e-02],\n",
      "         [-4.5693e-03, -1.2363e-02, -2.6141e-01,  ..., -1.0631e-02,\n",
      "          -1.5689e-02, -2.9884e-01],\n",
      "         [-2.2413e-03, -8.2426e-02, -2.7241e-01,  ...,  2.9781e-02,\n",
      "          -1.8295e-02, -1.4401e+00]]], grad_fn=<MulBackward0>)\n",
      "param_proj: {'weights_logits': tensor([[[[ 2.4898, -6.0223, -3.2138,  1.7527],\n",
      "          [ 2.5302, -6.0880, -3.1362,  1.7263],\n",
      "          [ 2.7601, -6.0299, -3.3804,  1.7311],\n",
      "          ...,\n",
      "          [-0.0210,  0.0000,  0.0000,  0.0000],\n",
      "          [-0.0493,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0410,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 5.0682, -6.0223, -3.2138,  1.7527],\n",
      "          [ 4.1304, -6.0880, -3.1362,  1.7263],\n",
      "          [ 3.7353, -6.0299, -3.3804,  1.7311],\n",
      "          ...,\n",
      "          [-0.0217,  0.0000,  0.0000,  0.0000],\n",
      "          [-0.0438,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0401,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 5.2812, -6.0223, -3.2138,  1.7527],\n",
      "          [ 4.4269, -6.0880, -3.1362,  1.7263],\n",
      "          [ 3.9409, -6.0299, -3.3804,  1.7311],\n",
      "          ...,\n",
      "          [-0.0220,  0.0000,  0.0000,  0.0000],\n",
      "          [-0.0453,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0415,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.1289, -6.0223, -3.2138,  1.7527],\n",
      "          [ 2.0500, -6.0880, -3.1362,  1.7263],\n",
      "          [ 2.0513, -6.0299, -3.3804,  1.7311],\n",
      "          ...,\n",
      "          [-0.0209,  0.0000,  0.0000,  0.0000],\n",
      "          [-0.0477,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0410,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 7.5348, -6.0223, -3.2138,  1.7527],\n",
      "          [ 6.5441, -6.0880, -3.1362,  1.7263],\n",
      "          [ 5.9918, -6.0299, -3.3804,  1.7311],\n",
      "          ...,\n",
      "          [-0.0228,  0.0000,  0.0000,  0.0000],\n",
      "          [-0.0473,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0389,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 4.1612, -6.0223, -3.2138,  1.7527],\n",
      "          [ 3.9727, -6.0880, -3.1362,  1.7263],\n",
      "          [ 4.0293, -6.0299, -3.3804,  1.7311],\n",
      "          ...,\n",
      "          [-0.0215,  0.0000,  0.0000,  0.0000],\n",
      "          [-0.0466,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0397,  0.0000,  0.0000,  0.0000]]]], grad_fn=<PermuteBackward0>), 'components': [{'df': tensor([[[2.8641, 2.9000, 2.9429,  ..., 2.6931, 2.6931, 2.6931],\n",
      "         [2.2770, 2.1930, 2.1412,  ..., 2.6931, 2.6931, 2.6931],\n",
      "         [3.4029, 3.3792, 3.6570,  ..., 2.6931, 2.6931, 2.6931],\n",
      "         ...,\n",
      "         [6.3486, 6.7595, 6.9342,  ..., 2.6931, 2.6931, 2.6931],\n",
      "         [6.6440, 6.5607, 6.6623,  ..., 2.6931, 2.6931, 2.6931],\n",
      "         [4.4285, 4.5819, 4.6074,  ..., 2.6931, 2.6931, 2.6931]]],\n",
      "       grad_fn=<SqueezeBackward1>), 'loc': tensor([[[ 0.1226,  0.1233,  0.1139,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.1790, -1.1252, -1.1019,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.3957, -1.4506, -1.4500,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-0.1654, -0.1287, -0.0994,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.4566, -0.4089, -0.3725,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.5290, -0.5155, -0.5029,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<SqueezeBackward1>), 'scale': tensor([[[0.0919, 0.1017, 0.1165,  ..., 0.6931, 0.6931, 0.6931],\n",
      "         [0.4250, 0.5652, 0.6322,  ..., 0.6931, 0.6931, 0.6931],\n",
      "         [0.4034, 0.5089, 0.5573,  ..., 0.6931, 0.6931, 0.6931],\n",
      "         ...,\n",
      "         [1.0397, 1.0332, 1.0230,  ..., 0.6931, 0.6931, 0.6931],\n",
      "         [0.1434, 0.2233, 0.2825,  ..., 0.6931, 0.6931, 0.6931],\n",
      "         [0.8188, 0.8210, 0.8156,  ..., 0.6931, 0.6931, 0.6931]]],\n",
      "       grad_fn=<SqueezeBackward1>)}, {'loc': tensor([[[-0.2405, -0.8622,  0.9963,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.8450,  0.7336,  0.8501,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.4140,  0.4648,  1.3308,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-0.2109, -0.5761,  1.6489,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7014, -0.2611,  1.5758,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.4974, -0.4947,  1.7928,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<SqueezeBackward1>)}, {'total_count': tensor([[[1.3186, 1.4309, 1.3370,  ..., 0.6931, 0.6931, 0.6931],\n",
      "         [0.4666, 0.4730, 0.4109,  ..., 0.6931, 0.6931, 0.6931],\n",
      "         [0.4593, 0.4478, 0.4242,  ..., 0.6931, 0.6931, 0.6931],\n",
      "         ...,\n",
      "         [1.1836, 1.2996, 1.1995,  ..., 0.6931, 0.6931, 0.6931],\n",
      "         [0.9479, 1.0601, 0.9225,  ..., 0.6931, 0.6931, 0.6931],\n",
      "         [0.9984, 1.1237, 0.9761,  ..., 0.6931, 0.6931, 0.6931]]],\n",
      "       grad_fn=<SqueezeBackward1>), 'logits': tensor([[[1.6301, 1.4877, 1.6889,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.9718, 1.1021, 1.2244,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [2.4487, 2.3909, 2.4570,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [2.4198, 2.3521, 2.6003,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [1.4649, 1.4935, 1.9088,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [2.3396, 2.2271, 2.5712,  ..., 0.0000, 0.0000, 0.0000]]],\n",
      "       grad_fn=<SqueezeBackward1>)}, {'loc': tensor([[[ 0.1581,  0.1456,  0.0803,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.1186,  0.0999,  0.1760,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.4890,  0.6321,  0.6719,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 1.2552,  1.2241,  1.2235,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.2978, -1.0291, -0.8653,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.1929,  0.2115,  0.1921,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<SqueezeBackward1>), 'scale': tensor([[[1.2989, 1.3326, 1.3480,  ..., 0.6931, 0.6931, 0.6931],\n",
      "         [1.1436, 1.0577, 1.0747,  ..., 0.6931, 0.6931, 0.6931],\n",
      "         [1.9073, 1.8378, 1.7902,  ..., 0.6931, 0.6931, 0.6931],\n",
      "         ...,\n",
      "         [0.6377, 0.6209, 0.6128,  ..., 0.6931, 0.6931, 0.6931],\n",
      "         [0.6453, 0.6855, 0.6966,  ..., 0.6931, 0.6931, 0.6931],\n",
      "         [0.7025, 0.6746, 0.6932,  ..., 0.6931, 0.6931, 0.6931]]],\n",
      "       grad_fn=<SqueezeBackward1>)}]}\n"
     ]
    }
   ],
   "source": [
    "forecasts = verbose_model(\n",
    "    target = target,\n",
    "    observed_mask = observed_mask,\n",
    "    sample_id = sample_id,\n",
    "    time_id = time_id,\n",
    "    variate_id = variate_id,\n",
    "    prediction_mask = prediction_mask,\n",
    "    patch_size = torch.ones((target.shape[0], target.shape[1]))*32.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f07ae4c-f2f3-4632-9eba-72b553bd9e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose_model = VerboseExecution(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7cf67fc8-9d41-4825-8ff0-446c96fff551",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose_forecast_model = VerboseMoiraiForecast(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee2bcd4b-9bfb-4fcb-bcdc-8167e1336d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_convert | patch_size ~ 32\n",
      "_convert | past_target ~ torch.Size([1, 168, 3])\n",
      "_convert | past_observed_target ~ torch.Size([1, 168, 3])\n",
      "_convert | past_is_pad ~ torch.Size([1, 168])\n",
      "_convert | future_target ~  torch.Size([1, 48, 3])\n",
      "_convert | target before extend ~  0\n",
      "_convert | target extend ~  2\n",
      "_convert | future_observed_target ~  torch.Size([1, 48, 3])\n",
      "_convert | observed_mask before extend ~  0\n",
      "_convert | observed_mask extend ~  2\n",
      "_convert | future_is_pad ~  torch.Size([1, 48])\n",
      "_convert | sample_id before extend ~  0\n",
      "_convert | sample_id after extend ~  2\n",
      "_convert | time_id after extend ~  6\n",
      "_convert | variate_id after extend ~  2\n",
      "_convert --> |  target~torch.Size([1, 24, 128])\n",
      "_convert --> |  observed_mask~ torch.Size([1, 24, 128])\n",
      "_convert --> |  sample_id~torch.Size([1, 24])\n",
      "_convert --> |  time_id~torch.Size([1, 24])\n",
      "_convert --> |  variate_id~torch.Size([1, 24])\n",
      "_convert --> |  prediction_mask~torch.Size([1, 24])\n",
      "_convert | Target ~ torch.Size([1, 24, 128])\n",
      "_convert | observed_mask ~ torch.Size([1, 24, 128])\n",
      "_convert |prediction_mask ~ torch.Size([1, 24])\n",
      "_convert |sample_id ~ torch.Size([1, 24])\n",
      "_convert |time_id ~ torch.Size([1, 24])\n",
      "_convert |variate_id ~ torch.Size([1, 24])\n",
      "scaler: (tensor([[[0.3261],\n",
      "         [0.3261],\n",
      "         [0.3261],\n",
      "         [0.3261],\n",
      "         [0.3261],\n",
      "         [0.3261],\n",
      "         [0.4834],\n",
      "         [0.4834],\n",
      "         [0.4834],\n",
      "         [0.4834],\n",
      "         [0.4834],\n",
      "         [0.4834],\n",
      "         [0.4883],\n",
      "         [0.4883],\n",
      "         [0.4883],\n",
      "         [0.4883],\n",
      "         [0.4883],\n",
      "         [0.4883],\n",
      "         [0.3261],\n",
      "         [0.3261],\n",
      "         [0.4834],\n",
      "         [0.4834],\n",
      "         [0.4883],\n",
      "         [0.4883]]]), tensor([[[0.1359],\n",
      "         [0.1359],\n",
      "         [0.1359],\n",
      "         [0.1359],\n",
      "         [0.1359],\n",
      "         [0.1359],\n",
      "         [0.1380],\n",
      "         [0.1380],\n",
      "         [0.1380],\n",
      "         [0.1380],\n",
      "         [0.1380],\n",
      "         [0.1380],\n",
      "         [0.2568],\n",
      "         [0.2568],\n",
      "         [0.2568],\n",
      "         [0.2568],\n",
      "         [0.2568],\n",
      "         [0.2568],\n",
      "         [0.1359],\n",
      "         [0.1359],\n",
      "         [0.1380],\n",
      "         [0.1380],\n",
      "         [0.2568],\n",
      "         [0.2568]]]))\n",
      "in_proj: tensor([[[ 0.1473,  0.3972,  0.3520,  ...,  0.2306, -0.1476,  0.2901],\n",
      "         [-0.0434,  0.0168,  0.2162,  ...,  0.0494, -0.0658,  0.2191],\n",
      "         [-0.0939,  0.0242,  0.2115,  ...,  0.0534, -0.0681,  0.1983],\n",
      "         ...,\n",
      "         [ 0.0314,  0.0286, -0.7216,  ...,  0.0284, -0.0995,  0.0419],\n",
      "         [ 0.0160,  0.0124, -0.3768,  ...,  0.0152, -0.0542,  0.0986],\n",
      "         [ 0.0160,  0.0124, -0.3768,  ...,  0.0152, -0.0542,  0.0986]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "encoder: tensor([[[-4.0408e-03, -7.2437e-02, -3.8505e-01,  ...,  3.4193e-02,\n",
      "          -1.3278e-02,  2.9317e-01],\n",
      "         [-7.7536e-04, -3.0185e-01, -3.1727e-01,  ..., -3.7425e-03,\n",
      "           3.6138e-02, -5.3705e+00],\n",
      "         [-5.8768e-04,  3.4007e-02,  4.2628e-01,  ..., -2.7119e-02,\n",
      "           7.7394e-03, -5.9764e+00],\n",
      "         ...,\n",
      "         [-1.4654e-03, -2.8225e-02, -1.5043e-01,  ...,  2.5066e-02,\n",
      "          -1.7496e-02,  1.3423e-02],\n",
      "         [-4.5693e-03, -1.2363e-02, -2.6141e-01,  ..., -1.0631e-02,\n",
      "          -1.5689e-02, -2.9884e-01],\n",
      "         [-2.2413e-03, -8.2426e-02, -2.7241e-01,  ...,  2.9781e-02,\n",
      "          -1.8295e-02, -1.4401e+00]]], grad_fn=<MulBackward0>)\n",
      "param_proj: {'weights_logits': tensor([[[[ 2.4898, -6.0223, -3.2138,  1.7527],\n",
      "          [ 2.5302, -6.0880, -3.1362,  1.7263],\n",
      "          [ 2.7601, -6.0299, -3.3804,  1.7311],\n",
      "          ...,\n",
      "          [-0.0210,  0.0000,  0.0000,  0.0000],\n",
      "          [-0.0493,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0410,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 5.0682, -6.0223, -3.2138,  1.7527],\n",
      "          [ 4.1304, -6.0880, -3.1362,  1.7263],\n",
      "          [ 3.7353, -6.0299, -3.3804,  1.7311],\n",
      "          ...,\n",
      "          [-0.0217,  0.0000,  0.0000,  0.0000],\n",
      "          [-0.0438,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0401,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 5.2812, -6.0223, -3.2138,  1.7527],\n",
      "          [ 4.4269, -6.0880, -3.1362,  1.7263],\n",
      "          [ 3.9409, -6.0299, -3.3804,  1.7311],\n",
      "          ...,\n",
      "          [-0.0220,  0.0000,  0.0000,  0.0000],\n",
      "          [-0.0453,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0415,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.1289, -6.0223, -3.2138,  1.7527],\n",
      "          [ 2.0500, -6.0880, -3.1362,  1.7263],\n",
      "          [ 2.0513, -6.0299, -3.3804,  1.7311],\n",
      "          ...,\n",
      "          [-0.0209,  0.0000,  0.0000,  0.0000],\n",
      "          [-0.0477,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0410,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 7.5348, -6.0223, -3.2138,  1.7527],\n",
      "          [ 6.5441, -6.0880, -3.1362,  1.7263],\n",
      "          [ 5.9918, -6.0299, -3.3804,  1.7311],\n",
      "          ...,\n",
      "          [-0.0228,  0.0000,  0.0000,  0.0000],\n",
      "          [-0.0473,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0389,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 4.1612, -6.0223, -3.2138,  1.7527],\n",
      "          [ 3.9727, -6.0880, -3.1362,  1.7263],\n",
      "          [ 4.0293, -6.0299, -3.3804,  1.7311],\n",
      "          ...,\n",
      "          [-0.0215,  0.0000,  0.0000,  0.0000],\n",
      "          [-0.0466,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0397,  0.0000,  0.0000,  0.0000]]]], grad_fn=<PermuteBackward0>), 'components': [{'df': tensor([[[2.8641, 2.9000, 2.9429,  ..., 2.6931, 2.6931, 2.6931],\n",
      "         [2.2770, 2.1930, 2.1412,  ..., 2.6931, 2.6931, 2.6931],\n",
      "         [3.4029, 3.3792, 3.6570,  ..., 2.6931, 2.6931, 2.6931],\n",
      "         ...,\n",
      "         [6.3486, 6.7595, 6.9342,  ..., 2.6931, 2.6931, 2.6931],\n",
      "         [6.6440, 6.5607, 6.6623,  ..., 2.6931, 2.6931, 2.6931],\n",
      "         [4.4285, 4.5819, 4.6074,  ..., 2.6931, 2.6931, 2.6931]]],\n",
      "       grad_fn=<SqueezeBackward1>), 'loc': tensor([[[ 0.1226,  0.1233,  0.1139,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.1790, -1.1252, -1.1019,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.3957, -1.4506, -1.4500,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-0.1654, -0.1287, -0.0994,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.4566, -0.4089, -0.3725,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.5290, -0.5155, -0.5029,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<SqueezeBackward1>), 'scale': tensor([[[0.0919, 0.1017, 0.1165,  ..., 0.6931, 0.6931, 0.6931],\n",
      "         [0.4250, 0.5652, 0.6322,  ..., 0.6931, 0.6931, 0.6931],\n",
      "         [0.4034, 0.5089, 0.5573,  ..., 0.6931, 0.6931, 0.6931],\n",
      "         ...,\n",
      "         [1.0397, 1.0332, 1.0230,  ..., 0.6931, 0.6931, 0.6931],\n",
      "         [0.1434, 0.2233, 0.2825,  ..., 0.6931, 0.6931, 0.6931],\n",
      "         [0.8188, 0.8210, 0.8156,  ..., 0.6931, 0.6931, 0.6931]]],\n",
      "       grad_fn=<SqueezeBackward1>)}, {'loc': tensor([[[-0.2405, -0.8622,  0.9963,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.8450,  0.7336,  0.8501,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.4140,  0.4648,  1.3308,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-0.2109, -0.5761,  1.6489,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7014, -0.2611,  1.5758,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.4974, -0.4947,  1.7928,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<SqueezeBackward1>)}, {'total_count': tensor([[[1.3186, 1.4309, 1.3370,  ..., 0.6931, 0.6931, 0.6931],\n",
      "         [0.4666, 0.4730, 0.4109,  ..., 0.6931, 0.6931, 0.6931],\n",
      "         [0.4593, 0.4478, 0.4242,  ..., 0.6931, 0.6931, 0.6931],\n",
      "         ...,\n",
      "         [1.1836, 1.2996, 1.1995,  ..., 0.6931, 0.6931, 0.6931],\n",
      "         [0.9479, 1.0601, 0.9225,  ..., 0.6931, 0.6931, 0.6931],\n",
      "         [0.9984, 1.1237, 0.9761,  ..., 0.6931, 0.6931, 0.6931]]],\n",
      "       grad_fn=<SqueezeBackward1>), 'logits': tensor([[[1.6301, 1.4877, 1.6889,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.9718, 1.1021, 1.2244,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [2.4487, 2.3909, 2.4570,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [2.4198, 2.3521, 2.6003,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [1.4649, 1.4935, 1.9088,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [2.3396, 2.2271, 2.5712,  ..., 0.0000, 0.0000, 0.0000]]],\n",
      "       grad_fn=<SqueezeBackward1>)}, {'loc': tensor([[[ 0.1581,  0.1456,  0.0803,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.1186,  0.0999,  0.1760,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.4890,  0.6321,  0.6719,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 1.2552,  1.2241,  1.2235,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.2978, -1.0291, -0.8653,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.1929,  0.2115,  0.1921,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<SqueezeBackward1>), 'scale': tensor([[[1.2989, 1.3326, 1.3480,  ..., 0.6931, 0.6931, 0.6931],\n",
      "         [1.1436, 1.0577, 1.0747,  ..., 0.6931, 0.6931, 0.6931],\n",
      "         [1.9073, 1.8378, 1.7902,  ..., 0.6931, 0.6931, 0.6931],\n",
      "         ...,\n",
      "         [0.6377, 0.6209, 0.6128,  ..., 0.6931, 0.6931, 0.6931],\n",
      "         [0.6453, 0.6855, 0.6966,  ..., 0.6931, 0.6931, 0.6931],\n",
      "         [0.7025, 0.6746, 0.6932,  ..., 0.6931, 0.6931, 0.6931]]],\n",
      "       grad_fn=<SqueezeBackward1>)}]}\n",
      "module: AffineTransformed()\n",
      "module: AffineTransformed()\n"
     ]
    }
   ],
   "source": [
    "forecast = verbose_forecast_model(\n",
    "    past_target=past_target,\n",
    "    past_observed_target=past_observed_target,\n",
    "    past_is_pad=past_is_pad,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aad4a92f-f916-4681-9886-990e3c3331c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = verbose_forecast_model.model.create_predictor(batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36ae10f6-9fe0-4c34-90a8-49aa9c2a3e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = predictor.predict(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3ecf01cf-2fcd-4654-a048-f09cd0e1c2ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m        Tensor\n",
       "\u001b[0;31mString form:\u001b[0m\n",
       "tensor([[[[ 0.3411,  0.9530,  0.3353],\n",
       "           [ 0.3122,  0.6123,  0.4024],\n",
       "           [ 0.2464, <...> .4287,  0.1193],\n",
       "           [ 0.2072,  0.5882,  0.4531],\n",
       "           [ 0.4335,  0.9185,  0.3483]]]])\n",
       "\u001b[0;31mLength:\u001b[0m      1\n",
       "\u001b[0;31mFile:\u001b[0m        /usr/local/share/miniconda3/envs/env/lib/python3.10/site-packages/torch/__init__.py\n",
       "\u001b[0;31mDocstring:\u001b[0m   <no docstring>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "? forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146ec8be-5d5a-4011-ad73-3391723877ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517f501e-1406-40f0-8d4c-2b9e05ba9cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1674da4d-06f1-4f21-bc3d-a4df994dc7be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40b634d-4d1c-407f-afe0-f500dee69cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d13daf96-c3db-409a-a0af-d79c155dc63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def verbose_predict(original_predict):\n",
    "    @functools.wraps(original_predict)\n",
    "    def wrapper(self, *args, **kwargs):\n",
    "        print(\"Entrando a predict...\")\n",
    "        result = original_predict(self, *args, **kwargs)\n",
    "        print(\"Saliendo de predict...\")\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2f26b4a1-fcfa-45ea-aeef-c69d262e608e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import trace\n",
    "\n",
    "def trace_calls(frame, event, arg):\n",
    "    if event != 'call':\n",
    "        return\n",
    "    co = frame.f_code\n",
    "    func_name = co.co_name\n",
    "    if func_name == 'write':\n",
    "        # Ignore write() calls from printing\n",
    "        return\n",
    "    func_line_no = frame.f_lineno\n",
    "    func_filename = co.co_filename\n",
    "    print(f\"Call to {func_name} on line {func_line_no} of {func_filename}\")\n",
    "    return trace_calls\n",
    "\n",
    "def verbose_trace(original_func):\n",
    "    tracer = trace.Trace(count=False, trace=True)\n",
    "    \n",
    "    @functools.wraps(original_func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        print(\"Starting trace...\")\n",
    "        tracer.runfunc(original_func, *args, **kwargs)\n",
    "        print(\"Ending trace...\")\n",
    "        \n",
    "    return wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "04f2f13f-8617-4812-8d5d-b7f3bb3e4168",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict = verbose_predict(predictor.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "04480da7-f559-454e-b1ef-9a388c1dae87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrando a predict...\n",
      "Saliendo de predict...\n"
     ]
    }
   ],
   "source": [
    "forecasts = predictor.predict(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "53a6bd34-40b0-4a58-a27b-2cbac2488d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.modules of MoiraiForecast(\n",
       "  (module): MoiraiModule(\n",
       "    (mask_encoding): Embedding(1, 384)\n",
       "    (scaler): PackedStdScaler()\n",
       "    (in_proj): MultiInSizeLinear(in_features_ls=[8, 16, 32, 64, 128], out_features=384, bias=True, dtype=torch.float32)\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerEncoderLayer(\n",
       "          (self_attn): GroupedQueryAttention(\n",
       "            (var_attn_bias): BinaryAttentionBias(\n",
       "              (emb): Embedding(2, 6)\n",
       "            )\n",
       "            (time_qk_proj): QueryKeyProjection(\n",
       "              (query_proj): RotaryProjection()\n",
       "              (key_proj): RotaryProjection()\n",
       "            )\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (q_norm): RMSNorm(normalized_shape=(64,), eps=1e-05, weight=True)\n",
       "            (k_norm): RMSNorm(normalized_shape=(64,), eps=1e-05, weight=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          )\n",
       "          (ffn): GatedLinearUnitFeedForward(\n",
       "            (fc1): Linear(in_features=384, out_features=1024, bias=False)\n",
       "            (fc2): Linear(in_features=1024, out_features=384, bias=False)\n",
       "            (dropout1): Dropout(p=0.0, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (fc_gate): Linear(in_features=384, out_features=1024, bias=False)\n",
       "          )\n",
       "          (norm1): RMSNorm(normalized_shape=(384,), eps=1e-05, weight=True)\n",
       "          (norm2): RMSNorm(normalized_shape=(384,), eps=1e-05, weight=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): RMSNorm(normalized_shape=(384,), eps=1e-05, weight=True)\n",
       "    )\n",
       "    (param_proj): DistrParamProj(\n",
       "      (proj): ModuleDict(\n",
       "        (weights_logits): MultiOutSizeLinear(in_features=384, out_features_ls=(32, 64, 128, 256, 512), bias=True, dtype=torch.float32)\n",
       "        (components): ModuleList(\n",
       "          (0): ModuleDict(\n",
       "            (df): MultiOutSizeLinear(in_features=384, out_features_ls=(8, 16, 32, 64, 128), bias=True, dtype=torch.float32)\n",
       "            (loc): MultiOutSizeLinear(in_features=384, out_features_ls=(8, 16, 32, 64, 128), bias=True, dtype=torch.float32)\n",
       "            (scale): MultiOutSizeLinear(in_features=384, out_features_ls=(8, 16, 32, 64, 128), bias=True, dtype=torch.float32)\n",
       "          )\n",
       "          (1): ModuleDict(\n",
       "            (loc): MultiOutSizeLinear(in_features=384, out_features_ls=(8, 16, 32, 64, 128), bias=True, dtype=torch.float32)\n",
       "          )\n",
       "          (2): ModuleDict(\n",
       "            (total_count): MultiOutSizeLinear(in_features=384, out_features_ls=(8, 16, 32, 64, 128), bias=True, dtype=torch.float32)\n",
       "            (logits): MultiOutSizeLinear(in_features=384, out_features_ls=(8, 16, 32, 64, 128), bias=True, dtype=torch.float32)\n",
       "          )\n",
       "          (3): ModuleDict(\n",
       "            (loc): MultiOutSizeLinear(in_features=384, out_features_ls=(8, 16, 32, 64, 128), bias=True, dtype=torch.float32)\n",
       "            (scale): MultiOutSizeLinear(in_features=384, out_features_ls=(8, 16, 32, 64, 128), bias=True, dtype=torch.float32)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "397dede9-1d9c-4662-9086-16d6ae1b2367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trace...\n",
      " --- modulename: forecast, funcname: create_predictor\n",
      "forecast.py(128):         ts_fields = []\n",
      "forecast.py(129):         if self.hparams.feat_dynamic_real_dim > 0:\n",
      " --- modulename: hparams_mixin, funcname: hparams\n",
      "hparams_mixin.py(161):         if not hasattr(self, \"_hparams\"):\n",
      "hparams_mixin.py(163):         return self._hparams\n",
      " --- modulename: data, funcname: __getattr__\n",
      "data.py(486):         try:\n",
      "data.py(487):             return self[key]\n",
      "forecast.py(132):         past_ts_fields = []\n",
      "forecast.py(133):         if self.hparams.past_feat_dynamic_real_dim > 0:\n",
      " --- modulename: hparams_mixin, funcname: hparams\n",
      "hparams_mixin.py(161):         if not hasattr(self, \"_hparams\"):\n",
      "hparams_mixin.py(163):         return self._hparams\n",
      " --- modulename: data, funcname: __getattr__\n",
      "data.py(486):         try:\n",
      "data.py(487):             return self[key]\n",
      "forecast.py(136):         instance_splitter = TFTInstanceSplitter(\n",
      "forecast.py(137):             instance_sampler=TestSplitSampler(),\n",
      " --- modulename: sampler, funcname: TestSplitSampler\n",
      "sampler.py(121):     return PredictionSplitSampler(\n",
      "sampler.py(122):         allow_empty_interval=False,\n",
      "sampler.py(123):         axis=axis,\n",
      "sampler.py(124):         min_past=min_past,\n",
      "sampler.py(125):         min_future=0,\n",
      "sampler.py(121):     return PredictionSplitSampler(\n",
      " --- modulename: main, funcname: __init__\n",
      "main.py(339):         values, fields_set, validation_error = validate_model(__pydantic_self__.__class__, data)\n",
      " --- modulename: main, funcname: validate_model\n",
      "main.py(1036):     values = {}\n",
      "main.py(1037):     errors = []\n",
      "main.py(1039):     names_used = set()\n",
      "main.py(1041):     fields_set = set()\n",
      "main.py(1042):     config = model.__config__\n",
      "main.py(1043):     check_extra = config.extra is not Extra.ignore\n",
      "main.py(1044):     cls_ = cls or model\n",
      "main.py(1046):     for validator in model.__pre_root_validators__:\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: int_validator\n",
      "validators.py(128):     if isinstance(v, int) and not (v is True or v is False):\n",
      "validators.py(129):         return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: int_validator\n",
      "validators.py(128):     if isinstance(v, int) and not (v is True or v is False):\n",
      "validators.py(129):         return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: int_validator\n",
      "validators.py(128):     if isinstance(v, int) and not (v is True or v is False):\n",
      "validators.py(129):         return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: bool_validator\n",
      "validators.py(107):     if v is True or v is False:\n",
      "validators.py(108):         return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1082):     if check_extra:\n",
      "main.py(1096):     for skip_on_failure, validator in model.__post_root_validators__:\n",
      "main.py(1104):     if errors:\n",
      "main.py(1107):         return values, fields_set, None\n",
      "main.py(340):         if validation_error:\n",
      "main.py(342):         try:\n",
      "main.py(343):             object_setattr(__pydantic_self__, '__dict__', values)\n",
      "main.py(348):         object_setattr(__pydantic_self__, '__fields_set__', fields_set)\n",
      "main.py(349):         __pydantic_self__._init_private_attributes()\n",
      " --- modulename: main, funcname: _init_private_attributes\n",
      "main.py(422):         for name, private_attr in self.__private_attributes__.items():\n",
      "forecast.py(138):             past_length=self.past_length,\n",
      " --- modulename: forecast, funcname: past_length\n",
      "forecast.py(224):             if self.hparams.patch_size == \"auto\"\n",
      " --- modulename: hparams_mixin, funcname: hparams\n",
      "hparams_mixin.py(161):         if not hasattr(self, \"_hparams\"):\n",
      "hparams_mixin.py(163):         return self._hparams\n",
      " --- modulename: data, funcname: __getattr__\n",
      "data.py(486):         try:\n",
      "data.py(487):             return self[key]\n",
      "forecast.py(225):             else self.hparams.context_length\n",
      " --- modulename: hparams_mixin, funcname: hparams\n",
      "hparams_mixin.py(161):         if not hasattr(self, \"_hparams\"):\n",
      "hparams_mixin.py(163):         return self._hparams\n",
      " --- modulename: data, funcname: __getattr__\n",
      "data.py(486):         try:\n",
      "data.py(487):             return self[key]\n",
      "forecast.py(222):         return (\n",
      "forecast.py(139):             future_length=self.hparams.prediction_length,\n",
      " --- modulename: hparams_mixin, funcname: hparams\n",
      "hparams_mixin.py(161):         if not hasattr(self, \"_hparams\"):\n",
      "hparams_mixin.py(163):         return self._hparams\n",
      " --- modulename: data, funcname: __getattr__\n",
      "data.py(486):         try:\n",
      "data.py(487):             return self[key]\n",
      "forecast.py(140):             observed_value_field=\"observed_target\",\n",
      "forecast.py(141):             time_series_fields=ts_fields,\n",
      "forecast.py(142):             past_time_series_fields=past_ts_fields,\n",
      "forecast.py(136):         instance_splitter = TFTInstanceSplitter(\n",
      " --- modulename: component, funcname: init_wrapper\n",
      "component.py(336):             self, *args = args\n",
      "component.py(338):             nmargs = {\n",
      "component.py(340):                 for (name, param), arg in zip(\n",
      "component.py(341):                     list(init_params.items()), [self] + args\n",
      "component.py(340):                 for (name, param), arg in zip(\n",
      "component.py(338):             nmargs = {\n",
      " --- modulename: component, funcname: <dictcomp>\n",
      "component.py(338):             nmargs = {\n",
      "component.py(340):                 for (name, param), arg in zip(\n",
      "component.py(343):                 if name != \"self\"\n",
      "component.py(338):             nmargs = {\n",
      "component.py(343):                 if name != \"self\"\n",
      "component.py(345):             model = PydanticModel(**{**nmargs, **kwargs})\n",
      " --- modulename: main, funcname: __init__\n",
      "main.py(339):         values, fields_set, validation_error = validate_model(__pydantic_self__.__class__, data)\n",
      " --- modulename: main, funcname: validate_model\n",
      "main.py(1036):     values = {}\n",
      "main.py(1037):     errors = []\n",
      "main.py(1039):     names_used = set()\n",
      "main.py(1041):     fields_set = set()\n",
      "main.py(1042):     config = model.__config__\n",
      "main.py(1043):     check_extra = config.extra is not Extra.ignore\n",
      "main.py(1044):     cls_ = cls or model\n",
      "main.py(1046):     for validator in model.__pre_root_validators__:\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: main, funcname: validate\n",
      "main.py(684):         if isinstance(value, cls):\n",
      " --- modulename: main, funcname: __instancecheck__\n",
      "main.py(304):         return hasattr(instance, '__fields__') and super().__instancecheck__(instance)\n",
      " --- modulename: abc, funcname: __instancecheck__\n",
      "abc.py(119):             return _abc_instancecheck(cls, instance)\n",
      "main.py(685):             copy_on_model_validation = cls.__config__.copy_on_model_validation\n",
      "main.py(687):             deep_copy: Optional[bool] = None\n",
      "main.py(688):             if copy_on_model_validation not in {'deep', 'shallow', 'none'}:\n",
      "main.py(696):             if copy_on_model_validation == 'shallow':\n",
      "main.py(698):                 deep_copy = False\n",
      "main.py(703):             if deep_copy is None:\n",
      "main.py(706):                 return value._copy_and_set_values(value.__dict__, value.__fields_set__, deep=deep_copy)\n",
      " --- modulename: main, funcname: _copy_and_set_values\n",
      "main.py(610):         if deep:\n",
      "main.py(614):         cls = self.__class__\n",
      "main.py(615):         m = cls.__new__(cls)\n",
      "main.py(616):         object_setattr(m, '__dict__', values)\n",
      "main.py(617):         object_setattr(m, '__fields_set__', fields_set)\n",
      "main.py(618):         for name in self.__private_attributes__:\n",
      "main.py(625):         return m\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: int_validator\n",
      "validators.py(128):     if isinstance(v, int) and not (v is True or v is False):\n",
      "validators.py(129):         return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: int_validator\n",
      "validators.py(128):     if isinstance(v, int) and not (v is True or v is False):\n",
      "validators.py(129):         return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1060):             if field.required:\n",
      "main.py(1064):             value = field.get_default()\n",
      " --- modulename: fields, funcname: get_default\n",
      "fields.py(437):         return smart_deepcopy(self.default) if self.default_factory is None else self.default_factory()\n",
      " --- modulename: utils, funcname: smart_deepcopy\n",
      "utils.py(682):     obj_type = obj.__class__\n",
      "utils.py(683):     if obj_type in IMMUTABLE_NON_COLLECTIONS_TYPES:\n",
      "utils.py(684):         return obj  # fastest case: obj is immutable and not collection therefore will not be copied anyway\n",
      "main.py(1066):             if not config.validate_all and not field.validate_always:\n",
      "main.py(1067):                 values[name] = value\n",
      "main.py(1068):                 continue\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1060):             if field.required:\n",
      "main.py(1064):             value = field.get_default()\n",
      " --- modulename: fields, funcname: get_default\n",
      "fields.py(437):         return smart_deepcopy(self.default) if self.default_factory is None else self.default_factory()\n",
      " --- modulename: utils, funcname: smart_deepcopy\n",
      "utils.py(682):     obj_type = obj.__class__\n",
      "utils.py(683):     if obj_type in IMMUTABLE_NON_COLLECTIONS_TYPES:\n",
      "utils.py(684):         return obj  # fastest case: obj is immutable and not collection therefore will not be copied anyway\n",
      "main.py(1066):             if not config.validate_all and not field.validate_always:\n",
      "main.py(1067):                 values[name] = value\n",
      "main.py(1068):                 continue\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1060):             if field.required:\n",
      "main.py(1064):             value = field.get_default()\n",
      " --- modulename: fields, funcname: get_default\n",
      "fields.py(437):         return smart_deepcopy(self.default) if self.default_factory is None else self.default_factory()\n",
      " --- modulename: utils, funcname: smart_deepcopy\n",
      "utils.py(682):     obj_type = obj.__class__\n",
      "utils.py(683):     if obj_type in IMMUTABLE_NON_COLLECTIONS_TYPES:\n",
      "utils.py(684):         return obj  # fastest case: obj is immutable and not collection therefore will not be copied anyway\n",
      "main.py(1066):             if not config.validate_all and not field.validate_always:\n",
      "main.py(1067):                 values[name] = value\n",
      "main.py(1068):                 continue\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1060):             if field.required:\n",
      "main.py(1064):             value = field.get_default()\n",
      " --- modulename: fields, funcname: get_default\n",
      "fields.py(437):         return smart_deepcopy(self.default) if self.default_factory is None else self.default_factory()\n",
      " --- modulename: utils, funcname: smart_deepcopy\n",
      "utils.py(682):     obj_type = obj.__class__\n",
      "utils.py(683):     if obj_type in IMMUTABLE_NON_COLLECTIONS_TYPES:\n",
      "utils.py(684):         return obj  # fastest case: obj is immutable and not collection therefore will not be copied anyway\n",
      "main.py(1066):             if not config.validate_all and not field.validate_always:\n",
      "main.py(1067):                 values[name] = value\n",
      "main.py(1068):                 continue\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: str_validator\n",
      "validators.py(60):     if isinstance(v, str):\n",
      "validators.py(61):         if isinstance(v, Enum):\n",
      "validators.py(64):             return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1060):             if field.required:\n",
      "main.py(1064):             value = field.get_default()\n",
      " --- modulename: fields, funcname: get_default\n",
      "fields.py(437):         return smart_deepcopy(self.default) if self.default_factory is None else self.default_factory()\n",
      " --- modulename: utils, funcname: smart_deepcopy\n",
      "utils.py(682):     obj_type = obj.__class__\n",
      "utils.py(683):     if obj_type in IMMUTABLE_NON_COLLECTIONS_TYPES:\n",
      "utils.py(684):         return obj  # fastest case: obj is immutable and not collection therefore will not be copied anyway\n",
      "main.py(1066):             if not config.validate_all and not field.validate_always:\n",
      "main.py(1067):                 values[name] = value\n",
      "main.py(1068):                 continue\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1060):             if field.required:\n",
      "main.py(1064):             value = field.get_default()\n",
      " --- modulename: fields, funcname: get_default\n",
      "fields.py(437):         return smart_deepcopy(self.default) if self.default_factory is None else self.default_factory()\n",
      " --- modulename: utils, funcname: smart_deepcopy\n",
      "utils.py(682):     obj_type = obj.__class__\n",
      "utils.py(683):     if obj_type in IMMUTABLE_NON_COLLECTIONS_TYPES:\n",
      "utils.py(684):         return obj  # fastest case: obj is immutable and not collection therefore will not be copied anyway\n",
      "main.py(1066):             if not config.validate_all and not field.validate_always:\n",
      "main.py(1067):                 values[name] = value\n",
      "main.py(1068):                 continue\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(882):         elif self.shape in MAPPING_LIKE_SHAPES:\n",
      "fields.py(884):         elif self.shape == SHAPE_TUPLE:\n",
      "fields.py(886):         elif self.shape == SHAPE_ITERABLE:\n",
      "fields.py(888):         elif self.shape == SHAPE_GENERIC:\n",
      "fields.py(892):             v, errors = self._validate_sequence_like(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_sequence_like\n",
      "fields.py(906):         if not sequence_like(v):\n",
      " --- modulename: utils, funcname: sequence_like\n",
      "utils.py(158):     return isinstance(v, (list, tuple, set, frozenset, GeneratorType, deque))\n",
      "fields.py(920):         loc = loc if isinstance(loc, tuple) else (loc,)\n",
      "fields.py(921):         result = []\n",
      "fields.py(922):         errors: List[ErrorList] = []\n",
      "fields.py(923):         for i, v_ in enumerate(v):\n",
      "fields.py(931):         if errors:\n",
      "fields.py(934):         converted: Union[List[Any], Set[Any], FrozenSet[Any], Tuple[Any, ...], Iterator[Any], Deque[Any]] = result\n",
      "fields.py(936):         if self.shape == SHAPE_SET:\n",
      "fields.py(938):         elif self.shape == SHAPE_FROZENSET:\n",
      "fields.py(940):         elif self.shape == SHAPE_TUPLE_ELLIPSIS:\n",
      "fields.py(942):         elif self.shape == SHAPE_DEQUE:\n",
      "fields.py(944):         elif self.shape == SHAPE_SEQUENCE:\n",
      "fields.py(953):         return converted, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(882):         elif self.shape in MAPPING_LIKE_SHAPES:\n",
      "fields.py(884):         elif self.shape == SHAPE_TUPLE:\n",
      "fields.py(886):         elif self.shape == SHAPE_ITERABLE:\n",
      "fields.py(888):         elif self.shape == SHAPE_GENERIC:\n",
      "fields.py(892):             v, errors = self._validate_sequence_like(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_sequence_like\n",
      "fields.py(906):         if not sequence_like(v):\n",
      " --- modulename: utils, funcname: sequence_like\n",
      "utils.py(158):     return isinstance(v, (list, tuple, set, frozenset, GeneratorType, deque))\n",
      "fields.py(920):         loc = loc if isinstance(loc, tuple) else (loc,)\n",
      "fields.py(921):         result = []\n",
      "fields.py(922):         errors: List[ErrorList] = []\n",
      "fields.py(923):         for i, v_ in enumerate(v):\n",
      "fields.py(931):         if errors:\n",
      "fields.py(934):         converted: Union[List[Any], Set[Any], FrozenSet[Any], Tuple[Any, ...], Iterator[Any], Deque[Any]] = result\n",
      "fields.py(936):         if self.shape == SHAPE_SET:\n",
      "fields.py(938):         elif self.shape == SHAPE_FROZENSET:\n",
      "fields.py(940):         elif self.shape == SHAPE_TUPLE_ELLIPSIS:\n",
      "fields.py(942):         elif self.shape == SHAPE_DEQUE:\n",
      "fields.py(944):         elif self.shape == SHAPE_SEQUENCE:\n",
      "fields.py(953):         return converted, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1060):             if field.required:\n",
      "main.py(1064):             value = field.get_default()\n",
      " --- modulename: fields, funcname: get_default\n",
      "fields.py(437):         return smart_deepcopy(self.default) if self.default_factory is None else self.default_factory()\n",
      " --- modulename: utils, funcname: smart_deepcopy\n",
      "utils.py(682):     obj_type = obj.__class__\n",
      "utils.py(683):     if obj_type in IMMUTABLE_NON_COLLECTIONS_TYPES:\n",
      "utils.py(684):         return obj  # fastest case: obj is immutable and not collection therefore will not be copied anyway\n",
      "main.py(1066):             if not config.validate_all and not field.validate_always:\n",
      "main.py(1067):                 values[name] = value\n",
      "main.py(1068):                 continue\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1082):     if check_extra:\n",
      "main.py(1096):     for skip_on_failure, validator in model.__post_root_validators__:\n",
      "main.py(1104):     if errors:\n",
      "main.py(1107):         return values, fields_set, None\n",
      "main.py(340):         if validation_error:\n",
      "main.py(342):         try:\n",
      "main.py(343):             object_setattr(__pydantic_self__, '__dict__', values)\n",
      "main.py(348):         object_setattr(__pydantic_self__, '__fields_set__', fields_set)\n",
      "main.py(349):         __pydantic_self__._init_private_attributes()\n",
      " --- modulename: main, funcname: _init_private_attributes\n",
      "main.py(422):         for name, private_attr in self.__private_attributes__.items():\n",
      "component.py(348):             all_args = {**nmargs, **kwargs, **model.__dict__}\n",
      "component.py(353):             if not getattr(self, \"__init_args__\", {}):\n",
      "component.py(354):                 self.__init_args__ = OrderedDict(\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(355):                     {\n",
      " --- modulename: component, funcname: <dictcomp>\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(354):                 self.__init_args__ = OrderedDict(\n",
      "component.py(361):                 self.__class__.__getnewargs_ex__ = validated_getnewargs_ex\n",
      "component.py(362):                 self.__class__.__repr__ = validated_repr\n",
      "component.py(364):             return init(self, **all_args)\n",
      " --- modulename: split, funcname: __init__\n",
      "split.py(503):         super().__init__(\n",
      "split.py(504):             target_field=target_field,\n",
      "split.py(505):             is_pad_field=is_pad_field,\n",
      "split.py(506):             start_field=start_field,\n",
      "split.py(507):             forecast_start_field=forecast_start_field,\n",
      "split.py(508):             instance_sampler=instance_sampler,\n",
      "split.py(509):             past_length=past_length,\n",
      "split.py(510):             future_length=future_length,\n",
      "split.py(511):             lead_time=lead_time,\n",
      "split.py(512):             output_NTC=output_NTC,\n",
      "split.py(513):             time_series_fields=time_series_fields,\n",
      "split.py(514):             dummy_value=dummy_value,\n",
      "split.py(503):         super().__init__(\n",
      " --- modulename: component, funcname: init_wrapper\n",
      "component.py(336):             self, *args = args\n",
      "component.py(338):             nmargs = {\n",
      "component.py(340):                 for (name, param), arg in zip(\n",
      "component.py(341):                     list(init_params.items()), [self] + args\n",
      "component.py(340):                 for (name, param), arg in zip(\n",
      "component.py(338):             nmargs = {\n",
      " --- modulename: component, funcname: <dictcomp>\n",
      "component.py(338):             nmargs = {\n",
      "component.py(340):                 for (name, param), arg in zip(\n",
      "component.py(343):                 if name != \"self\"\n",
      "component.py(338):             nmargs = {\n",
      "component.py(343):                 if name != \"self\"\n",
      "component.py(345):             model = PydanticModel(**{**nmargs, **kwargs})\n",
      " --- modulename: main, funcname: __init__\n",
      "main.py(339):         values, fields_set, validation_error = validate_model(__pydantic_self__.__class__, data)\n",
      " --- modulename: main, funcname: validate_model\n",
      "main.py(1036):     values = {}\n",
      "main.py(1037):     errors = []\n",
      "main.py(1039):     names_used = set()\n",
      "main.py(1041):     fields_set = set()\n",
      "main.py(1042):     config = model.__config__\n",
      "main.py(1043):     check_extra = config.extra is not Extra.ignore\n",
      "main.py(1044):     cls_ = cls or model\n",
      "main.py(1046):     for validator in model.__pre_root_validators__:\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: str_validator\n",
      "validators.py(60):     if isinstance(v, str):\n",
      "validators.py(61):         if isinstance(v, Enum):\n",
      "validators.py(64):             return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: str_validator\n",
      "validators.py(60):     if isinstance(v, str):\n",
      "validators.py(61):         if isinstance(v, Enum):\n",
      "validators.py(64):             return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: str_validator\n",
      "validators.py(60):     if isinstance(v, str):\n",
      "validators.py(61):         if isinstance(v, Enum):\n",
      "validators.py(64):             return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: str_validator\n",
      "validators.py(60):     if isinstance(v, str):\n",
      "validators.py(61):         if isinstance(v, Enum):\n",
      "validators.py(64):             return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: main, funcname: validate\n",
      "main.py(684):         if isinstance(value, cls):\n",
      " --- modulename: main, funcname: __instancecheck__\n",
      "main.py(304):         return hasattr(instance, '__fields__') and super().__instancecheck__(instance)\n",
      " --- modulename: abc, funcname: __instancecheck__\n",
      "abc.py(119):             return _abc_instancecheck(cls, instance)\n",
      "main.py(685):             copy_on_model_validation = cls.__config__.copy_on_model_validation\n",
      "main.py(687):             deep_copy: Optional[bool] = None\n",
      "main.py(688):             if copy_on_model_validation not in {'deep', 'shallow', 'none'}:\n",
      "main.py(696):             if copy_on_model_validation == 'shallow':\n",
      "main.py(698):                 deep_copy = False\n",
      "main.py(703):             if deep_copy is None:\n",
      "main.py(706):                 return value._copy_and_set_values(value.__dict__, value.__fields_set__, deep=deep_copy)\n",
      " --- modulename: main, funcname: _copy_and_set_values\n",
      "main.py(610):         if deep:\n",
      "main.py(614):         cls = self.__class__\n",
      "main.py(615):         m = cls.__new__(cls)\n",
      "main.py(616):         object_setattr(m, '__dict__', values)\n",
      "main.py(617):         object_setattr(m, '__fields_set__', fields_set)\n",
      "main.py(618):         for name in self.__private_attributes__:\n",
      "main.py(625):         return m\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: int_validator\n",
      "validators.py(128):     if isinstance(v, int) and not (v is True or v is False):\n",
      "validators.py(129):         return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: int_validator\n",
      "validators.py(128):     if isinstance(v, int) and not (v is True or v is False):\n",
      "validators.py(129):         return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: int_validator\n",
      "validators.py(128):     if isinstance(v, int) and not (v is True or v is False):\n",
      "validators.py(129):         return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: bool_validator\n",
      "validators.py(107):     if v is True or v is False:\n",
      "validators.py(108):         return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(882):         elif self.shape in MAPPING_LIKE_SHAPES:\n",
      "fields.py(884):         elif self.shape == SHAPE_TUPLE:\n",
      "fields.py(886):         elif self.shape == SHAPE_ITERABLE:\n",
      "fields.py(888):         elif self.shape == SHAPE_GENERIC:\n",
      "fields.py(892):             v, errors = self._validate_sequence_like(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_sequence_like\n",
      "fields.py(906):         if not sequence_like(v):\n",
      " --- modulename: utils, funcname: sequence_like\n",
      "utils.py(158):     return isinstance(v, (list, tuple, set, frozenset, GeneratorType, deque))\n",
      "fields.py(920):         loc = loc if isinstance(loc, tuple) else (loc,)\n",
      "fields.py(921):         result = []\n",
      "fields.py(922):         errors: List[ErrorList] = []\n",
      "fields.py(923):         for i, v_ in enumerate(v):\n",
      "fields.py(931):         if errors:\n",
      "fields.py(934):         converted: Union[List[Any], Set[Any], FrozenSet[Any], Tuple[Any, ...], Iterator[Any], Deque[Any]] = result\n",
      "fields.py(936):         if self.shape == SHAPE_SET:\n",
      "fields.py(938):         elif self.shape == SHAPE_FROZENSET:\n",
      "fields.py(940):         elif self.shape == SHAPE_TUPLE_ELLIPSIS:\n",
      "fields.py(942):         elif self.shape == SHAPE_DEQUE:\n",
      "fields.py(944):         elif self.shape == SHAPE_SEQUENCE:\n",
      "fields.py(953):         return converted, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: float_validator\n",
      "validators.py(153):     if isinstance(v, float):\n",
      "validators.py(154):         return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1082):     if check_extra:\n",
      "main.py(1096):     for skip_on_failure, validator in model.__post_root_validators__:\n",
      "main.py(1104):     if errors:\n",
      "main.py(1107):         return values, fields_set, None\n",
      "main.py(340):         if validation_error:\n",
      "main.py(342):         try:\n",
      "main.py(343):             object_setattr(__pydantic_self__, '__dict__', values)\n",
      "main.py(348):         object_setattr(__pydantic_self__, '__fields_set__', fields_set)\n",
      "main.py(349):         __pydantic_self__._init_private_attributes()\n",
      " --- modulename: main, funcname: _init_private_attributes\n",
      "main.py(422):         for name, private_attr in self.__private_attributes__.items():\n",
      "component.py(348):             all_args = {**nmargs, **kwargs, **model.__dict__}\n",
      "component.py(353):             if not getattr(self, \"__init_args__\", {}):\n",
      "component.py(364):             return init(self, **all_args)\n",
      " --- modulename: split, funcname: __init__\n",
      "split.py(86):         super().__init__()\n",
      " --- modulename: component, funcname: init_wrapper\n",
      "component.py(336):             self, *args = args\n",
      "component.py(338):             nmargs = {\n",
      "component.py(340):                 for (name, param), arg in zip(\n",
      "component.py(341):                     list(init_params.items()), [self] + args\n",
      "component.py(340):                 for (name, param), arg in zip(\n",
      "component.py(338):             nmargs = {\n",
      " --- modulename: component, funcname: <dictcomp>\n",
      "component.py(338):             nmargs = {\n",
      "component.py(340):                 for (name, param), arg in zip(\n",
      "component.py(343):                 if name != \"self\"\n",
      "component.py(338):             nmargs = {\n",
      "component.py(343):                 if name != \"self\"\n",
      "component.py(345):             model = PydanticModel(**{**nmargs, **kwargs})\n",
      " --- modulename: main, funcname: __init__\n",
      "main.py(339):         values, fields_set, validation_error = validate_model(__pydantic_self__.__class__, data)\n",
      " --- modulename: main, funcname: validate_model\n",
      "main.py(1036):     values = {}\n",
      "main.py(1037):     errors = []\n",
      "main.py(1039):     names_used = set()\n",
      "main.py(1041):     fields_set = set()\n",
      "main.py(1042):     config = model.__config__\n",
      "main.py(1043):     check_extra = config.extra is not Extra.ignore\n",
      "main.py(1044):     cls_ = cls or model\n",
      "main.py(1046):     for validator in model.__pre_root_validators__:\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1082):     if check_extra:\n",
      "main.py(1096):     for skip_on_failure, validator in model.__post_root_validators__:\n",
      "main.py(1104):     if errors:\n",
      "main.py(1107):         return values, fields_set, None\n",
      "main.py(340):         if validation_error:\n",
      "main.py(342):         try:\n",
      "main.py(343):             object_setattr(__pydantic_self__, '__dict__', values)\n",
      "main.py(348):         object_setattr(__pydantic_self__, '__fields_set__', fields_set)\n",
      "main.py(349):         __pydantic_self__._init_private_attributes()\n",
      " --- modulename: main, funcname: _init_private_attributes\n",
      "main.py(422):         for name, private_attr in self.__private_attributes__.items():\n",
      "component.py(348):             all_args = {**nmargs, **kwargs, **model.__dict__}\n",
      "component.py(353):             if not getattr(self, \"__init_args__\", {}):\n",
      "component.py(364):             return init(self, **all_args)\n",
      " --- modulename: _base, funcname: __init__\n",
      "_base.py(180):         self.max_idle_transforms = env.max_idle_transforms\n",
      " --- modulename: settings, funcname: __getattribute__\n",
      "settings.py(286):         if key.startswith(\"_\"):\n",
      "settings.py(289):             return self[key]\n",
      " --- modulename: settings, funcname: __getitem__\n",
      "settings.py(273):         if key in self._dependencies:\n",
      " --- modulename: settings, funcname: __getattribute__\n",
      "settings.py(286):         if key.startswith(\"_\"):\n",
      "settings.py(287):             return super().__getattribute__(key)\n",
      "settings.py(276):         for dct in self._chain.reverse():\n",
      " --- modulename: settings, funcname: __getattribute__\n",
      "settings.py(286):         if key.startswith(\"_\"):\n",
      "settings.py(287):             return super().__getattribute__(key)\n",
      " --- modulename: settings, funcname: reverse\n",
      "settings.py(137):         current = self.end\n",
      "settings.py(139):         while current is not None:\n",
      "settings.py(140):             yield current.val\n",
      "settings.py(277):             try:\n",
      "settings.py(278):                 return dct[key]\n",
      "settings.py(279):             except KeyError:\n",
      "settings.py(280):                 pass\n",
      "settings.py(276):         for dct in self._chain.reverse():\n",
      " --- modulename: settings, funcname: reverse\n",
      "settings.py(141):             current = current.prv\n",
      "settings.py(139):         while current is not None:\n",
      "settings.py(140):             yield current.val\n",
      "settings.py(277):             try:\n",
      "settings.py(278):                 return dct[key]\n",
      " --- modulename: settings, funcname: reverse\n",
      "split.py(88):         assert future_length > 0, \"The value of `future_length` should be > 0\"\n",
      "split.py(90):         self.instance_sampler = instance_sampler\n",
      "split.py(91):         self.past_length = past_length\n",
      "split.py(92):         self.future_length = future_length\n",
      "split.py(93):         self.lead_time = lead_time\n",
      "split.py(94):         self.output_NTC = output_NTC\n",
      "split.py(95):         self.ts_fields = time_series_fields\n",
      "split.py(96):         self.target_field = target_field\n",
      "split.py(97):         self.is_pad_field = is_pad_field\n",
      "split.py(98):         self.start_field = start_field\n",
      "split.py(99):         self.forecast_start_field = forecast_start_field\n",
      "split.py(100):         self.dummy_value = dummy_value\n",
      "split.py(517):         assert past_length > 0, \"The value of `past_length` should be > 0\"\n",
      "split.py(519):         self.observed_value_field = observed_value_field\n",
      "split.py(520):         self.past_ts_fields = past_time_series_fields\n",
      "forecast.py(144):         return PyTorchPredictor(\n",
      "forecast.py(145):             input_names=self.prediction_input_names,\n",
      " --- modulename: forecast, funcname: prediction_input_names\n",
      "forecast.py(214):         return list(self.describe_inputs())\n",
      " --- modulename: forecast, funcname: describe_inputs\n",
      "forecast.py(155):             \"past_target\": Input(\n",
      "forecast.py(157):                     batch_size,\n",
      "forecast.py(158):                     self.past_length,\n",
      " --- modulename: forecast, funcname: past_length\n",
      "forecast.py(224):             if self.hparams.patch_size == \"auto\"\n",
      " --- modulename: hparams_mixin, funcname: hparams\n",
      "hparams_mixin.py(161):         if not hasattr(self, \"_hparams\"):\n",
      "hparams_mixin.py(163):         return self._hparams\n",
      " --- modulename: data, funcname: __getattr__\n",
      "data.py(486):         try:\n",
      "data.py(487):             return self[key]\n",
      "forecast.py(225):             else self.hparams.context_length\n",
      " --- modulename: hparams_mixin, funcname: hparams\n",
      "hparams_mixin.py(161):         if not hasattr(self, \"_hparams\"):\n",
      "hparams_mixin.py(163):         return self._hparams\n",
      " --- modulename: data, funcname: __getattr__\n",
      "data.py(486):         try:\n",
      "data.py(487):             return self[key]\n",
      "forecast.py(222):         return (\n",
      "forecast.py(159):                     self.hparams.target_dim,\n",
      " --- modulename: hparams_mixin, funcname: hparams\n",
      "hparams_mixin.py(161):         if not hasattr(self, \"_hparams\"):\n",
      "hparams_mixin.py(163):         return self._hparams\n",
      " --- modulename: data, funcname: __getattr__\n",
      "data.py(486):         try:\n",
      "data.py(487):             return self[key]\n",
      "forecast.py(156):                 shape=(\n",
      "forecast.py(161):                 dtype=torch.float,\n",
      "forecast.py(155):             \"past_target\": Input(\n",
      " --- modulename: inputs, funcname: __init__\n",
      "<string>(3): <string>(4): <string>(5): forecast.py(163):             \"past_observed_target\": Input(\n",
      "forecast.py(165):                     batch_size,\n",
      "forecast.py(166):                     self.past_length,\n",
      " --- modulename: forecast, funcname: past_length\n",
      "forecast.py(224):             if self.hparams.patch_size == \"auto\"\n",
      " --- modulename: hparams_mixin, funcname: hparams\n",
      "hparams_mixin.py(161):         if not hasattr(self, \"_hparams\"):\n",
      "hparams_mixin.py(163):         return self._hparams\n",
      " --- modulename: data, funcname: __getattr__\n",
      "data.py(486):         try:\n",
      "data.py(487):             return self[key]\n",
      "forecast.py(225):             else self.hparams.context_length\n",
      " --- modulename: hparams_mixin, funcname: hparams\n",
      "hparams_mixin.py(161):         if not hasattr(self, \"_hparams\"):\n",
      "hparams_mixin.py(163):         return self._hparams\n",
      " --- modulename: data, funcname: __getattr__\n",
      "data.py(486):         try:\n",
      "data.py(487):             return self[key]\n",
      "forecast.py(222):         return (\n",
      "forecast.py(167):                     self.hparams.target_dim,\n",
      " --- modulename: hparams_mixin, funcname: hparams\n",
      "hparams_mixin.py(161):         if not hasattr(self, \"_hparams\"):\n",
      "hparams_mixin.py(163):         return self._hparams\n",
      " --- modulename: data, funcname: __getattr__\n",
      "data.py(486):         try:\n",
      "data.py(487):             return self[key]\n",
      "forecast.py(164):                 shape=(\n",
      "forecast.py(169):                 dtype=torch.bool,\n",
      "forecast.py(163):             \"past_observed_target\": Input(\n",
      " --- modulename: inputs, funcname: __init__\n",
      "<string>(3): <string>(4): <string>(5): forecast.py(171):             \"past_is_pad\": Input(\n",
      "forecast.py(172):                 shape=(batch_size, self.past_length),\n",
      " --- modulename: forecast, funcname: past_length\n",
      "forecast.py(224):             if self.hparams.patch_size == \"auto\"\n",
      " --- modulename: hparams_mixin, funcname: hparams\n",
      "hparams_mixin.py(161):         if not hasattr(self, \"_hparams\"):\n",
      "hparams_mixin.py(163):         return self._hparams\n",
      " --- modulename: data, funcname: __getattr__\n",
      "data.py(486):         try:\n",
      "data.py(487):             return self[key]\n",
      "forecast.py(225):             else self.hparams.context_length\n",
      " --- modulename: hparams_mixin, funcname: hparams\n",
      "hparams_mixin.py(161):         if not hasattr(self, \"_hparams\"):\n",
      "hparams_mixin.py(163):         return self._hparams\n",
      " --- modulename: data, funcname: __getattr__\n",
      "data.py(486):         try:\n",
      "data.py(487):             return self[key]\n",
      "forecast.py(222):         return (\n",
      "forecast.py(173):                 dtype=torch.bool,\n",
      "forecast.py(171):             \"past_is_pad\": Input(\n",
      " --- modulename: inputs, funcname: __init__\n",
      "<string>(3): <string>(4): <string>(5): forecast.py(154):         data = {\n",
      "forecast.py(176):         if self.hparams.feat_dynamic_real_dim > 0:\n",
      " --- modulename: hparams_mixin, funcname: hparams\n",
      "hparams_mixin.py(161):         if not hasattr(self, \"_hparams\"):\n",
      "hparams_mixin.py(163):         return self._hparams\n",
      " --- modulename: data, funcname: __getattr__\n",
      "data.py(486):         try:\n",
      "data.py(487):             return self[key]\n",
      "forecast.py(193):         if self.hparams.past_feat_dynamic_real_dim > 0:\n",
      " --- modulename: hparams_mixin, funcname: hparams\n",
      "hparams_mixin.py(161):         if not hasattr(self, \"_hparams\"):\n",
      "hparams_mixin.py(163):         return self._hparams\n",
      " --- modulename: data, funcname: __getattr__\n",
      "data.py(486):         try:\n",
      "data.py(487):             return self[key]\n",
      "forecast.py(210):         return InputSpec(data=data, zeros_fn=torch.zeros)\n",
      " --- modulename: inputs, funcname: __init__\n",
      "<string>(3): <string>(4):  --- modulename: __init__, funcname: __iter__\n",
      "__init__.py(1115):         return iter(self.data)\n",
      " --- modulename: __init__, funcname: __len__\n",
      "__init__.py(1099):         return len(self.data)\n",
      "forecast.py(146):             prediction_net=self,\n",
      "forecast.py(147):             batch_size=batch_size,\n",
      "forecast.py(148):             prediction_length=self.hparams.prediction_length,\n",
      " --- modulename: hparams_mixin, funcname: hparams\n",
      "hparams_mixin.py(161):         if not hasattr(self, \"_hparams\"):\n",
      "hparams_mixin.py(163):         return self._hparams\n",
      " --- modulename: data, funcname: __getattr__\n",
      "data.py(486):         try:\n",
      "data.py(487):             return self[key]\n",
      "forecast.py(149):             input_transform=self.get_default_transform() + instance_splitter,\n",
      " --- modulename: forecast, funcname: get_default_transform\n",
      "forecast.py(1019):         transform = AsNumpyArray(\n",
      "forecast.py(1020):             field=\"target\",\n",
      "forecast.py(1021):             expected_ndim=1 if self.hparams.target_dim == 1 else 2,\n",
      " --- modulename: hparams_mixin, funcname: hparams\n",
      "hparams_mixin.py(161):         if not hasattr(self, \"_hparams\"):\n",
      "hparams_mixin.py(163):         return self._hparams\n",
      " --- modulename: data, funcname: __getattr__\n",
      "data.py(486):         try:\n",
      "data.py(487):             return self[key]\n",
      "forecast.py(1022):             dtype=np.float32,\n",
      "forecast.py(1019):         transform = AsNumpyArray(\n",
      " --- modulename: component, funcname: init_wrapper\n",
      "component.py(336):             self, *args = args\n",
      "component.py(338):             nmargs = {\n",
      "component.py(340):                 for (name, param), arg in zip(\n",
      "component.py(341):                     list(init_params.items()), [self] + args\n",
      "component.py(340):                 for (name, param), arg in zip(\n",
      "component.py(338):             nmargs = {\n",
      " --- modulename: component, funcname: <dictcomp>\n",
      "component.py(338):             nmargs = {\n",
      "component.py(340):                 for (name, param), arg in zip(\n",
      "component.py(343):                 if name != \"self\"\n",
      "component.py(338):             nmargs = {\n",
      "component.py(343):                 if name != \"self\"\n",
      "component.py(345):             model = PydanticModel(**{**nmargs, **kwargs})\n",
      " --- modulename: main, funcname: __init__\n",
      "main.py(339):         values, fields_set, validation_error = validate_model(__pydantic_self__.__class__, data)\n",
      " --- modulename: main, funcname: validate_model\n",
      "main.py(1036):     values = {}\n",
      "main.py(1037):     errors = []\n",
      "main.py(1039):     names_used = set()\n",
      "main.py(1041):     fields_set = set()\n",
      "main.py(1042):     config = model.__config__\n",
      "main.py(1043):     check_extra = config.extra is not Extra.ignore\n",
      "main.py(1044):     cls_ = cls or model\n",
      "main.py(1046):     for validator in model.__pre_root_validators__:\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: str_validator\n",
      "validators.py(60):     if isinstance(v, str):\n",
      "validators.py(61):         if isinstance(v, Enum):\n",
      "validators.py(64):             return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: int_validator\n",
      "validators.py(128):     if isinstance(v, int) and not (v is True or v is False):\n",
      "validators.py(129):         return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: any_class_validator\n",
      "validators.py(568):     if isinstance(v, type):\n",
      "validators.py(569):         return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1082):     if check_extra:\n",
      "main.py(1096):     for skip_on_failure, validator in model.__post_root_validators__:\n",
      "main.py(1104):     if errors:\n",
      "main.py(1107):         return values, fields_set, None\n",
      "main.py(340):         if validation_error:\n",
      "main.py(342):         try:\n",
      "main.py(343):             object_setattr(__pydantic_self__, '__dict__', values)\n",
      "main.py(348):         object_setattr(__pydantic_self__, '__fields_set__', fields_set)\n",
      "main.py(349):         __pydantic_self__._init_private_attributes()\n",
      " --- modulename: main, funcname: _init_private_attributes\n",
      "main.py(422):         for name, private_attr in self.__private_attributes__.items():\n",
      "component.py(348):             all_args = {**nmargs, **kwargs, **model.__dict__}\n",
      "component.py(353):             if not getattr(self, \"__init_args__\", {}):\n",
      "component.py(354):                 self.__init_args__ = OrderedDict(\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(355):                     {\n",
      " --- modulename: component, funcname: <dictcomp>\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(354):                 self.__init_args__ = OrderedDict(\n",
      "component.py(361):                 self.__class__.__getnewargs_ex__ = validated_getnewargs_ex\n",
      "component.py(362):                 self.__class__.__repr__ = validated_repr\n",
      "component.py(364):             return init(self, **all_args)\n",
      " --- modulename: convert, funcname: __init__\n",
      "convert.py(132):         self.field = field\n",
      "convert.py(133):         self.expected_ndim = expected_ndim\n",
      "convert.py(134):         self.dtype = dtype\n",
      "forecast.py(1024):         if self.hparams.target_dim == 1:\n",
      " --- modulename: hparams_mixin, funcname: hparams\n",
      "hparams_mixin.py(161):         if not hasattr(self, \"_hparams\"):\n",
      "hparams_mixin.py(163):         return self._hparams\n",
      " --- modulename: data, funcname: __getattr__\n",
      "data.py(486):         try:\n",
      "data.py(487):             return self[key]\n",
      "forecast.py(1026):         transform += AddObservedValuesIndicator(\n",
      "forecast.py(1027):             target_field=\"target\",\n",
      "forecast.py(1028):             output_field=\"observed_target\",\n",
      "forecast.py(1029):             dtype=bool,\n",
      "forecast.py(1026):         transform += AddObservedValuesIndicator(\n",
      " --- modulename: component, funcname: init_wrapper\n",
      "component.py(336):             self, *args = args\n",
      "component.py(338):             nmargs = {\n",
      "component.py(340):                 for (name, param), arg in zip(\n",
      "component.py(341):                     list(init_params.items()), [self] + args\n",
      "component.py(340):                 for (name, param), arg in zip(\n",
      "component.py(338):             nmargs = {\n",
      " --- modulename: component, funcname: <dictcomp>\n",
      "component.py(338):             nmargs = {\n",
      "component.py(340):                 for (name, param), arg in zip(\n",
      "component.py(343):                 if name != \"self\"\n",
      "component.py(338):             nmargs = {\n",
      "component.py(343):                 if name != \"self\"\n",
      "component.py(345):             model = PydanticModel(**{**nmargs, **kwargs})\n",
      " --- modulename: main, funcname: __init__\n",
      "main.py(339):         values, fields_set, validation_error = validate_model(__pydantic_self__.__class__, data)\n",
      " --- modulename: main, funcname: validate_model\n",
      "main.py(1036):     values = {}\n",
      "main.py(1037):     errors = []\n",
      "main.py(1039):     names_used = set()\n",
      "main.py(1041):     fields_set = set()\n",
      "main.py(1042):     config = model.__config__\n",
      "main.py(1043):     check_extra = config.extra is not Extra.ignore\n",
      "main.py(1044):     cls_ = cls or model\n",
      "main.py(1046):     for validator in model.__pre_root_validators__:\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: str_validator\n",
      "validators.py(60):     if isinstance(v, str):\n",
      "validators.py(61):         if isinstance(v, Enum):\n",
      "validators.py(64):             return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: str_validator\n",
      "validators.py(60):     if isinstance(v, str):\n",
      "validators.py(61):         if isinstance(v, Enum):\n",
      "validators.py(64):             return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1060):             if field.required:\n",
      "main.py(1064):             value = field.get_default()\n",
      " --- modulename: fields, funcname: get_default\n",
      "fields.py(437):         return smart_deepcopy(self.default) if self.default_factory is None else self.default_factory()\n",
      " --- modulename: utils, funcname: smart_deepcopy\n",
      "utils.py(682):     obj_type = obj.__class__\n",
      "utils.py(683):     if obj_type in IMMUTABLE_NON_COLLECTIONS_TYPES:\n",
      "utils.py(685):     try:\n",
      "utils.py(686):         if not obj and obj_type in BUILTIN_COLLECTIONS:\n",
      "utils.py(693):     return deepcopy(obj)  # slowest way when we actually might need a deepcopy\n",
      " --- modulename: copy, funcname: deepcopy\n",
      "copy.py(134):     if memo is None:\n",
      "copy.py(135):         memo = {}\n",
      "copy.py(137):     d = id(x)\n",
      "copy.py(138):     y = memo.get(d, _nil)\n",
      "copy.py(139):     if y is not _nil:\n",
      "copy.py(142):     cls = type(x)\n",
      "copy.py(144):     copier = _deepcopy_dispatch.get(cls)\n",
      "copy.py(145):     if copier is not None:\n",
      "copy.py(148):         if issubclass(cls, type):\n",
      "copy.py(151):             copier = getattr(x, \"__deepcopy__\", None)\n",
      "copy.py(152):             if copier is not None:\n",
      "copy.py(155):                 reductor = dispatch_table.get(cls)\n",
      "copy.py(156):                 if reductor:\n",
      "copy.py(159):                     reductor = getattr(x, \"__reduce_ex__\", None)\n",
      "copy.py(160):                     if reductor is not None:\n",
      "copy.py(161):                         rv = reductor(4)\n",
      " --- modulename: component, funcname: validated_getnewargs_ex\n",
      "component.py(332):             return (), self.__init_args__\n",
      "copy.py(169):                 if isinstance(rv, str):\n",
      "copy.py(172):                     y = _reconstruct(x, memo, *rv)\n",
      " --- modulename: copy, funcname: _reconstruct\n",
      "copy.py(262):     deep = memo is not None\n",
      "copy.py(263):     if deep and args:\n",
      "copy.py(264):         args = (deepcopy(arg, memo) for arg in args)\n",
      "copy.py(265):     y = func(*args)\n",
      " --- modulename: copy, funcname: <genexpr>\n",
      "copy.py(264):         args = (deepcopy(arg, memo) for arg in args)\n",
      " --- modulename: copy, funcname: deepcopy\n",
      "copy.py(134):     if memo is None:\n",
      "copy.py(137):     d = id(x)\n",
      "copy.py(138):     y = memo.get(d, _nil)\n",
      "copy.py(139):     if y is not _nil:\n",
      "copy.py(142):     cls = type(x)\n",
      "copy.py(144):     copier = _deepcopy_dispatch.get(cls)\n",
      "copy.py(145):     if copier is not None:\n",
      "copy.py(146):         y = copier(x, memo)\n",
      " --- modulename: copy, funcname: _deepcopy_atomic\n",
      "copy.py(183):     return x\n",
      "copy.py(175):     if y is not x:\n",
      "copy.py(178):     return y\n",
      " --- modulename: copy, funcname: <genexpr>\n",
      "copy.py(264):         args = (deepcopy(arg, memo) for arg in args)\n",
      " --- modulename: copy, funcname: deepcopy\n",
      "copy.py(134):     if memo is None:\n",
      "copy.py(137):     d = id(x)\n",
      "copy.py(138):     y = memo.get(d, _nil)\n",
      "copy.py(139):     if y is not _nil:\n",
      "copy.py(142):     cls = type(x)\n",
      "copy.py(144):     copier = _deepcopy_dispatch.get(cls)\n",
      "copy.py(145):     if copier is not None:\n",
      "copy.py(146):         y = copier(x, memo)\n",
      " --- modulename: copy, funcname: _deepcopy_tuple\n",
      "copy.py(211):     y = [deepcopy(a, memo) for a in x]\n",
      " --- modulename: copy, funcname: <listcomp>\n",
      "copy.py(211):     y = [deepcopy(a, memo) for a in x]\n",
      "copy.py(214):     try:\n",
      "copy.py(215):         return memo[id(x)]\n",
      "copy.py(216):     except KeyError:\n",
      "copy.py(217):         pass\n",
      "copy.py(218):     for k, j in zip(x, y):\n",
      "copy.py(223):         y = x\n",
      "copy.py(224):     return y\n",
      "copy.py(175):     if y is not x:\n",
      "copy.py(178):     return y\n",
      " --- modulename: copy, funcname: <genexpr>\n",
      "copy.py(264):         args = (deepcopy(arg, memo) for arg in args)\n",
      " --- modulename: copy, funcname: deepcopy\n",
      "copy.py(134):     if memo is None:\n",
      "copy.py(137):     d = id(x)\n",
      "copy.py(138):     y = memo.get(d, _nil)\n",
      "copy.py(139):     if y is not _nil:\n",
      "copy.py(142):     cls = type(x)\n",
      "copy.py(144):     copier = _deepcopy_dispatch.get(cls)\n",
      "copy.py(145):     if copier is not None:\n",
      "copy.py(148):         if issubclass(cls, type):\n",
      "copy.py(151):             copier = getattr(x, \"__deepcopy__\", None)\n",
      "copy.py(152):             if copier is not None:\n",
      "copy.py(155):                 reductor = dispatch_table.get(cls)\n",
      "copy.py(156):                 if reductor:\n",
      "copy.py(159):                     reductor = getattr(x, \"__reduce_ex__\", None)\n",
      "copy.py(160):                     if reductor is not None:\n",
      "copy.py(161):                         rv = reductor(4)\n",
      "copy.py(169):                 if isinstance(rv, str):\n",
      "copy.py(172):                     y = _reconstruct(x, memo, *rv)\n",
      " --- modulename: copy, funcname: _reconstruct\n",
      "copy.py(262):     deep = memo is not None\n",
      "copy.py(263):     if deep and args:\n",
      "copy.py(265):     y = func(*args)\n",
      "copy.py(266):     if deep:\n",
      "copy.py(267):         memo[id(x)] = y\n",
      "copy.py(269):     if state is not None:\n",
      "copy.py(285):     if listiter is not None:\n",
      "copy.py(293):     if dictiter is not None:\n",
      "copy.py(294):         if deep:\n",
      "copy.py(295):             for key, value in dictiter:\n",
      "copy.py(296):                 key = deepcopy(key, memo)\n",
      " --- modulename: copy, funcname: deepcopy\n",
      "copy.py(134):     if memo is None:\n",
      "copy.py(137):     d = id(x)\n",
      "copy.py(138):     y = memo.get(d, _nil)\n",
      "copy.py(139):     if y is not _nil:\n",
      "copy.py(142):     cls = type(x)\n",
      "copy.py(144):     copier = _deepcopy_dispatch.get(cls)\n",
      "copy.py(145):     if copier is not None:\n",
      "copy.py(146):         y = copier(x, memo)\n",
      " --- modulename: copy, funcname: _deepcopy_atomic\n",
      "copy.py(183):     return x\n",
      "copy.py(175):     if y is not x:\n",
      "copy.py(178):     return y\n",
      "copy.py(297):                 value = deepcopy(value, memo)\n",
      " --- modulename: copy, funcname: deepcopy\n",
      "copy.py(134):     if memo is None:\n",
      "copy.py(137):     d = id(x)\n",
      "copy.py(138):     y = memo.get(d, _nil)\n",
      "copy.py(139):     if y is not _nil:\n",
      "copy.py(142):     cls = type(x)\n",
      "copy.py(144):     copier = _deepcopy_dispatch.get(cls)\n",
      "copy.py(145):     if copier is not None:\n",
      "copy.py(146):         y = copier(x, memo)\n",
      " --- modulename: copy, funcname: _deepcopy_atomic\n",
      "copy.py(183):     return x\n",
      "copy.py(175):     if y is not x:\n",
      "copy.py(178):     return y\n",
      "copy.py(298):                 y[key] = value\n",
      "copy.py(295):             for key, value in dictiter:\n",
      "copy.py(302):     return y\n",
      "copy.py(175):     if y is not x:\n",
      "copy.py(176):         memo[d] = y\n",
      "copy.py(177):         _keep_alive(x, memo) # Make sure x lives at least as long as d\n",
      " --- modulename: copy, funcname: _keep_alive\n",
      "copy.py(253):     try:\n",
      "copy.py(254):         memo[id(memo)].append(x)\n",
      "copy.py(255):     except KeyError:\n",
      "copy.py(257):         memo[id(memo)]=[x]\n",
      "copy.py(178):     return y\n",
      " --- modulename: copy, funcname: <genexpr>\n",
      "copy.py(264):         args = (deepcopy(arg, memo) for arg in args)\n",
      " --- modulename: copyreg, funcname: __newobj_ex__\n",
      "copyreg.py(107):     return cls.__new__(cls, *args, **kwargs)\n",
      "copy.py(266):     if deep:\n",
      "copy.py(267):         memo[id(x)] = y\n",
      "copy.py(269):     if state is not None:\n",
      "copy.py(270):         if deep:\n",
      "copy.py(271):             state = deepcopy(state, memo)\n",
      " --- modulename: copy, funcname: deepcopy\n",
      "copy.py(134):     if memo is None:\n",
      "copy.py(137):     d = id(x)\n",
      "copy.py(138):     y = memo.get(d, _nil)\n",
      "copy.py(139):     if y is not _nil:\n",
      "copy.py(142):     cls = type(x)\n",
      "copy.py(144):     copier = _deepcopy_dispatch.get(cls)\n",
      "copy.py(145):     if copier is not None:\n",
      "copy.py(146):         y = copier(x, memo)\n",
      " --- modulename: copy, funcname: _deepcopy_dict\n",
      "copy.py(228):     y = {}\n",
      "copy.py(229):     memo[id(x)] = y\n",
      "copy.py(230):     for key, value in x.items():\n",
      "copy.py(231):         y[deepcopy(key, memo)] = deepcopy(value, memo)\n",
      " --- modulename: copy, funcname: deepcopy\n",
      "copy.py(134):     if memo is None:\n",
      "copy.py(137):     d = id(x)\n",
      "copy.py(138):     y = memo.get(d, _nil)\n",
      "copy.py(139):     if y is not _nil:\n",
      "copy.py(140):         return y\n",
      " --- modulename: copy, funcname: deepcopy\n",
      "copy.py(134):     if memo is None:\n",
      "copy.py(137):     d = id(x)\n",
      "copy.py(138):     y = memo.get(d, _nil)\n",
      "copy.py(139):     if y is not _nil:\n",
      "copy.py(142):     cls = type(x)\n",
      "copy.py(144):     copier = _deepcopy_dispatch.get(cls)\n",
      "copy.py(145):     if copier is not None:\n",
      "copy.py(146):         y = copier(x, memo)\n",
      " --- modulename: copy, funcname: _deepcopy_atomic\n",
      "copy.py(183):     return x\n",
      "copy.py(175):     if y is not x:\n",
      "copy.py(178):     return y\n",
      "copy.py(230):     for key, value in x.items():\n",
      "copy.py(231):         y[deepcopy(key, memo)] = deepcopy(value, memo)\n",
      " --- modulename: copy, funcname: deepcopy\n",
      "copy.py(134):     if memo is None:\n",
      "copy.py(137):     d = id(x)\n",
      "copy.py(138):     y = memo.get(d, _nil)\n",
      "copy.py(139):     if y is not _nil:\n",
      "copy.py(142):     cls = type(x)\n",
      "copy.py(144):     copier = _deepcopy_dispatch.get(cls)\n",
      "copy.py(145):     if copier is not None:\n",
      "copy.py(146):         y = copier(x, memo)\n",
      " --- modulename: copy, funcname: _deepcopy_atomic\n",
      "copy.py(183):     return x\n",
      "copy.py(175):     if y is not x:\n",
      "copy.py(178):     return y\n",
      " --- modulename: copy, funcname: deepcopy\n",
      "copy.py(134):     if memo is None:\n",
      "copy.py(137):     d = id(x)\n",
      "copy.py(138):     y = memo.get(d, _nil)\n",
      "copy.py(139):     if y is not _nil:\n",
      "copy.py(142):     cls = type(x)\n",
      "copy.py(144):     copier = _deepcopy_dispatch.get(cls)\n",
      "copy.py(145):     if copier is not None:\n",
      "copy.py(146):         y = copier(x, memo)\n",
      " --- modulename: copy, funcname: _deepcopy_atomic\n",
      "copy.py(183):     return x\n",
      "copy.py(175):     if y is not x:\n",
      "copy.py(178):     return y\n",
      "copy.py(230):     for key, value in x.items():\n",
      "copy.py(232):     return y\n",
      "copy.py(175):     if y is not x:\n",
      "copy.py(176):         memo[d] = y\n",
      "copy.py(177):         _keep_alive(x, memo) # Make sure x lives at least as long as d\n",
      " --- modulename: copy, funcname: _keep_alive\n",
      "copy.py(253):     try:\n",
      "copy.py(254):         memo[id(memo)].append(x)\n",
      "copy.py(178):     return y\n",
      "copy.py(272):         if hasattr(y, '__setstate__'):\n",
      "copy.py(275):             if isinstance(state, tuple) and len(state) == 2:\n",
      "copy.py(278):                 slotstate = None\n",
      "copy.py(279):             if state is not None:\n",
      "copy.py(280):                 y.__dict__.update(state)\n",
      "copy.py(281):             if slotstate is not None:\n",
      "copy.py(285):     if listiter is not None:\n",
      "copy.py(293):     if dictiter is not None:\n",
      "copy.py(302):     return y\n",
      "copy.py(175):     if y is not x:\n",
      "copy.py(176):         memo[d] = y\n",
      "copy.py(177):         _keep_alive(x, memo) # Make sure x lives at least as long as d\n",
      " --- modulename: copy, funcname: _keep_alive\n",
      "copy.py(253):     try:\n",
      "copy.py(254):         memo[id(memo)].append(x)\n",
      "copy.py(178):     return y\n",
      "main.py(1066):             if not config.validate_all and not field.validate_always:\n",
      "main.py(1067):                 values[name] = value\n",
      "main.py(1068):                 continue\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: any_class_validator\n",
      "validators.py(568):     if isinstance(v, type):\n",
      "validators.py(569):         return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1082):     if check_extra:\n",
      "main.py(1096):     for skip_on_failure, validator in model.__post_root_validators__:\n",
      "main.py(1104):     if errors:\n",
      "main.py(1107):         return values, fields_set, None\n",
      "main.py(340):         if validation_error:\n",
      "main.py(342):         try:\n",
      "main.py(343):             object_setattr(__pydantic_self__, '__dict__', values)\n",
      "main.py(348):         object_setattr(__pydantic_self__, '__fields_set__', fields_set)\n",
      "main.py(349):         __pydantic_self__._init_private_attributes()\n",
      " --- modulename: main, funcname: _init_private_attributes\n",
      "main.py(422):         for name, private_attr in self.__private_attributes__.items():\n",
      "component.py(348):             all_args = {**nmargs, **kwargs, **model.__dict__}\n",
      "component.py(353):             if not getattr(self, \"__init_args__\", {}):\n",
      "component.py(354):                 self.__init_args__ = OrderedDict(\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(355):                     {\n",
      " --- modulename: component, funcname: <dictcomp>\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(354):                 self.__init_args__ = OrderedDict(\n",
      "component.py(361):                 self.__class__.__getnewargs_ex__ = validated_getnewargs_ex\n",
      "component.py(362):                 self.__class__.__repr__ = validated_repr\n",
      "component.py(364):             return init(self, **all_args)\n",
      " --- modulename: feature, funcname: __init__\n",
      "feature.py(244):         self.target_field = target_field\n",
      "feature.py(245):         self.output_field = output_field\n",
      "feature.py(246):         self.dtype = dtype\n",
      "feature.py(247):         self.imputation_method = imputation_method\n",
      " --- modulename: _base, funcname: __add__\n",
      "_base.py(40):         return self.chain(other)\n",
      " --- modulename: _base, funcname: chain\n",
      "_base.py(37):         return Chain([self, other])\n",
      " --- modulename: _base, funcname: __init__\n",
      "<string>(3): <string>(4):  --- modulename: _base, funcname: __post_init__\n",
      "_base.py(57):         transformations = []\n",
      "_base.py(59):         for transformation in self.transformations:\n",
      "_base.py(60):             if isinstance(transformation, Identity):\n",
      " --- modulename: abc, funcname: __instancecheck__\n",
      "abc.py(119):             return _abc_instancecheck(cls, instance)\n",
      "_base.py(62):             elif isinstance(transformation, Chain):\n",
      " --- modulename: abc, funcname: __instancecheck__\n",
      "abc.py(119):             return _abc_instancecheck(cls, instance)\n",
      "_base.py(65):                 assert isinstance(transformation, Transformation)\n",
      " --- modulename: abc, funcname: __instancecheck__\n",
      "abc.py(119):             return _abc_instancecheck(cls, instance)\n",
      "_base.py(66):                 transformations.append(transformation)\n",
      "_base.py(59):         for transformation in self.transformations:\n",
      "_base.py(60):             if isinstance(transformation, Identity):\n",
      " --- modulename: abc, funcname: __instancecheck__\n",
      "abc.py(119):             return _abc_instancecheck(cls, instance)\n",
      "_base.py(62):             elif isinstance(transformation, Chain):\n",
      " --- modulename: abc, funcname: __instancecheck__\n",
      "abc.py(119):             return _abc_instancecheck(cls, instance)\n",
      "_base.py(65):                 assert isinstance(transformation, Transformation)\n",
      " --- modulename: abc, funcname: __instancecheck__\n",
      "abc.py(119):             return _abc_instancecheck(cls, instance)\n",
      "_base.py(66):                 transformations.append(transformation)\n",
      "_base.py(59):         for transformation in self.transformations:\n",
      "_base.py(68):         self.transformations = transformations\n",
      "_base.py(69):         self.__init_passed_kwargs__ = {\"transformations\": transformations}\n",
      "forecast.py(1032):         if self.hparams.feat_dynamic_real_dim > 0:\n",
      " --- modulename: hparams_mixin, funcname: hparams\n",
      "hparams_mixin.py(161):         if not hasattr(self, \"_hparams\"):\n",
      "hparams_mixin.py(163):         return self._hparams\n",
      " --- modulename: data, funcname: __getattr__\n",
      "data.py(486):         try:\n",
      "data.py(487):             return self[key]\n",
      "forecast.py(1044):         if self.hparams.past_feat_dynamic_real_dim > 0:\n",
      " --- modulename: hparams_mixin, funcname: hparams\n",
      "hparams_mixin.py(161):         if not hasattr(self, \"_hparams\"):\n",
      "hparams_mixin.py(163):         return self._hparams\n",
      " --- modulename: data, funcname: __getattr__\n",
      "data.py(486):         try:\n",
      "data.py(487):             return self[key]\n",
      "forecast.py(1055):         return transform\n",
      " --- modulename: _base, funcname: __add__\n",
      "_base.py(40):         return self.chain(other)\n",
      " --- modulename: _base, funcname: chain\n",
      "_base.py(37):         return Chain([self, other])\n",
      " --- modulename: _base, funcname: __init__\n",
      "<string>(3): <string>(4):  --- modulename: _base, funcname: __post_init__\n",
      "_base.py(57):         transformations = []\n",
      "_base.py(59):         for transformation in self.transformations:\n",
      "_base.py(60):             if isinstance(transformation, Identity):\n",
      " --- modulename: abc, funcname: __instancecheck__\n",
      "abc.py(119):             return _abc_instancecheck(cls, instance)\n",
      "_base.py(62):             elif isinstance(transformation, Chain):\n",
      "_base.py(63):                 transformations.extend(transformation.transformations)\n",
      "_base.py(59):         for transformation in self.transformations:\n",
      "_base.py(60):             if isinstance(transformation, Identity):\n",
      " --- modulename: abc, funcname: __instancecheck__\n",
      "abc.py(119):             return _abc_instancecheck(cls, instance)\n",
      "_base.py(62):             elif isinstance(transformation, Chain):\n",
      " --- modulename: abc, funcname: __instancecheck__\n",
      "abc.py(119):             return _abc_instancecheck(cls, instance)\n",
      "_base.py(65):                 assert isinstance(transformation, Transformation)\n",
      " --- modulename: abc, funcname: __instancecheck__\n",
      "abc.py(119):             return _abc_instancecheck(cls, instance)\n",
      "_base.py(66):                 transformations.append(transformation)\n",
      "_base.py(59):         for transformation in self.transformations:\n",
      "_base.py(68):         self.transformations = transformations\n",
      "_base.py(69):         self.__init_passed_kwargs__ = {\"transformations\": transformations}\n",
      "forecast.py(150):             device=device,\n",
      "forecast.py(144):         return PyTorchPredictor(\n",
      " --- modulename: component, funcname: init_wrapper\n",
      "component.py(336):             self, *args = args\n",
      "component.py(338):             nmargs = {\n",
      "component.py(340):                 for (name, param), arg in zip(\n",
      "component.py(341):                     list(init_params.items()), [self] + args\n",
      "component.py(340):                 for (name, param), arg in zip(\n",
      "component.py(338):             nmargs = {\n",
      " --- modulename: component, funcname: <dictcomp>\n",
      "component.py(338):             nmargs = {\n",
      "component.py(340):                 for (name, param), arg in zip(\n",
      "component.py(343):                 if name != \"self\"\n",
      "component.py(338):             nmargs = {\n",
      "component.py(343):                 if name != \"self\"\n",
      "component.py(345):             model = PydanticModel(**{**nmargs, **kwargs})\n",
      " --- modulename: main, funcname: __init__\n",
      "main.py(339):         values, fields_set, validation_error = validate_model(__pydantic_self__.__class__, data)\n",
      " --- modulename: main, funcname: validate_model\n",
      "main.py(1036):     values = {}\n",
      "main.py(1037):     errors = []\n",
      "main.py(1039):     names_used = set()\n",
      "main.py(1041):     fields_set = set()\n",
      "main.py(1042):     config = model.__config__\n",
      "main.py(1043):     check_extra = config.extra is not Extra.ignore\n",
      "main.py(1044):     cls_ = cls or model\n",
      "main.py(1046):     for validator in model.__pre_root_validators__:\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(882):         elif self.shape in MAPPING_LIKE_SHAPES:\n",
      "fields.py(884):         elif self.shape == SHAPE_TUPLE:\n",
      "fields.py(886):         elif self.shape == SHAPE_ITERABLE:\n",
      "fields.py(888):         elif self.shape == SHAPE_GENERIC:\n",
      "fields.py(892):             v, errors = self._validate_sequence_like(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_sequence_like\n",
      "fields.py(906):         if not sequence_like(v):\n",
      " --- modulename: utils, funcname: sequence_like\n",
      "utils.py(158):     return isinstance(v, (list, tuple, set, frozenset, GeneratorType, deque))\n",
      "fields.py(920):         loc = loc if isinstance(loc, tuple) else (loc,)\n",
      "fields.py(921):         result = []\n",
      "fields.py(922):         errors: List[ErrorList] = []\n",
      "fields.py(923):         for i, v_ in enumerate(v):\n",
      "fields.py(924):             v_loc = *loc, i\n",
      "fields.py(925):             r, ee = self._validate_singleton(v_, values, v_loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1057):             if self.discriminator_key is not None:\n",
      "fields.py(1060):             errors = []\n",
      "fields.py(1062):             if self.model_config.smart_union and is_union(get_origin(self.type_)):\n",
      "fields.py(1090):             for field in self.sub_fields:\n",
      "fields.py(1091):                 value, error = field.validate(v, values, loc=loc, cls=cls)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: str_validator\n",
      "validators.py(60):     if isinstance(v, str):\n",
      "validators.py(61):         if isinstance(v, Enum):\n",
      "validators.py(64):             return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "fields.py(1092):                 if error:\n",
      "fields.py(1095):                     return value, None\n",
      "fields.py(926):             if ee:\n",
      "fields.py(929):                 result.append(r)\n",
      "fields.py(923):         for i, v_ in enumerate(v):\n",
      "fields.py(924):             v_loc = *loc, i\n",
      "fields.py(925):             r, ee = self._validate_singleton(v_, values, v_loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1057):             if self.discriminator_key is not None:\n",
      "fields.py(1060):             errors = []\n",
      "fields.py(1062):             if self.model_config.smart_union and is_union(get_origin(self.type_)):\n",
      "fields.py(1090):             for field in self.sub_fields:\n",
      "fields.py(1091):                 value, error = field.validate(v, values, loc=loc, cls=cls)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: str_validator\n",
      "validators.py(60):     if isinstance(v, str):\n",
      "validators.py(61):         if isinstance(v, Enum):\n",
      "validators.py(64):             return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "fields.py(1092):                 if error:\n",
      "fields.py(1095):                     return value, None\n",
      "fields.py(926):             if ee:\n",
      "fields.py(929):                 result.append(r)\n",
      "fields.py(923):         for i, v_ in enumerate(v):\n",
      "fields.py(924):             v_loc = *loc, i\n",
      "fields.py(925):             r, ee = self._validate_singleton(v_, values, v_loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1057):             if self.discriminator_key is not None:\n",
      "fields.py(1060):             errors = []\n",
      "fields.py(1062):             if self.model_config.smart_union and is_union(get_origin(self.type_)):\n",
      "fields.py(1090):             for field in self.sub_fields:\n",
      "fields.py(1091):                 value, error = field.validate(v, values, loc=loc, cls=cls)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: str_validator\n",
      "validators.py(60):     if isinstance(v, str):\n",
      "validators.py(61):         if isinstance(v, Enum):\n",
      "validators.py(64):             return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "fields.py(1092):                 if error:\n",
      "fields.py(1095):                     return value, None\n",
      "fields.py(926):             if ee:\n",
      "fields.py(929):                 result.append(r)\n",
      "fields.py(923):         for i, v_ in enumerate(v):\n",
      "fields.py(931):         if errors:\n",
      "fields.py(934):         converted: Union[List[Any], Set[Any], FrozenSet[Any], Tuple[Any, ...], Iterator[Any], Deque[Any]] = result\n",
      "fields.py(936):         if self.shape == SHAPE_SET:\n",
      "fields.py(938):         elif self.shape == SHAPE_FROZENSET:\n",
      "fields.py(940):         elif self.shape == SHAPE_TUPLE_ELLIPSIS:\n",
      "fields.py(942):         elif self.shape == SHAPE_DEQUE:\n",
      "fields.py(944):         elif self.shape == SHAPE_SEQUENCE:\n",
      "fields.py(953):         return converted, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: arbitrary_type_validator\n",
      "validators.py(551):         if isinstance(v, type_):\n",
      "validators.py(552):             return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: int_validator\n",
      "validators.py(128):     if isinstance(v, int) and not (v is True or v is False):\n",
      "validators.py(129):         return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: int_validator\n",
      "validators.py(128):     if isinstance(v, int) and not (v is True or v is False):\n",
      "validators.py(129):         return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: arbitrary_type_validator\n",
      "validators.py(551):         if isinstance(v, type_):\n",
      " --- modulename: abc, funcname: __instancecheck__\n",
      "abc.py(119):             return _abc_instancecheck(cls, instance)\n",
      "validators.py(552):             return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1060):             if field.required:\n",
      "main.py(1064):             value = field.get_default()\n",
      " --- modulename: fields, funcname: get_default\n",
      "fields.py(437):         return smart_deepcopy(self.default) if self.default_factory is None else self.default_factory()\n",
      " --- modulename: utils, funcname: smart_deepcopy\n",
      "utils.py(682):     obj_type = obj.__class__\n",
      "utils.py(683):     if obj_type in IMMUTABLE_NON_COLLECTIONS_TYPES:\n",
      "utils.py(685):     try:\n",
      "utils.py(686):         if not obj and obj_type in BUILTIN_COLLECTIONS:\n",
      "utils.py(693):     return deepcopy(obj)  # slowest way when we actually might need a deepcopy\n",
      " --- modulename: copy, funcname: deepcopy\n",
      "copy.py(134):     if memo is None:\n",
      "copy.py(135):         memo = {}\n",
      "copy.py(137):     d = id(x)\n",
      "copy.py(138):     y = memo.get(d, _nil)\n",
      "copy.py(139):     if y is not _nil:\n",
      "copy.py(142):     cls = type(x)\n",
      "copy.py(144):     copier = _deepcopy_dispatch.get(cls)\n",
      "copy.py(145):     if copier is not None:\n",
      "copy.py(148):         if issubclass(cls, type):\n",
      "copy.py(151):             copier = getattr(x, \"__deepcopy__\", None)\n",
      "copy.py(152):             if copier is not None:\n",
      "copy.py(155):                 reductor = dispatch_table.get(cls)\n",
      "copy.py(156):                 if reductor:\n",
      "copy.py(159):                     reductor = getattr(x, \"__reduce_ex__\", None)\n",
      "copy.py(160):                     if reductor is not None:\n",
      "copy.py(161):                         rv = reductor(4)\n",
      " --- modulename: component, funcname: validated_getnewargs_ex\n",
      "component.py(332):             return (), self.__init_args__\n",
      "copy.py(169):                 if isinstance(rv, str):\n",
      "copy.py(172):                     y = _reconstruct(x, memo, *rv)\n",
      " --- modulename: copy, funcname: _reconstruct\n",
      "copy.py(262):     deep = memo is not None\n",
      "copy.py(263):     if deep and args:\n",
      "copy.py(264):         args = (deepcopy(arg, memo) for arg in args)\n",
      "copy.py(265):     y = func(*args)\n",
      " --- modulename: copy, funcname: <genexpr>\n",
      "copy.py(264):         args = (deepcopy(arg, memo) for arg in args)\n",
      " --- modulename: copy, funcname: deepcopy\n",
      "copy.py(134):     if memo is None:\n",
      "copy.py(137):     d = id(x)\n",
      "copy.py(138):     y = memo.get(d, _nil)\n",
      "copy.py(139):     if y is not _nil:\n",
      "copy.py(142):     cls = type(x)\n",
      "copy.py(144):     copier = _deepcopy_dispatch.get(cls)\n",
      "copy.py(145):     if copier is not None:\n",
      "copy.py(146):         y = copier(x, memo)\n",
      " --- modulename: copy, funcname: _deepcopy_atomic\n",
      "copy.py(183):     return x\n",
      "copy.py(175):     if y is not x:\n",
      "copy.py(178):     return y\n",
      " --- modulename: copy, funcname: <genexpr>\n",
      "copy.py(264):         args = (deepcopy(arg, memo) for arg in args)\n",
      " --- modulename: copyreg, funcname: __newobj__\n",
      "copyreg.py(101):     return cls.__new__(cls, *args)\n",
      "copy.py(266):     if deep:\n",
      "copy.py(267):         memo[id(x)] = y\n",
      "copy.py(269):     if state is not None:\n",
      "copy.py(270):         if deep:\n",
      "copy.py(271):             state = deepcopy(state, memo)\n",
      " --- modulename: copy, funcname: deepcopy\n",
      "copy.py(134):     if memo is None:\n",
      "copy.py(137):     d = id(x)\n",
      "copy.py(138):     y = memo.get(d, _nil)\n",
      "copy.py(139):     if y is not _nil:\n",
      "copy.py(142):     cls = type(x)\n",
      "copy.py(144):     copier = _deepcopy_dispatch.get(cls)\n",
      "copy.py(145):     if copier is not None:\n",
      "copy.py(146):         y = copier(x, memo)\n",
      " --- modulename: copy, funcname: _deepcopy_dict\n",
      "copy.py(228):     y = {}\n",
      "copy.py(229):     memo[id(x)] = y\n",
      "copy.py(230):     for key, value in x.items():\n",
      "copy.py(231):         y[deepcopy(key, memo)] = deepcopy(value, memo)\n",
      " --- modulename: copy, funcname: deepcopy\n",
      "copy.py(134):     if memo is None:\n",
      "copy.py(137):     d = id(x)\n",
      "copy.py(138):     y = memo.get(d, _nil)\n",
      "copy.py(139):     if y is not _nil:\n",
      "copy.py(142):     cls = type(x)\n",
      "copy.py(144):     copier = _deepcopy_dispatch.get(cls)\n",
      "copy.py(145):     if copier is not None:\n",
      "copy.py(148):         if issubclass(cls, type):\n",
      "copy.py(151):             copier = getattr(x, \"__deepcopy__\", None)\n",
      "copy.py(152):             if copier is not None:\n",
      "copy.py(155):                 reductor = dispatch_table.get(cls)\n",
      "copy.py(156):                 if reductor:\n",
      "copy.py(159):                     reductor = getattr(x, \"__reduce_ex__\", None)\n",
      "copy.py(160):                     if reductor is not None:\n",
      "copy.py(161):                         rv = reductor(4)\n",
      "copy.py(169):                 if isinstance(rv, str):\n",
      "copy.py(172):                     y = _reconstruct(x, memo, *rv)\n",
      " --- modulename: copy, funcname: _reconstruct\n",
      "copy.py(262):     deep = memo is not None\n",
      "copy.py(263):     if deep and args:\n",
      "copy.py(265):     y = func(*args)\n",
      "copy.py(266):     if deep:\n",
      "copy.py(267):         memo[id(x)] = y\n",
      "copy.py(269):     if state is not None:\n",
      "copy.py(285):     if listiter is not None:\n",
      "copy.py(293):     if dictiter is not None:\n",
      "copy.py(294):         if deep:\n",
      "copy.py(295):             for key, value in dictiter:\n",
      "copy.py(302):     return y\n",
      "copy.py(175):     if y is not x:\n",
      "copy.py(176):         memo[d] = y\n",
      "copy.py(177):         _keep_alive(x, memo) # Make sure x lives at least as long as d\n",
      " --- modulename: copy, funcname: _keep_alive\n",
      "copy.py(253):     try:\n",
      "copy.py(254):         memo[id(memo)].append(x)\n",
      "copy.py(255):     except KeyError:\n",
      "copy.py(257):         memo[id(memo)]=[x]\n",
      "copy.py(178):     return y\n",
      " --- modulename: copy, funcname: deepcopy\n",
      "copy.py(134):     if memo is None:\n",
      "copy.py(137):     d = id(x)\n",
      "copy.py(138):     y = memo.get(d, _nil)\n",
      "copy.py(139):     if y is not _nil:\n",
      "copy.py(142):     cls = type(x)\n",
      "copy.py(144):     copier = _deepcopy_dispatch.get(cls)\n",
      "copy.py(145):     if copier is not None:\n",
      "copy.py(146):         y = copier(x, memo)\n",
      " --- modulename: copy, funcname: _deepcopy_atomic\n",
      "copy.py(183):     return x\n",
      "copy.py(175):     if y is not x:\n",
      "copy.py(178):     return y\n",
      "copy.py(230):     for key, value in x.items():\n",
      "copy.py(232):     return y\n",
      "copy.py(175):     if y is not x:\n",
      "copy.py(176):         memo[d] = y\n",
      "copy.py(177):         _keep_alive(x, memo) # Make sure x lives at least as long as d\n",
      " --- modulename: copy, funcname: _keep_alive\n",
      "copy.py(253):     try:\n",
      "copy.py(254):         memo[id(memo)].append(x)\n",
      "copy.py(178):     return y\n",
      "copy.py(272):         if hasattr(y, '__setstate__'):\n",
      "copy.py(275):             if isinstance(state, tuple) and len(state) == 2:\n",
      "copy.py(278):                 slotstate = None\n",
      "copy.py(279):             if state is not None:\n",
      "copy.py(280):                 y.__dict__.update(state)\n",
      "copy.py(281):             if slotstate is not None:\n",
      "copy.py(285):     if listiter is not None:\n",
      "copy.py(293):     if dictiter is not None:\n",
      "copy.py(302):     return y\n",
      "copy.py(175):     if y is not x:\n",
      "copy.py(176):         memo[d] = y\n",
      "copy.py(177):         _keep_alive(x, memo) # Make sure x lives at least as long as d\n",
      " --- modulename: copy, funcname: _keep_alive\n",
      "copy.py(253):     try:\n",
      "copy.py(254):         memo[id(memo)].append(x)\n",
      "copy.py(178):     return y\n",
      "main.py(1066):             if not config.validate_all and not field.validate_always:\n",
      "main.py(1067):                 values[name] = value\n",
      "main.py(1068):                 continue\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1060):             if field.required:\n",
      "main.py(1064):             value = field.get_default()\n",
      " --- modulename: fields, funcname: get_default\n",
      "fields.py(437):         return smart_deepcopy(self.default) if self.default_factory is None else self.default_factory()\n",
      " --- modulename: utils, funcname: smart_deepcopy\n",
      "utils.py(682):     obj_type = obj.__class__\n",
      "utils.py(683):     if obj_type in IMMUTABLE_NON_COLLECTIONS_TYPES:\n",
      "utils.py(684):         return obj  # fastest case: obj is immutable and not collection therefore will not be copied anyway\n",
      "main.py(1066):             if not config.validate_all and not field.validate_always:\n",
      "main.py(1067):                 values[name] = value\n",
      "main.py(1068):                 continue\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1060):             if field.required:\n",
      "main.py(1064):             value = field.get_default()\n",
      " --- modulename: fields, funcname: get_default\n",
      "fields.py(437):         return smart_deepcopy(self.default) if self.default_factory is None else self.default_factory()\n",
      " --- modulename: utils, funcname: smart_deepcopy\n",
      "utils.py(682):     obj_type = obj.__class__\n",
      "utils.py(683):     if obj_type in IMMUTABLE_NON_COLLECTIONS_TYPES:\n",
      "utils.py(684):         return obj  # fastest case: obj is immutable and not collection therefore will not be copied anyway\n",
      "main.py(1066):             if not config.validate_all and not field.validate_always:\n",
      "main.py(1067):                 values[name] = value\n",
      "main.py(1068):                 continue\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1057):             if self.discriminator_key is not None:\n",
      "fields.py(1060):             errors = []\n",
      "fields.py(1062):             if self.model_config.smart_union and is_union(get_origin(self.type_)):\n",
      "fields.py(1090):             for field in self.sub_fields:\n",
      "fields.py(1091):                 value, error = field.validate(v, values, loc=loc, cls=cls)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: str_validator\n",
      "validators.py(60):     if isinstance(v, str):\n",
      "validators.py(61):         if isinstance(v, Enum):\n",
      "validators.py(64):             return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "fields.py(1092):                 if error:\n",
      "fields.py(1095):                     return value, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1082):     if check_extra:\n",
      "main.py(1096):     for skip_on_failure, validator in model.__post_root_validators__:\n",
      "main.py(1104):     if errors:\n",
      "main.py(1107):         return values, fields_set, None\n",
      "main.py(340):         if validation_error:\n",
      "main.py(342):         try:\n",
      "main.py(343):             object_setattr(__pydantic_self__, '__dict__', values)\n",
      "main.py(348):         object_setattr(__pydantic_self__, '__fields_set__', fields_set)\n",
      "main.py(349):         __pydantic_self__._init_private_attributes()\n",
      " --- modulename: main, funcname: _init_private_attributes\n",
      "main.py(422):         for name, private_attr in self.__private_attributes__.items():\n",
      "component.py(348):             all_args = {**nmargs, **kwargs, **model.__dict__}\n",
      "component.py(353):             if not getattr(self, \"__init_args__\", {}):\n",
      "component.py(354):                 self.__init_args__ = OrderedDict(\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(355):                     {\n",
      " --- modulename: component, funcname: <dictcomp>\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(354):                 self.__init_args__ = OrderedDict(\n",
      "component.py(361):                 self.__class__.__getnewargs_ex__ = validated_getnewargs_ex\n",
      "component.py(362):                 self.__class__.__repr__ = validated_repr\n",
      "component.py(364):             return init(self, **all_args)\n",
      " --- modulename: predictor, funcname: __init__\n",
      "predictor.py(55):         super().__init__(prediction_length, lead_time=lead_time)\n",
      " --- modulename: predictor, funcname: __init__\n",
      "predictor.py(54):             prediction_length > 0\n",
      "predictor.py(56):         assert lead_time >= 0, \"The value of `lead_time` should be >= 0\"\n",
      "predictor.py(58):         self.prediction_length = prediction_length\n",
      "predictor.py(59):         self.lead_time = lead_time\n",
      "predictor.py(56):         self.input_names = input_names\n",
      "predictor.py(57):         self.batch_size = batch_size\n",
      "predictor.py(58):         self.input_transform = input_transform\n",
      "predictor.py(59):         self.forecast_generator = forecast_generator\n",
      "predictor.py(60):         self.output_transform = output_transform\n",
      "predictor.py(61):         self.device = resolve_device(device)\n",
      " --- modulename: util, funcname: resolve_device\n",
      "util.py(29):     if device == \"auto\":\n",
      "util.py(30):         if torch.cuda.is_available():\n",
      " --- modulename: __init__, funcname: is_available\n",
      "__init__.py(117):     if not _is_compiled():\n",
      " --- modulename: __init__, funcname: _is_compiled\n",
      "__init__.py(108):     return hasattr(torch._C, \"_cuda_getDeviceCount\")\n",
      "__init__.py(119):     if _nvml_based_avail():\n",
      " --- modulename: __init__, funcname: _nvml_based_avail\n",
      "__init__.py(112):     return os.getenv(\"PYTORCH_NVML_BASED_CUDA_CHECK\") == \"1\"\n",
      " --- modulename: os, funcname: getenv\n",
      "os.py(776):     return environ.get(key, default)\n",
      " --- modulename: _collections_abc, funcname: get\n",
      "_collections_abc.py(823):         try:\n",
      "_collections_abc.py(824):             return self[key]\n",
      " --- modulename: os, funcname: __getitem__\n",
      "os.py(676):         try:\n",
      "os.py(677):             value = self._data[self.encodekey(key)]\n",
      " --- modulename: os, funcname: encode\n",
      "os.py(756):             if not isinstance(value, str):\n",
      "os.py(758):             return value.encode(encoding, 'surrogateescape')\n",
      "os.py(681):         return self.decodevalue(value)\n",
      " --- modulename: os, funcname: decode\n",
      "os.py(760):             return value.decode(encoding, 'surrogateescape')\n",
      "__init__.py(123):         return device_count() > 0\n",
      " --- modulename: __init__, funcname: device_count\n",
      "__init__.py(837):     if not _is_compiled():\n",
      " --- modulename: __init__, funcname: _is_compiled\n",
      "__init__.py(108):     return hasattr(torch._C, \"_cuda_getDeviceCount\")\n",
      "__init__.py(839):     if _cached_device_count is not None:\n",
      "__init__.py(842):     nvml_count = _device_count_amdsmi() if torch.version.hip else _device_count_nvml()\n",
      " --- modulename: __init__, funcname: _device_count_nvml\n",
      "__init__.py(783):     visible_devices = _parse_visible_devices()\n",
      " --- modulename: __init__, funcname: _parse_visible_devices\n",
      "__init__.py(587):     var = os.getenv(\n",
      "__init__.py(588):         \"CUDA_VISIBLE_DEVICES\" if not torch.version.hip else \"HIP_VISIBLE_DEVICES\"\n",
      "__init__.py(587):     var = os.getenv(\n",
      " --- modulename: os, funcname: getenv\n",
      "os.py(776):     return environ.get(key, default)\n",
      " --- modulename: _collections_abc, funcname: get\n",
      "_collections_abc.py(823):         try:\n",
      "_collections_abc.py(824):             return self[key]\n",
      " --- modulename: os, funcname: __getitem__\n",
      "os.py(676):         try:\n",
      "os.py(677):             value = self._data[self.encodekey(key)]\n",
      " --- modulename: os, funcname: encode\n",
      "os.py(756):             if not isinstance(value, str):\n",
      "os.py(758):             return value.encode(encoding, 'surrogateescape')\n",
      "os.py(681):         return self.decodevalue(value)\n",
      " --- modulename: os, funcname: decode\n",
      "os.py(760):             return value.decode(encoding, 'surrogateescape')\n",
      "__init__.py(590):     if var is None:\n",
      "__init__.py(593):     def _strtoul(s: str) -> int:\n",
      "__init__.py(604):     def parse_list_with_prefix(lst: str, prefix: str) -> List[str]:\n",
      " --- modulename: typing, funcname: inner\n",
      "typing.py(308):             try:\n",
      "typing.py(309):                 return cached(*args, **kwds)\n",
      "__init__.py(616):     if var.startswith(\"GPU-\"):\n",
      "__init__.py(618):     if var.startswith(\"MIG-\"):\n",
      "__init__.py(622):     rc: List[int] = []\n",
      "__init__.py(623):     for elem in var.split(\",\"):\n",
      "__init__.py(624):         x = _strtoul(elem.strip())\n",
      " --- modulename: __init__, funcname: _strtoul\n",
      "__init__.py(595):         if not s:\n",
      "__init__.py(597):         for idx, c in enumerate(s):\n",
      "__init__.py(598):             if not (c.isdigit() or (idx == 0 and c in \"+-\")):\n",
      "__init__.py(600):             if idx + 1 == len(s):\n",
      "__init__.py(601):                 idx += 1\n",
      "__init__.py(597):         for idx, c in enumerate(s):\n",
      "__init__.py(602):         return int(s[:idx]) if idx > 0 else -1\n",
      "__init__.py(626):         if x in rc:\n",
      "__init__.py(629):         if x < 0:\n",
      "__init__.py(631):         rc.append(x)\n",
      "__init__.py(623):     for elem in var.split(\",\"):\n",
      "__init__.py(624):         x = _strtoul(elem.strip())\n",
      " --- modulename: __init__, funcname: _strtoul\n",
      "__init__.py(595):         if not s:\n",
      "__init__.py(597):         for idx, c in enumerate(s):\n",
      "__init__.py(598):             if not (c.isdigit() or (idx == 0 and c in \"+-\")):\n",
      "__init__.py(600):             if idx + 1 == len(s):\n",
      "__init__.py(601):                 idx += 1\n",
      "__init__.py(597):         for idx, c in enumerate(s):\n",
      "__init__.py(602):         return int(s[:idx]) if idx > 0 else -1\n",
      "__init__.py(626):         if x in rc:\n",
      "__init__.py(629):         if x < 0:\n",
      "__init__.py(631):         rc.append(x)\n",
      "__init__.py(623):     for elem in var.split(\",\"):\n",
      "__init__.py(624):         x = _strtoul(elem.strip())\n",
      " --- modulename: __init__, funcname: _strtoul\n",
      "__init__.py(595):         if not s:\n",
      "__init__.py(597):         for idx, c in enumerate(s):\n",
      "__init__.py(598):             if not (c.isdigit() or (idx == 0 and c in \"+-\")):\n",
      "__init__.py(600):             if idx + 1 == len(s):\n",
      "__init__.py(601):                 idx += 1\n",
      "__init__.py(597):         for idx, c in enumerate(s):\n",
      "__init__.py(602):         return int(s[:idx]) if idx > 0 else -1\n",
      "__init__.py(626):         if x in rc:\n",
      "__init__.py(629):         if x < 0:\n",
      "__init__.py(631):         rc.append(x)\n",
      "__init__.py(623):     for elem in var.split(\",\"):\n",
      "__init__.py(632):     return rc\n",
      "__init__.py(784):     if not visible_devices:\n",
      "__init__.py(786):     try:\n",
      "__init__.py(787):         if type(visible_devices[0]) is str:\n",
      "__init__.py(798):             raw_cnt = _raw_device_count_nvml()\n",
      " --- modulename: __init__, funcname: _raw_device_count_nvml\n",
      "__init__.py(649):     from ctypes import byref, c_int, CDLL\n",
      " --- modulename: _bootstrap, funcname: _handle_fromlist\n",
      "<frozen importlib._bootstrap>(1063): <frozen importlib._bootstrap>(1064): <frozen importlib._bootstrap>(1071): <frozen importlib._bootstrap>(1075): <frozen importlib._bootstrap>(1063): <frozen importlib._bootstrap>(1064): <frozen importlib._bootstrap>(1071): <frozen importlib._bootstrap>(1075): <frozen importlib._bootstrap>(1063): <frozen importlib._bootstrap>(1064): <frozen importlib._bootstrap>(1071): <frozen importlib._bootstrap>(1075): <frozen importlib._bootstrap>(1063): <frozen importlib._bootstrap>(1087): __init__.py(651):     nvml_h = CDLL(\"libnvidia-ml.so.1\")\n",
      " --- modulename: __init__, funcname: __init__\n",
      "__init__.py(344):         self._name = name\n",
      "__init__.py(345):         flags = self._func_flags_\n",
      "__init__.py(346):         if use_errno:\n",
      "__init__.py(348):         if use_last_error:\n",
      "__init__.py(350):         if _sys.platform.startswith(\"aix\"):\n",
      "__init__.py(358):         if _os.name == \"nt\":\n",
      "__init__.py(368):         class _FuncPtr(_CFuncPtr):\n",
      " --- modulename: __init__, funcname: _FuncPtr\n",
      "__init__.py(368):         class _FuncPtr(_CFuncPtr):\n",
      "__init__.py(369):             _flags_ = flags\n",
      "__init__.py(370):             _restype_ = self._func_restype_\n",
      "__init__.py(371):         self._FuncPtr = _FuncPtr\n",
      "__init__.py(373):         if handle is None:\n",
      "__init__.py(374):             self._handle = _dlopen(self._name, mode)\n",
      "__init__.py(652):     rc = nvml_h.nvmlInit()\n",
      " --- modulename: __init__, funcname: __getattr__\n",
      "__init__.py(385):         if name.startswith('__') and name.endswith('__'):\n",
      "__init__.py(387):         func = self.__getitem__(name)\n",
      " --- modulename: __init__, funcname: __getitem__\n",
      "__init__.py(392):         func = self._FuncPtr((name_or_ordinal, self))\n",
      "__init__.py(393):         if not isinstance(name_or_ordinal, int):\n",
      "__init__.py(394):             func.__name__ = name_or_ordinal\n",
      "__init__.py(395):         return func\n",
      "__init__.py(388):         setattr(self, name, func)\n",
      "__init__.py(389):         return func\n",
      "__init__.py(653):     if rc != 0:\n",
      "__init__.py(656):     dev_count = c_int(-1)\n",
      "__init__.py(657):     rc = nvml_h.nvmlDeviceGetCount_v2(byref(dev_count))\n",
      " --- modulename: __init__, funcname: __getattr__\n",
      "__init__.py(385):         if name.startswith('__') and name.endswith('__'):\n",
      "__init__.py(387):         func = self.__getitem__(name)\n",
      " --- modulename: __init__, funcname: __getitem__\n",
      "__init__.py(392):         func = self._FuncPtr((name_or_ordinal, self))\n",
      "__init__.py(393):         if not isinstance(name_or_ordinal, int):\n",
      "__init__.py(394):             func.__name__ = name_or_ordinal\n",
      "__init__.py(395):         return func\n",
      "__init__.py(388):         setattr(self, name, func)\n",
      "__init__.py(389):         return func\n",
      "__init__.py(658):     if rc != 0:\n",
      "__init__.py(661):     del nvml_h\n",
      "__init__.py(662):     return dev_count.value\n",
      "__init__.py(799):             if raw_cnt <= 0:\n",
      "__init__.py(802):             for idx, val in enumerate(visible_devices):\n",
      "__init__.py(803):                 if cast(int, val) >= raw_cnt:\n",
      " --- modulename: typing, funcname: cast\n",
      "typing.py(1745):     return val\n",
      "__init__.py(802):             for idx, val in enumerate(visible_devices):\n",
      "__init__.py(803):                 if cast(int, val) >= raw_cnt:\n",
      " --- modulename: typing, funcname: cast\n",
      "typing.py(1745):     return val\n",
      "__init__.py(802):             for idx, val in enumerate(visible_devices):\n",
      "__init__.py(803):                 if cast(int, val) >= raw_cnt:\n",
      " --- modulename: typing, funcname: cast\n",
      "typing.py(1745):     return val\n",
      "__init__.py(802):             for idx, val in enumerate(visible_devices):\n",
      "__init__.py(809):     return len(visible_devices)\n",
      "__init__.py(843):     r = torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n",
      "__init__.py(847):     if _initialized:\n",
      "__init__.py(848):         _cached_device_count = r\n",
      "__init__.py(849):     return r\n",
      "util.py(31):             return \"cuda\"\n",
      "predictor.py(62):         self.prediction_net = prediction_net.to(self.device)\n",
      " --- modulename: device_dtype_mixin, funcname: to\n",
      "device_dtype_mixin.py(53):         device, dtype = torch._C._nn._parse_to(*args, **kwargs)[:2]\n",
      "device_dtype_mixin.py(54):         _update_properties(self, device=device, dtype=dtype)\n",
      " --- modulename: device_dtype_mixin, funcname: _update_properties\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2426):             memo = set()\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(117):         if device is not None:\n",
      "device_dtype_mixin.py(118):             module._device = device\n",
      " --- modulename: module, funcname: __setattr__\n",
      "module.py(1732):         def remove_from(*dicts_or_sets):\n",
      "module.py(1740):         params = self.__dict__.get('_parameters')\n",
      "module.py(1741):         if isinstance(value, Parameter):\n",
      " --- modulename: parameter, funcname: __instancecheck__\n",
      "parameter.py(9):         return super().__instancecheck__(instance) or (\n",
      "parameter.py(10):             isinstance(instance, torch.Tensor) and getattr(instance, '_is_param', False))\n",
      "parameter.py(9):         return super().__instancecheck__(instance) or (\n",
      "module.py(1747):         elif params is not None and name in params:\n",
      "module.py(1754):             modules = self.__dict__.get('_modules')\n",
      "module.py(1755):             if isinstance(value, Module):\n",
      "module.py(1765):             elif modules is not None and name in modules:\n",
      "module.py(1776):                 buffers = self.__dict__.get('_buffers')\n",
      "module.py(1777):                 if buffers is not None and name in buffers:\n",
      "module.py(1788):                     super().__setattr__(name, value)\n",
      "device_dtype_mixin.py(119):         if dtype is not None:\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "device_dtype_mixin.py(55):         return super().to(*args, **kwargs)\n",
      " --- modulename: module, funcname: to\n",
      "module.py(1138):         device, dtype, non_blocking, convert_to_format = torch._C._nn._parse_to(*args, **kwargs)\n",
      "module.py(1140):         if dtype is not None:\n",
      "module.py(1151):         def convert(t):\n",
      "module.py(1174):         return self._apply(convert)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "predictor.py(63):         self.required_fields = [\"forecast_start\", \"item_id\", \"info\"]\n",
      "Ending trace...\n"
     ]
    }
   ],
   "source": [
    "verbose_forecast_model.model.create_predictor = verbose_trace(model.create_predictor)\n",
    "predictor = verbose_forecast_model.model.create_predictor(batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b49873bd-7234-4727-bdac-f40d63d7da08",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m predictor\u001b[38;5;241m.\u001b[39mpredict \u001b[38;5;241m=\u001b[39m verbose_trace(\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m)\n\u001b[1;32m      2\u001b[0m forecasts \u001b[38;5;241m=\u001b[39m predictor\u001b[38;5;241m.\u001b[39mpredict(target)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "predictor.predict = verbose_trace(predictor.predict)\n",
    "forecasts = predictor.predict(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712e6044-0ce3-4fc0-9a7b-a0793cefef35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model\n",
    "model = MoiraiForecast(\n",
    "    module=module,\n",
    "    prediction_length=48,\n",
    "    context_length=168,\n",
    "    patch_size=32,\n",
    "    num_samples=100,\n",
    "    target_dim=inp[\"target\"].shape[1],\n",
    "    feat_dynamic_real_dim=0,\n",
    "    past_feat_dynamic_real_dim=0,\n",
    ")\n",
    "predictor = model.create_predictor(batch_size = 32)\n",
    "predictor.predict(target)\n",
    "network = predictor.prediction_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9603f0a3-7eb7-4042-82a2-7cbcb3893604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m        \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mType:\u001b[0m            MoiraiForecast\n",
       "\u001b[0;31mString form:\u001b[0m    \n",
       "MoiraiForecast(\n",
       "           (module): MoiraiModule(\n",
       "           (mask_encoding): Embedding(1, 384)\n",
       "           (scaler): Pa <...> s=(8, 16, 32, 64, 128), bias=True, dtype=torch.float32)\n",
       "           )\n",
       "           )\n",
       "           )\n",
       "           )\n",
       "           )\n",
       "           )\n",
       "\u001b[0;31mFile:\u001b[0m            ~/work/nbs_pipeline/uni2ts/src/uni2ts/model/moirai/forecast.py\n",
       "\u001b[0;31mDocstring:\u001b[0m       <no docstring>\n",
       "\u001b[0;31mClass docstring:\u001b[0m Hooks to be used in LightningModule."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "? model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50181b8-c0bf-4c12-9987-e657af01fa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "? network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02294f0-696f-4e01-80fe-cbd7823a2dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "? module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "90e8fa8c-65f4-403b-a70e-bd8be97e3209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trace...\n",
      " --- modulename: forecast, funcname: create_predictor\n",
      "forecast.py(128):         ts_fields = []\n",
      "forecast.py(129):         if self.hparams.feat_dynamic_real_dim > 0:\n",
      " --- modulename: hparams_mixin, funcname: hparams\n",
      "hparams_mixin.py(161):         if not hasattr(self, \"_hparams\"):\n",
      "hparams_mixin.py(163):         return self._hparams\n",
      " --- modulename: data, funcname: __getattr__\n",
      "data.py(486):         try:\n",
      "data.py(487):             return self[key]\n",
      "forecast.py(132):         past_ts_fields = []\n",
      "forecast.py(133):         if self.hparams.past_feat_dynamic_real_dim > 0:\n",
      " --- modulename: hparams_mixin, funcname: hparams\n",
      "hparams_mixin.py(161):         if not hasattr(self, \"_hparams\"):\n",
      "hparams_mixin.py(163):         return self._hparams\n",
      " --- modulename: data, funcname: __getattr__\n",
      "data.py(486):         try:\n",
      "data.py(487):             return self[key]\n",
      "forecast.py(136):         instance_splitter = TFTInstanceSplitter(\n",
      "forecast.py(137):             instance_sampler=TestSplitSampler(),\n",
      " --- modulename: sampler, funcname: TestSplitSampler\n",
      "sampler.py(121):     return PredictionSplitSampler(\n",
      "sampler.py(122):         allow_empty_interval=False,\n",
      "sampler.py(123):         axis=axis,\n",
      "sampler.py(124):         min_past=min_past,\n",
      "sampler.py(125):         min_future=0,\n",
      "sampler.py(121):     return PredictionSplitSampler(\n",
      " --- modulename: main, funcname: __init__\n",
      "main.py(339):         values, fields_set, validation_error = validate_model(__pydantic_self__.__class__, data)\n",
      " --- modulename: main, funcname: validate_model\n",
      "main.py(1036):     values = {}\n",
      "main.py(1037):     errors = []\n",
      "main.py(1039):     names_used = set()\n",
      "main.py(1041):     fields_set = set()\n",
      "main.py(1042):     config = model.__config__\n",
      "main.py(1043):     check_extra = config.extra is not Extra.ignore\n",
      "main.py(1044):     cls_ = cls or model\n",
      "main.py(1046):     for validator in model.__pre_root_validators__:\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: int_validator\n",
      "validators.py(128):     if isinstance(v, int) and not (v is True or v is False):\n",
      "validators.py(129):         return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: int_validator\n",
      "validators.py(128):     if isinstance(v, int) and not (v is True or v is False):\n",
      "validators.py(129):         return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: int_validator\n",
      "validators.py(128):     if isinstance(v, int) and not (v is True or v is False):\n",
      "validators.py(129):         return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: bool_validator\n",
      "validators.py(107):     if v is True or v is False:\n",
      "validators.py(108):         return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1082):     if check_extra:\n",
      "main.py(1096):     for skip_on_failure, validator in model.__post_root_validators__:\n",
      "main.py(1104):     if errors:\n",
      "main.py(1107):         return values, fields_set, None\n",
      "main.py(340):         if validation_error:\n",
      "main.py(342):         try:\n",
      "main.py(343):             object_setattr(__pydantic_self__, '__dict__', values)\n",
      "main.py(348):         object_setattr(__pydantic_self__, '__fields_set__', fields_set)\n",
      "main.py(349):         __pydantic_self__._init_private_attributes()\n",
      " --- modulename: main, funcname: _init_private_attributes\n",
      "main.py(422):         for name, private_attr in self.__private_attributes__.items():\n",
      "forecast.py(138):             past_length=self.past_length,\n",
      " --- modulename: forecast, funcname: past_length\n",
      "forecast.py(224):             if self.hparams.patch_size == \"auto\"\n",
      " --- modulename: hparams_mixin, funcname: hparams\n",
      "hparams_mixin.py(161):         if not hasattr(self, \"_hparams\"):\n",
      "hparams_mixin.py(163):         return self._hparams\n",
      " --- modulename: data, funcname: __getattr__\n",
      "data.py(486):         try:\n",
      "data.py(487):             return self[key]\n",
      "forecast.py(225):             else self.hparams.context_length\n",
      " --- modulename: hparams_mixin, funcname: hparams\n",
      "hparams_mixin.py(161):         if not hasattr(self, \"_hparams\"):\n",
      "hparams_mixin.py(163):         return self._hparams\n",
      " --- modulename: data, funcname: __getattr__\n",
      "data.py(486):         try:\n",
      "data.py(487):             return self[key]\n",
      "forecast.py(222):         return (\n",
      "forecast.py(139):             future_length=self.hparams.prediction_length,\n",
      " --- modulename: hparams_mixin, funcname: hparams\n",
      "hparams_mixin.py(161):         if not hasattr(self, \"_hparams\"):\n",
      "hparams_mixin.py(163):         return self._hparams\n",
      " --- modulename: data, funcname: __getattr__\n",
      "data.py(486):         try:\n",
      "data.py(487):             return self[key]\n",
      "forecast.py(140):             observed_value_field=\"observed_target\",\n",
      "forecast.py(141):             time_series_fields=ts_fields,\n",
      "forecast.py(142):             past_time_series_fields=past_ts_fields,\n",
      "forecast.py(136):         instance_splitter = TFTInstanceSplitter(\n",
      " --- modulename: component, funcname: init_wrapper\n",
      "component.py(336):             self, *args = args\n",
      "component.py(338):             nmargs = {\n",
      "component.py(340):                 for (name, param), arg in zip(\n",
      "component.py(341):                     list(init_params.items()), [self] + args\n",
      "component.py(340):                 for (name, param), arg in zip(\n",
      "component.py(338):             nmargs = {\n",
      " --- modulename: component, funcname: <dictcomp>\n",
      "component.py(338):             nmargs = {\n",
      "component.py(340):                 for (name, param), arg in zip(\n",
      "component.py(343):                 if name != \"self\"\n",
      "component.py(338):             nmargs = {\n",
      "component.py(343):                 if name != \"self\"\n",
      "component.py(345):             model = PydanticModel(**{**nmargs, **kwargs})\n",
      " --- modulename: main, funcname: __init__\n",
      "main.py(339):         values, fields_set, validation_error = validate_model(__pydantic_self__.__class__, data)\n",
      " --- modulename: main, funcname: validate_model\n",
      "main.py(1036):     values = {}\n",
      "main.py(1037):     errors = []\n",
      "main.py(1039):     names_used = set()\n",
      "main.py(1041):     fields_set = set()\n",
      "main.py(1042):     config = model.__config__\n",
      "main.py(1043):     check_extra = config.extra is not Extra.ignore\n",
      "main.py(1044):     cls_ = cls or model\n",
      "main.py(1046):     for validator in model.__pre_root_validators__:\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: main, funcname: validate\n",
      "main.py(684):         if isinstance(value, cls):\n",
      " --- modulename: main, funcname: __instancecheck__\n",
      "main.py(304):         return hasattr(instance, '__fields__') and super().__instancecheck__(instance)\n",
      " --- modulename: abc, funcname: __instancecheck__\n",
      "abc.py(119):             return _abc_instancecheck(cls, instance)\n",
      "main.py(685):             copy_on_model_validation = cls.__config__.copy_on_model_validation\n",
      "main.py(687):             deep_copy: Optional[bool] = None\n",
      "main.py(688):             if copy_on_model_validation not in {'deep', 'shallow', 'none'}:\n",
      "main.py(696):             if copy_on_model_validation == 'shallow':\n",
      "main.py(698):                 deep_copy = False\n",
      "main.py(703):             if deep_copy is None:\n",
      "main.py(706):                 return value._copy_and_set_values(value.__dict__, value.__fields_set__, deep=deep_copy)\n",
      " --- modulename: main, funcname: _copy_and_set_values\n",
      "main.py(610):         if deep:\n",
      "main.py(614):         cls = self.__class__\n",
      "main.py(615):         m = cls.__new__(cls)\n",
      "main.py(616):         object_setattr(m, '__dict__', values)\n",
      "main.py(617):         object_setattr(m, '__fields_set__', fields_set)\n",
      "main.py(618):         for name in self.__private_attributes__:\n",
      "main.py(625):         return m\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: int_validator\n",
      "validators.py(128):     if isinstance(v, int) and not (v is True or v is False):\n",
      "validators.py(129):         return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: int_validator\n",
      "validators.py(128):     if isinstance(v, int) and not (v is True or v is False):\n",
      "validators.py(129):         return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1060):             if field.required:\n",
      "main.py(1064):             value = field.get_default()\n",
      " --- modulename: fields, funcname: get_default\n",
      "fields.py(437):         return smart_deepcopy(self.default) if self.default_factory is None else self.default_factory()\n",
      " --- modulename: utils, funcname: smart_deepcopy\n",
      "utils.py(682):     obj_type = obj.__class__\n",
      "utils.py(683):     if obj_type in IMMUTABLE_NON_COLLECTIONS_TYPES:\n",
      "utils.py(684):         return obj  # fastest case: obj is immutable and not collection therefore will not be copied anyway\n",
      "main.py(1066):             if not config.validate_all and not field.validate_always:\n",
      "main.py(1067):                 values[name] = value\n",
      "main.py(1068):                 continue\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1060):             if field.required:\n",
      "main.py(1064):             value = field.get_default()\n",
      " --- modulename: fields, funcname: get_default\n",
      "fields.py(437):         return smart_deepcopy(self.default) if self.default_factory is None else self.default_factory()\n",
      " --- modulename: utils, funcname: smart_deepcopy\n",
      "utils.py(682):     obj_type = obj.__class__\n",
      "utils.py(683):     if obj_type in IMMUTABLE_NON_COLLECTIONS_TYPES:\n",
      "utils.py(684):         return obj  # fastest case: obj is immutable and not collection therefore will not be copied anyway\n",
      "main.py(1066):             if not config.validate_all and not field.validate_always:\n",
      "main.py(1067):                 values[name] = value\n",
      "main.py(1068):                 continue\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1060):             if field.required:\n",
      "main.py(1064):             value = field.get_default()\n",
      " --- modulename: fields, funcname: get_default\n",
      "fields.py(437):         return smart_deepcopy(self.default) if self.default_factory is None else self.default_factory()\n",
      " --- modulename: utils, funcname: smart_deepcopy\n",
      "utils.py(682):     obj_type = obj.__class__\n",
      "utils.py(683):     if obj_type in IMMUTABLE_NON_COLLECTIONS_TYPES:\n",
      "utils.py(684):         return obj  # fastest case: obj is immutable and not collection therefore will not be copied anyway\n",
      "main.py(1066):             if not config.validate_all and not field.validate_always:\n",
      "main.py(1067):                 values[name] = value\n",
      "main.py(1068):                 continue\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1060):             if field.required:\n",
      "main.py(1064):             value = field.get_default()\n",
      " --- modulename: fields, funcname: get_default\n",
      "fields.py(437):         return smart_deepcopy(self.default) if self.default_factory is None else self.default_factory()\n",
      " --- modulename: utils, funcname: smart_deepcopy\n",
      "utils.py(682):     obj_type = obj.__class__\n",
      "utils.py(683):     if obj_type in IMMUTABLE_NON_COLLECTIONS_TYPES:\n",
      "utils.py(684):         return obj  # fastest case: obj is immutable and not collection therefore will not be copied anyway\n",
      "main.py(1066):             if not config.validate_all and not field.validate_always:\n",
      "main.py(1067):                 values[name] = value\n",
      "main.py(1068):                 continue\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: str_validator\n",
      "validators.py(60):     if isinstance(v, str):\n",
      "validators.py(61):         if isinstance(v, Enum):\n",
      "validators.py(64):             return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1060):             if field.required:\n",
      "main.py(1064):             value = field.get_default()\n",
      " --- modulename: fields, funcname: get_default\n",
      "fields.py(437):         return smart_deepcopy(self.default) if self.default_factory is None else self.default_factory()\n",
      " --- modulename: utils, funcname: smart_deepcopy\n",
      "utils.py(682):     obj_type = obj.__class__\n",
      "utils.py(683):     if obj_type in IMMUTABLE_NON_COLLECTIONS_TYPES:\n",
      "utils.py(684):         return obj  # fastest case: obj is immutable and not collection therefore will not be copied anyway\n",
      "main.py(1066):             if not config.validate_all and not field.validate_always:\n",
      "main.py(1067):                 values[name] = value\n",
      "main.py(1068):                 continue\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1060):             if field.required:\n",
      "main.py(1064):             value = field.get_default()\n",
      " --- modulename: fields, funcname: get_default\n",
      "fields.py(437):         return smart_deepcopy(self.default) if self.default_factory is None else self.default_factory()\n",
      " --- modulename: utils, funcname: smart_deepcopy\n",
      "utils.py(682):     obj_type = obj.__class__\n",
      "utils.py(683):     if obj_type in IMMUTABLE_NON_COLLECTIONS_TYPES:\n",
      "utils.py(684):         return obj  # fastest case: obj is immutable and not collection therefore will not be copied anyway\n",
      "main.py(1066):             if not config.validate_all and not field.validate_always:\n",
      "main.py(1067):                 values[name] = value\n",
      "main.py(1068):                 continue\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(882):         elif self.shape in MAPPING_LIKE_SHAPES:\n",
      "fields.py(884):         elif self.shape == SHAPE_TUPLE:\n",
      "fields.py(886):         elif self.shape == SHAPE_ITERABLE:\n",
      "fields.py(888):         elif self.shape == SHAPE_GENERIC:\n",
      "fields.py(892):             v, errors = self._validate_sequence_like(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_sequence_like\n",
      "fields.py(906):         if not sequence_like(v):\n",
      " --- modulename: utils, funcname: sequence_like\n",
      "utils.py(158):     return isinstance(v, (list, tuple, set, frozenset, GeneratorType, deque))\n",
      "fields.py(920):         loc = loc if isinstance(loc, tuple) else (loc,)\n",
      "fields.py(921):         result = []\n",
      "fields.py(922):         errors: List[ErrorList] = []\n",
      "fields.py(923):         for i, v_ in enumerate(v):\n",
      "fields.py(931):         if errors:\n",
      "fields.py(934):         converted: Union[List[Any], Set[Any], FrozenSet[Any], Tuple[Any, ...], Iterator[Any], Deque[Any]] = result\n",
      "fields.py(936):         if self.shape == SHAPE_SET:\n",
      "fields.py(938):         elif self.shape == SHAPE_FROZENSET:\n",
      "fields.py(940):         elif self.shape == SHAPE_TUPLE_ELLIPSIS:\n",
      "fields.py(942):         elif self.shape == SHAPE_DEQUE:\n",
      "fields.py(944):         elif self.shape == SHAPE_SEQUENCE:\n",
      "fields.py(953):         return converted, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(882):         elif self.shape in MAPPING_LIKE_SHAPES:\n",
      "fields.py(884):         elif self.shape == SHAPE_TUPLE:\n",
      "fields.py(886):         elif self.shape == SHAPE_ITERABLE:\n",
      "fields.py(888):         elif self.shape == SHAPE_GENERIC:\n",
      "fields.py(892):             v, errors = self._validate_sequence_like(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_sequence_like\n",
      "fields.py(906):         if not sequence_like(v):\n",
      " --- modulename: utils, funcname: sequence_like\n",
      "utils.py(158):     return isinstance(v, (list, tuple, set, frozenset, GeneratorType, deque))\n",
      "fields.py(920):         loc = loc if isinstance(loc, tuple) else (loc,)\n",
      "fields.py(921):         result = []\n",
      "fields.py(922):         errors: List[ErrorList] = []\n",
      "fields.py(923):         for i, v_ in enumerate(v):\n",
      "fields.py(931):         if errors:\n",
      "fields.py(934):         converted: Union[List[Any], Set[Any], FrozenSet[Any], Tuple[Any, ...], Iterator[Any], Deque[Any]] = result\n",
      "fields.py(936):         if self.shape == SHAPE_SET:\n",
      "fields.py(938):         elif self.shape == SHAPE_FROZENSET:\n",
      "fields.py(940):         elif self.shape == SHAPE_TUPLE_ELLIPSIS:\n",
      "fields.py(942):         elif self.shape == SHAPE_DEQUE:\n",
      "fields.py(944):         elif self.shape == SHAPE_SEQUENCE:\n",
      "fields.py(953):         return converted, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1060):             if field.required:\n",
      "main.py(1064):             value = field.get_default()\n",
      " --- modulename: fields, funcname: get_default\n",
      "fields.py(437):         return smart_deepcopy(self.default) if self.default_factory is None else self.default_factory()\n",
      " --- modulename: utils, funcname: smart_deepcopy\n",
      "utils.py(682):     obj_type = obj.__class__\n",
      "utils.py(683):     if obj_type in IMMUTABLE_NON_COLLECTIONS_TYPES:\n",
      "utils.py(684):         return obj  # fastest case: obj is immutable and not collection therefore will not be copied anyway\n",
      "main.py(1066):             if not config.validate_all and not field.validate_always:\n",
      "main.py(1067):                 values[name] = value\n",
      "main.py(1068):                 continue\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1082):     if check_extra:\n",
      "main.py(1096):     for skip_on_failure, validator in model.__post_root_validators__:\n",
      "main.py(1104):     if errors:\n",
      "main.py(1107):         return values, fields_set, None\n",
      "main.py(340):         if validation_error:\n",
      "main.py(342):         try:\n",
      "main.py(343):             object_setattr(__pydantic_self__, '__dict__', values)\n",
      "main.py(348):         object_setattr(__pydantic_self__, '__fields_set__', fields_set)\n",
      "main.py(349):         __pydantic_self__._init_private_attributes()\n",
      " --- modulename: main, funcname: _init_private_attributes\n",
      "main.py(422):         for name, private_attr in self.__private_attributes__.items():\n",
      "component.py(348):             all_args = {**nmargs, **kwargs, **model.__dict__}\n",
      "component.py(353):             if not getattr(self, \"__init_args__\", {}):\n",
      "component.py(354):                 self.__init_args__ = OrderedDict(\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(355):                     {\n",
      " --- modulename: component, funcname: <dictcomp>\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(354):                 self.__init_args__ = OrderedDict(\n",
      "component.py(361):                 self.__class__.__getnewargs_ex__ = validated_getnewargs_ex\n",
      "component.py(362):                 self.__class__.__repr__ = validated_repr\n",
      "component.py(364):             return init(self, **all_args)\n",
      " --- modulename: split, funcname: __init__\n",
      "split.py(503):         super().__init__(\n",
      "split.py(504):             target_field=target_field,\n",
      "split.py(505):             is_pad_field=is_pad_field,\n",
      "split.py(506):             start_field=start_field,\n",
      "split.py(507):             forecast_start_field=forecast_start_field,\n",
      "split.py(508):             instance_sampler=instance_sampler,\n",
      "split.py(509):             past_length=past_length,\n",
      "split.py(510):             future_length=future_length,\n",
      "split.py(511):             lead_time=lead_time,\n",
      "split.py(512):             output_NTC=output_NTC,\n",
      "split.py(513):             time_series_fields=time_series_fields,\n",
      "split.py(514):             dummy_value=dummy_value,\n",
      "split.py(503):         super().__init__(\n",
      " --- modulename: component, funcname: init_wrapper\n",
      "component.py(336):             self, *args = args\n",
      "component.py(338):             nmargs = {\n",
      "component.py(340):                 for (name, param), arg in zip(\n",
      "component.py(341):                     list(init_params.items()), [self] + args\n",
      "component.py(340):                 for (name, param), arg in zip(\n",
      "component.py(338):             nmargs = {\n",
      " --- modulename: component, funcname: <dictcomp>\n",
      "component.py(338):             nmargs = {\n",
      "component.py(340):                 for (name, param), arg in zip(\n",
      "component.py(343):                 if name != \"self\"\n",
      "component.py(338):             nmargs = {\n",
      "component.py(343):                 if name != \"self\"\n",
      "component.py(345):             model = PydanticModel(**{**nmargs, **kwargs})\n",
      " --- modulename: main, funcname: __init__\n",
      "main.py(339):         values, fields_set, validation_error = validate_model(__pydantic_self__.__class__, data)\n",
      " --- modulename: main, funcname: validate_model\n",
      "main.py(1036):     values = {}\n",
      "main.py(1037):     errors = []\n",
      "main.py(1039):     names_used = set()\n",
      "main.py(1041):     fields_set = set()\n",
      "main.py(1042):     config = model.__config__\n",
      "main.py(1043):     check_extra = config.extra is not Extra.ignore\n",
      "main.py(1044):     cls_ = cls or model\n",
      "main.py(1046):     for validator in model.__pre_root_validators__:\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: str_validator\n",
      "validators.py(60):     if isinstance(v, str):\n",
      "validators.py(61):         if isinstance(v, Enum):\n",
      "validators.py(64):             return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: str_validator\n",
      "validators.py(60):     if isinstance(v, str):\n",
      "validators.py(61):         if isinstance(v, Enum):\n",
      "validators.py(64):             return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: str_validator\n",
      "validators.py(60):     if isinstance(v, str):\n",
      "validators.py(61):         if isinstance(v, Enum):\n",
      "validators.py(64):             return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: str_validator\n",
      "validators.py(60):     if isinstance(v, str):\n",
      "validators.py(61):         if isinstance(v, Enum):\n",
      "validators.py(64):             return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: main, funcname: validate\n",
      "main.py(684):         if isinstance(value, cls):\n",
      " --- modulename: main, funcname: __instancecheck__\n",
      "main.py(304):         return hasattr(instance, '__fields__') and super().__instancecheck__(instance)\n",
      " --- modulename: abc, funcname: __instancecheck__\n",
      "abc.py(119):             return _abc_instancecheck(cls, instance)\n",
      "main.py(685):             copy_on_model_validation = cls.__config__.copy_on_model_validation\n",
      "main.py(687):             deep_copy: Optional[bool] = None\n",
      "main.py(688):             if copy_on_model_validation not in {'deep', 'shallow', 'none'}:\n",
      "main.py(696):             if copy_on_model_validation == 'shallow':\n",
      "main.py(698):                 deep_copy = False\n",
      "main.py(703):             if deep_copy is None:\n",
      "main.py(706):                 return value._copy_and_set_values(value.__dict__, value.__fields_set__, deep=deep_copy)\n",
      " --- modulename: main, funcname: _copy_and_set_values\n",
      "main.py(610):         if deep:\n",
      "main.py(614):         cls = self.__class__\n",
      "main.py(615):         m = cls.__new__(cls)\n",
      "main.py(616):         object_setattr(m, '__dict__', values)\n",
      "main.py(617):         object_setattr(m, '__fields_set__', fields_set)\n",
      "main.py(618):         for name in self.__private_attributes__:\n",
      "main.py(625):         return m\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: int_validator\n",
      "validators.py(128):     if isinstance(v, int) and not (v is True or v is False):\n",
      "validators.py(129):         return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: int_validator\n",
      "validators.py(128):     if isinstance(v, int) and not (v is True or v is False):\n",
      "validators.py(129):         return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: int_validator\n",
      "validators.py(128):     if isinstance(v, int) and not (v is True or v is False):\n",
      "validators.py(129):         return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: bool_validator\n",
      "validators.py(107):     if v is True or v is False:\n",
      "validators.py(108):         return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(882):         elif self.shape in MAPPING_LIKE_SHAPES:\n",
      "fields.py(884):         elif self.shape == SHAPE_TUPLE:\n",
      "fields.py(886):         elif self.shape == SHAPE_ITERABLE:\n",
      "fields.py(888):         elif self.shape == SHAPE_GENERIC:\n",
      "fields.py(892):             v, errors = self._validate_sequence_like(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_sequence_like\n",
      "fields.py(906):         if not sequence_like(v):\n",
      " --- modulename: utils, funcname: sequence_like\n",
      "utils.py(158):     return isinstance(v, (list, tuple, set, frozenset, GeneratorType, deque))\n",
      "fields.py(920):         loc = loc if isinstance(loc, tuple) else (loc,)\n",
      "fields.py(921):         result = []\n",
      "fields.py(922):         errors: List[ErrorList] = []\n",
      "fields.py(923):         for i, v_ in enumerate(v):\n",
      "fields.py(931):         if errors:\n",
      "fields.py(934):         converted: Union[List[Any], Set[Any], FrozenSet[Any], Tuple[Any, ...], Iterator[Any], Deque[Any]] = result\n",
      "fields.py(936):         if self.shape == SHAPE_SET:\n",
      "fields.py(938):         elif self.shape == SHAPE_FROZENSET:\n",
      "fields.py(940):         elif self.shape == SHAPE_TUPLE_ELLIPSIS:\n",
      "fields.py(942):         elif self.shape == SHAPE_DEQUE:\n",
      "fields.py(944):         elif self.shape == SHAPE_SEQUENCE:\n",
      "fields.py(953):         return converted, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: float_validator\n",
      "validators.py(153):     if isinstance(v, float):\n",
      "validators.py(154):         return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1082):     if check_extra:\n",
      "main.py(1096):     for skip_on_failure, validator in model.__post_root_validators__:\n",
      "main.py(1104):     if errors:\n",
      "main.py(1107):         return values, fields_set, None\n",
      "main.py(340):         if validation_error:\n",
      "main.py(342):         try:\n",
      "main.py(343):             object_setattr(__pydantic_self__, '__dict__', values)\n",
      "main.py(348):         object_setattr(__pydantic_self__, '__fields_set__', fields_set)\n",
      "main.py(349):         __pydantic_self__._init_private_attributes()\n",
      " --- modulename: main, funcname: _init_private_attributes\n",
      "main.py(422):         for name, private_attr in self.__private_attributes__.items():\n",
      "component.py(348):             all_args = {**nmargs, **kwargs, **model.__dict__}\n",
      "component.py(353):             if not getattr(self, \"__init_args__\", {}):\n",
      "component.py(364):             return init(self, **all_args)\n",
      " --- modulename: split, funcname: __init__\n",
      "split.py(86):         super().__init__()\n",
      " --- modulename: component, funcname: init_wrapper\n",
      "component.py(336):             self, *args = args\n",
      "component.py(338):             nmargs = {\n",
      "component.py(340):                 for (name, param), arg in zip(\n",
      "component.py(341):                     list(init_params.items()), [self] + args\n",
      "component.py(340):                 for (name, param), arg in zip(\n",
      "component.py(338):             nmargs = {\n",
      " --- modulename: component, funcname: <dictcomp>\n",
      "component.py(338):             nmargs = {\n",
      "component.py(340):                 for (name, param), arg in zip(\n",
      "component.py(343):                 if name != \"self\"\n",
      "component.py(338):             nmargs = {\n",
      "component.py(343):                 if name != \"self\"\n",
      "component.py(345):             model = PydanticModel(**{**nmargs, **kwargs})\n",
      " --- modulename: main, funcname: __init__\n",
      "main.py(339):         values, fields_set, validation_error = validate_model(__pydantic_self__.__class__, data)\n",
      " --- modulename: main, funcname: validate_model\n",
      "main.py(1036):     values = {}\n",
      "main.py(1037):     errors = []\n",
      "main.py(1039):     names_used = set()\n",
      "main.py(1041):     fields_set = set()\n",
      "main.py(1042):     config = model.__config__\n",
      "main.py(1043):     check_extra = config.extra is not Extra.ignore\n",
      "main.py(1044):     cls_ = cls or model\n",
      "main.py(1046):     for validator in model.__pre_root_validators__:\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1082):     if check_extra:\n",
      "main.py(1096):     for skip_on_failure, validator in model.__post_root_validators__:\n",
      "main.py(1104):     if errors:\n",
      "main.py(1107):         return values, fields_set, None\n",
      "main.py(340):         if validation_error:\n",
      "main.py(342):         try:\n",
      "main.py(343):             object_setattr(__pydantic_self__, '__dict__', values)\n",
      "main.py(348):         object_setattr(__pydantic_self__, '__fields_set__', fields_set)\n",
      "main.py(349):         __pydantic_self__._init_private_attributes()\n",
      " --- modulename: main, funcname: _init_private_attributes\n",
      "main.py(422):         for name, private_attr in self.__private_attributes__.items():\n",
      "component.py(348):             all_args = {**nmargs, **kwargs, **model.__dict__}\n",
      "component.py(353):             if not getattr(self, \"__init_args__\", {}):\n",
      "component.py(364):             return init(self, **all_args)\n",
      " --- modulename: _base, funcname: __init__\n",
      "_base.py(180):         self.max_idle_transforms = env.max_idle_transforms\n",
      " --- modulename: settings, funcname: __getattribute__\n",
      "settings.py(286):         if key.startswith(\"_\"):\n",
      "settings.py(289):             return self[key]\n",
      " --- modulename: settings, funcname: __getitem__\n",
      "settings.py(273):         if key in self._dependencies:\n",
      " --- modulename: settings, funcname: __getattribute__\n",
      "settings.py(286):         if key.startswith(\"_\"):\n",
      "settings.py(287):             return super().__getattribute__(key)\n",
      "settings.py(276):         for dct in self._chain.reverse():\n",
      " --- modulename: settings, funcname: __getattribute__\n",
      "settings.py(286):         if key.startswith(\"_\"):\n",
      "settings.py(287):             return super().__getattribute__(key)\n",
      " --- modulename: settings, funcname: reverse\n",
      "settings.py(137):         current = self.end\n",
      "settings.py(139):         while current is not None:\n",
      "settings.py(140):             yield current.val\n",
      "settings.py(277):             try:\n",
      "settings.py(278):                 return dct[key]\n",
      "settings.py(279):             except KeyError:\n",
      "settings.py(280):                 pass\n",
      "settings.py(276):         for dct in self._chain.reverse():\n",
      " --- modulename: settings, funcname: reverse\n",
      "settings.py(141):             current = current.prv\n",
      "settings.py(139):         while current is not None:\n",
      "settings.py(140):             yield current.val\n",
      "settings.py(277):             try:\n",
      "settings.py(278):                 return dct[key]\n",
      " --- modulename: settings, funcname: reverse\n",
      "split.py(88):         assert future_length > 0, \"The value of `future_length` should be > 0\"\n",
      "split.py(90):         self.instance_sampler = instance_sampler\n",
      "split.py(91):         self.past_length = past_length\n",
      "split.py(92):         self.future_length = future_length\n",
      "split.py(93):         self.lead_time = lead_time\n",
      "split.py(94):         self.output_NTC = output_NTC\n",
      "split.py(95):         self.ts_fields = time_series_fields\n",
      "split.py(96):         self.target_field = target_field\n",
      "split.py(97):         self.is_pad_field = is_pad_field\n",
      "split.py(98):         self.start_field = start_field\n",
      "split.py(99):         self.forecast_start_field = forecast_start_field\n",
      "split.py(100):         self.dummy_value = dummy_value\n",
      "split.py(517):         assert past_length > 0, \"The value of `past_length` should be > 0\"\n",
      "split.py(519):         self.observed_value_field = observed_value_field\n",
      "split.py(520):         self.past_ts_fields = past_time_series_fields\n",
      "forecast.py(144):         return PyTorchPredictor(\n",
      "forecast.py(145):             input_names=self.prediction_input_names,\n",
      " --- modulename: forecast, funcname: prediction_input_names\n",
      "forecast.py(214):         return list(self.describe_inputs())\n",
      " --- modulename: forecast, funcname: describe_inputs\n",
      "forecast.py(155):             \"past_target\": Input(\n",
      "forecast.py(157):                     batch_size,\n",
      "forecast.py(158):                     self.past_length,\n",
      " --- modulename: forecast, funcname: past_length\n",
      "forecast.py(224):             if self.hparams.patch_size == \"auto\"\n",
      " --- modulename: hparams_mixin, funcname: hparams\n",
      "hparams_mixin.py(161):         if not hasattr(self, \"_hparams\"):\n",
      "hparams_mixin.py(163):         return self._hparams\n",
      " --- modulename: data, funcname: __getattr__\n",
      "data.py(486):         try:\n",
      "data.py(487):             return self[key]\n",
      "forecast.py(225):             else self.hparams.context_length\n",
      " --- modulename: hparams_mixin, funcname: hparams\n",
      "hparams_mixin.py(161):         if not hasattr(self, \"_hparams\"):\n",
      "hparams_mixin.py(163):         return self._hparams\n",
      " --- modulename: data, funcname: __getattr__\n",
      "data.py(486):         try:\n",
      "data.py(487):             return self[key]\n",
      "forecast.py(222):         return (\n",
      "forecast.py(159):                     self.hparams.target_dim,\n",
      " --- modulename: hparams_mixin, funcname: hparams\n",
      "hparams_mixin.py(161):         if not hasattr(self, \"_hparams\"):\n",
      "hparams_mixin.py(163):         return self._hparams\n",
      " --- modulename: data, funcname: __getattr__\n",
      "data.py(486):         try:\n",
      "data.py(487):             return self[key]\n",
      "forecast.py(156):                 shape=(\n",
      "forecast.py(161):                 dtype=torch.float,\n",
      "forecast.py(155):             \"past_target\": Input(\n",
      " --- modulename: inputs, funcname: __init__\n",
      "<string>(3): <string>(4): <string>(5): forecast.py(163):             \"past_observed_target\": Input(\n",
      "forecast.py(165):                     batch_size,\n",
      "forecast.py(166):                     self.past_length,\n",
      " --- modulename: forecast, funcname: past_length\n",
      "forecast.py(224):             if self.hparams.patch_size == \"auto\"\n",
      " --- modulename: hparams_mixin, funcname: hparams\n",
      "hparams_mixin.py(161):         if not hasattr(self, \"_hparams\"):\n",
      "hparams_mixin.py(163):         return self._hparams\n",
      " --- modulename: data, funcname: __getattr__\n",
      "data.py(486):         try:\n",
      "data.py(487):             return self[key]\n",
      "forecast.py(225):             else self.hparams.context_length\n",
      " --- modulename: hparams_mixin, funcname: hparams\n",
      "hparams_mixin.py(161):         if not hasattr(self, \"_hparams\"):\n",
      "hparams_mixin.py(163):         return self._hparams\n",
      " --- modulename: data, funcname: __getattr__\n",
      "data.py(486):         try:\n",
      "data.py(487):             return self[key]\n",
      "forecast.py(222):         return (\n",
      "forecast.py(167):                     self.hparams.target_dim,\n",
      " --- modulename: hparams_mixin, funcname: hparams\n",
      "hparams_mixin.py(161):         if not hasattr(self, \"_hparams\"):\n",
      "hparams_mixin.py(163):         return self._hparams\n",
      " --- modulename: data, funcname: __getattr__\n",
      "data.py(486):         try:\n",
      "data.py(487):             return self[key]\n",
      "forecast.py(164):                 shape=(\n",
      "forecast.py(169):                 dtype=torch.bool,\n",
      "forecast.py(163):             \"past_observed_target\": Input(\n",
      " --- modulename: inputs, funcname: __init__\n",
      "<string>(3): <string>(4): <string>(5): forecast.py(171):             \"past_is_pad\": Input(\n",
      "forecast.py(172):                 shape=(batch_size, self.past_length),\n",
      " --- modulename: forecast, funcname: past_length\n",
      "forecast.py(224):             if self.hparams.patch_size == \"auto\"\n",
      " --- modulename: hparams_mixin, funcname: hparams\n",
      "hparams_mixin.py(161):         if not hasattr(self, \"_hparams\"):\n",
      "hparams_mixin.py(163):         return self._hparams\n",
      " --- modulename: data, funcname: __getattr__\n",
      "data.py(486):         try:\n",
      "data.py(487):             return self[key]\n",
      "forecast.py(225):             else self.hparams.context_length\n",
      " --- modulename: hparams_mixin, funcname: hparams\n",
      "hparams_mixin.py(161):         if not hasattr(self, \"_hparams\"):\n",
      "hparams_mixin.py(163):         return self._hparams\n",
      " --- modulename: data, funcname: __getattr__\n",
      "data.py(486):         try:\n",
      "data.py(487):             return self[key]\n",
      "forecast.py(222):         return (\n",
      "forecast.py(173):                 dtype=torch.bool,\n",
      "forecast.py(171):             \"past_is_pad\": Input(\n",
      " --- modulename: inputs, funcname: __init__\n",
      "<string>(3): <string>(4): <string>(5): forecast.py(154):         data = {\n",
      "forecast.py(176):         if self.hparams.feat_dynamic_real_dim > 0:\n",
      " --- modulename: hparams_mixin, funcname: hparams\n",
      "hparams_mixin.py(161):         if not hasattr(self, \"_hparams\"):\n",
      "hparams_mixin.py(163):         return self._hparams\n",
      " --- modulename: data, funcname: __getattr__\n",
      "data.py(486):         try:\n",
      "data.py(487):             return self[key]\n",
      "forecast.py(193):         if self.hparams.past_feat_dynamic_real_dim > 0:\n",
      " --- modulename: hparams_mixin, funcname: hparams\n",
      "hparams_mixin.py(161):         if not hasattr(self, \"_hparams\"):\n",
      "hparams_mixin.py(163):         return self._hparams\n",
      " --- modulename: data, funcname: __getattr__\n",
      "data.py(486):         try:\n",
      "data.py(487):             return self[key]\n",
      "forecast.py(210):         return InputSpec(data=data, zeros_fn=torch.zeros)\n",
      " --- modulename: inputs, funcname: __init__\n",
      "<string>(3): <string>(4):  --- modulename: __init__, funcname: __iter__\n",
      "__init__.py(1115):         return iter(self.data)\n",
      " --- modulename: __init__, funcname: __len__\n",
      "__init__.py(1099):         return len(self.data)\n",
      "forecast.py(146):             prediction_net=self,\n",
      "forecast.py(147):             batch_size=batch_size,\n",
      "forecast.py(148):             prediction_length=self.hparams.prediction_length,\n",
      " --- modulename: hparams_mixin, funcname: hparams\n",
      "hparams_mixin.py(161):         if not hasattr(self, \"_hparams\"):\n",
      "hparams_mixin.py(163):         return self._hparams\n",
      " --- modulename: data, funcname: __getattr__\n",
      "data.py(486):         try:\n",
      "data.py(487):             return self[key]\n",
      "forecast.py(149):             input_transform=self.get_default_transform() + instance_splitter,\n",
      " --- modulename: forecast, funcname: get_default_transform\n",
      "forecast.py(1019):         transform = AsNumpyArray(\n",
      "forecast.py(1020):             field=\"target\",\n",
      "forecast.py(1021):             expected_ndim=1 if self.hparams.target_dim == 1 else 2,\n",
      " --- modulename: hparams_mixin, funcname: hparams\n",
      "hparams_mixin.py(161):         if not hasattr(self, \"_hparams\"):\n",
      "hparams_mixin.py(163):         return self._hparams\n",
      " --- modulename: data, funcname: __getattr__\n",
      "data.py(486):         try:\n",
      "data.py(487):             return self[key]\n",
      "forecast.py(1022):             dtype=np.float32,\n",
      "forecast.py(1019):         transform = AsNumpyArray(\n",
      " --- modulename: component, funcname: init_wrapper\n",
      "component.py(336):             self, *args = args\n",
      "component.py(338):             nmargs = {\n",
      "component.py(340):                 for (name, param), arg in zip(\n",
      "component.py(341):                     list(init_params.items()), [self] + args\n",
      "component.py(340):                 for (name, param), arg in zip(\n",
      "component.py(338):             nmargs = {\n",
      " --- modulename: component, funcname: <dictcomp>\n",
      "component.py(338):             nmargs = {\n",
      "component.py(340):                 for (name, param), arg in zip(\n",
      "component.py(343):                 if name != \"self\"\n",
      "component.py(338):             nmargs = {\n",
      "component.py(343):                 if name != \"self\"\n",
      "component.py(345):             model = PydanticModel(**{**nmargs, **kwargs})\n",
      " --- modulename: main, funcname: __init__\n",
      "main.py(339):         values, fields_set, validation_error = validate_model(__pydantic_self__.__class__, data)\n",
      " --- modulename: main, funcname: validate_model\n",
      "main.py(1036):     values = {}\n",
      "main.py(1037):     errors = []\n",
      "main.py(1039):     names_used = set()\n",
      "main.py(1041):     fields_set = set()\n",
      "main.py(1042):     config = model.__config__\n",
      "main.py(1043):     check_extra = config.extra is not Extra.ignore\n",
      "main.py(1044):     cls_ = cls or model\n",
      "main.py(1046):     for validator in model.__pre_root_validators__:\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: str_validator\n",
      "validators.py(60):     if isinstance(v, str):\n",
      "validators.py(61):         if isinstance(v, Enum):\n",
      "validators.py(64):             return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: int_validator\n",
      "validators.py(128):     if isinstance(v, int) and not (v is True or v is False):\n",
      "validators.py(129):         return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: any_class_validator\n",
      "validators.py(568):     if isinstance(v, type):\n",
      "validators.py(569):         return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1082):     if check_extra:\n",
      "main.py(1096):     for skip_on_failure, validator in model.__post_root_validators__:\n",
      "main.py(1104):     if errors:\n",
      "main.py(1107):         return values, fields_set, None\n",
      "main.py(340):         if validation_error:\n",
      "main.py(342):         try:\n",
      "main.py(343):             object_setattr(__pydantic_self__, '__dict__', values)\n",
      "main.py(348):         object_setattr(__pydantic_self__, '__fields_set__', fields_set)\n",
      "main.py(349):         __pydantic_self__._init_private_attributes()\n",
      " --- modulename: main, funcname: _init_private_attributes\n",
      "main.py(422):         for name, private_attr in self.__private_attributes__.items():\n",
      "component.py(348):             all_args = {**nmargs, **kwargs, **model.__dict__}\n",
      "component.py(353):             if not getattr(self, \"__init_args__\", {}):\n",
      "component.py(354):                 self.__init_args__ = OrderedDict(\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(355):                     {\n",
      " --- modulename: component, funcname: <dictcomp>\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(354):                 self.__init_args__ = OrderedDict(\n",
      "component.py(361):                 self.__class__.__getnewargs_ex__ = validated_getnewargs_ex\n",
      "component.py(362):                 self.__class__.__repr__ = validated_repr\n",
      "component.py(364):             return init(self, **all_args)\n",
      " --- modulename: convert, funcname: __init__\n",
      "convert.py(132):         self.field = field\n",
      "convert.py(133):         self.expected_ndim = expected_ndim\n",
      "convert.py(134):         self.dtype = dtype\n",
      "forecast.py(1024):         if self.hparams.target_dim == 1:\n",
      " --- modulename: hparams_mixin, funcname: hparams\n",
      "hparams_mixin.py(161):         if not hasattr(self, \"_hparams\"):\n",
      "hparams_mixin.py(163):         return self._hparams\n",
      " --- modulename: data, funcname: __getattr__\n",
      "data.py(486):         try:\n",
      "data.py(487):             return self[key]\n",
      "forecast.py(1026):         transform += AddObservedValuesIndicator(\n",
      "forecast.py(1027):             target_field=\"target\",\n",
      "forecast.py(1028):             output_field=\"observed_target\",\n",
      "forecast.py(1029):             dtype=bool,\n",
      "forecast.py(1026):         transform += AddObservedValuesIndicator(\n",
      " --- modulename: component, funcname: init_wrapper\n",
      "component.py(336):             self, *args = args\n",
      "component.py(338):             nmargs = {\n",
      "component.py(340):                 for (name, param), arg in zip(\n",
      "component.py(341):                     list(init_params.items()), [self] + args\n",
      "component.py(340):                 for (name, param), arg in zip(\n",
      "component.py(338):             nmargs = {\n",
      " --- modulename: component, funcname: <dictcomp>\n",
      "component.py(338):             nmargs = {\n",
      "component.py(340):                 for (name, param), arg in zip(\n",
      "component.py(343):                 if name != \"self\"\n",
      "component.py(338):             nmargs = {\n",
      "component.py(343):                 if name != \"self\"\n",
      "component.py(345):             model = PydanticModel(**{**nmargs, **kwargs})\n",
      " --- modulename: main, funcname: __init__\n",
      "main.py(339):         values, fields_set, validation_error = validate_model(__pydantic_self__.__class__, data)\n",
      " --- modulename: main, funcname: validate_model\n",
      "main.py(1036):     values = {}\n",
      "main.py(1037):     errors = []\n",
      "main.py(1039):     names_used = set()\n",
      "main.py(1041):     fields_set = set()\n",
      "main.py(1042):     config = model.__config__\n",
      "main.py(1043):     check_extra = config.extra is not Extra.ignore\n",
      "main.py(1044):     cls_ = cls or model\n",
      "main.py(1046):     for validator in model.__pre_root_validators__:\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: str_validator\n",
      "validators.py(60):     if isinstance(v, str):\n",
      "validators.py(61):         if isinstance(v, Enum):\n",
      "validators.py(64):             return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: str_validator\n",
      "validators.py(60):     if isinstance(v, str):\n",
      "validators.py(61):         if isinstance(v, Enum):\n",
      "validators.py(64):             return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1060):             if field.required:\n",
      "main.py(1064):             value = field.get_default()\n",
      " --- modulename: fields, funcname: get_default\n",
      "fields.py(437):         return smart_deepcopy(self.default) if self.default_factory is None else self.default_factory()\n",
      " --- modulename: utils, funcname: smart_deepcopy\n",
      "utils.py(682):     obj_type = obj.__class__\n",
      "utils.py(683):     if obj_type in IMMUTABLE_NON_COLLECTIONS_TYPES:\n",
      "utils.py(685):     try:\n",
      "utils.py(686):         if not obj and obj_type in BUILTIN_COLLECTIONS:\n",
      "utils.py(693):     return deepcopy(obj)  # slowest way when we actually might need a deepcopy\n",
      " --- modulename: copy, funcname: deepcopy\n",
      "copy.py(134):     if memo is None:\n",
      "copy.py(135):         memo = {}\n",
      "copy.py(137):     d = id(x)\n",
      "copy.py(138):     y = memo.get(d, _nil)\n",
      "copy.py(139):     if y is not _nil:\n",
      "copy.py(142):     cls = type(x)\n",
      "copy.py(144):     copier = _deepcopy_dispatch.get(cls)\n",
      "copy.py(145):     if copier is not None:\n",
      "copy.py(148):         if issubclass(cls, type):\n",
      "copy.py(151):             copier = getattr(x, \"__deepcopy__\", None)\n",
      "copy.py(152):             if copier is not None:\n",
      "copy.py(155):                 reductor = dispatch_table.get(cls)\n",
      "copy.py(156):                 if reductor:\n",
      "copy.py(159):                     reductor = getattr(x, \"__reduce_ex__\", None)\n",
      "copy.py(160):                     if reductor is not None:\n",
      "copy.py(161):                         rv = reductor(4)\n",
      " --- modulename: component, funcname: validated_getnewargs_ex\n",
      "component.py(332):             return (), self.__init_args__\n",
      "copy.py(169):                 if isinstance(rv, str):\n",
      "copy.py(172):                     y = _reconstruct(x, memo, *rv)\n",
      " --- modulename: copy, funcname: _reconstruct\n",
      "copy.py(262):     deep = memo is not None\n",
      "copy.py(263):     if deep and args:\n",
      "copy.py(264):         args = (deepcopy(arg, memo) for arg in args)\n",
      "copy.py(265):     y = func(*args)\n",
      " --- modulename: copy, funcname: <genexpr>\n",
      "copy.py(264):         args = (deepcopy(arg, memo) for arg in args)\n",
      " --- modulename: copy, funcname: deepcopy\n",
      "copy.py(134):     if memo is None:\n",
      "copy.py(137):     d = id(x)\n",
      "copy.py(138):     y = memo.get(d, _nil)\n",
      "copy.py(139):     if y is not _nil:\n",
      "copy.py(142):     cls = type(x)\n",
      "copy.py(144):     copier = _deepcopy_dispatch.get(cls)\n",
      "copy.py(145):     if copier is not None:\n",
      "copy.py(146):         y = copier(x, memo)\n",
      " --- modulename: copy, funcname: _deepcopy_atomic\n",
      "copy.py(183):     return x\n",
      "copy.py(175):     if y is not x:\n",
      "copy.py(178):     return y\n",
      " --- modulename: copy, funcname: <genexpr>\n",
      "copy.py(264):         args = (deepcopy(arg, memo) for arg in args)\n",
      " --- modulename: copy, funcname: deepcopy\n",
      "copy.py(134):     if memo is None:\n",
      "copy.py(137):     d = id(x)\n",
      "copy.py(138):     y = memo.get(d, _nil)\n",
      "copy.py(139):     if y is not _nil:\n",
      "copy.py(142):     cls = type(x)\n",
      "copy.py(144):     copier = _deepcopy_dispatch.get(cls)\n",
      "copy.py(145):     if copier is not None:\n",
      "copy.py(146):         y = copier(x, memo)\n",
      " --- modulename: copy, funcname: _deepcopy_tuple\n",
      "copy.py(211):     y = [deepcopy(a, memo) for a in x]\n",
      " --- modulename: copy, funcname: <listcomp>\n",
      "copy.py(211):     y = [deepcopy(a, memo) for a in x]\n",
      "copy.py(214):     try:\n",
      "copy.py(215):         return memo[id(x)]\n",
      "copy.py(216):     except KeyError:\n",
      "copy.py(217):         pass\n",
      "copy.py(218):     for k, j in zip(x, y):\n",
      "copy.py(223):         y = x\n",
      "copy.py(224):     return y\n",
      "copy.py(175):     if y is not x:\n",
      "copy.py(178):     return y\n",
      " --- modulename: copy, funcname: <genexpr>\n",
      "copy.py(264):         args = (deepcopy(arg, memo) for arg in args)\n",
      " --- modulename: copy, funcname: deepcopy\n",
      "copy.py(134):     if memo is None:\n",
      "copy.py(137):     d = id(x)\n",
      "copy.py(138):     y = memo.get(d, _nil)\n",
      "copy.py(139):     if y is not _nil:\n",
      "copy.py(142):     cls = type(x)\n",
      "copy.py(144):     copier = _deepcopy_dispatch.get(cls)\n",
      "copy.py(145):     if copier is not None:\n",
      "copy.py(148):         if issubclass(cls, type):\n",
      "copy.py(151):             copier = getattr(x, \"__deepcopy__\", None)\n",
      "copy.py(152):             if copier is not None:\n",
      "copy.py(155):                 reductor = dispatch_table.get(cls)\n",
      "copy.py(156):                 if reductor:\n",
      "copy.py(159):                     reductor = getattr(x, \"__reduce_ex__\", None)\n",
      "copy.py(160):                     if reductor is not None:\n",
      "copy.py(161):                         rv = reductor(4)\n",
      "copy.py(169):                 if isinstance(rv, str):\n",
      "copy.py(172):                     y = _reconstruct(x, memo, *rv)\n",
      " --- modulename: copy, funcname: _reconstruct\n",
      "copy.py(262):     deep = memo is not None\n",
      "copy.py(263):     if deep and args:\n",
      "copy.py(265):     y = func(*args)\n",
      "copy.py(266):     if deep:\n",
      "copy.py(267):         memo[id(x)] = y\n",
      "copy.py(269):     if state is not None:\n",
      "copy.py(285):     if listiter is not None:\n",
      "copy.py(293):     if dictiter is not None:\n",
      "copy.py(294):         if deep:\n",
      "copy.py(295):             for key, value in dictiter:\n",
      "copy.py(296):                 key = deepcopy(key, memo)\n",
      " --- modulename: copy, funcname: deepcopy\n",
      "copy.py(134):     if memo is None:\n",
      "copy.py(137):     d = id(x)\n",
      "copy.py(138):     y = memo.get(d, _nil)\n",
      "copy.py(139):     if y is not _nil:\n",
      "copy.py(142):     cls = type(x)\n",
      "copy.py(144):     copier = _deepcopy_dispatch.get(cls)\n",
      "copy.py(145):     if copier is not None:\n",
      "copy.py(146):         y = copier(x, memo)\n",
      " --- modulename: copy, funcname: _deepcopy_atomic\n",
      "copy.py(183):     return x\n",
      "copy.py(175):     if y is not x:\n",
      "copy.py(178):     return y\n",
      "copy.py(297):                 value = deepcopy(value, memo)\n",
      " --- modulename: copy, funcname: deepcopy\n",
      "copy.py(134):     if memo is None:\n",
      "copy.py(137):     d = id(x)\n",
      "copy.py(138):     y = memo.get(d, _nil)\n",
      "copy.py(139):     if y is not _nil:\n",
      "copy.py(142):     cls = type(x)\n",
      "copy.py(144):     copier = _deepcopy_dispatch.get(cls)\n",
      "copy.py(145):     if copier is not None:\n",
      "copy.py(146):         y = copier(x, memo)\n",
      " --- modulename: copy, funcname: _deepcopy_atomic\n",
      "copy.py(183):     return x\n",
      "copy.py(175):     if y is not x:\n",
      "copy.py(178):     return y\n",
      "copy.py(298):                 y[key] = value\n",
      "copy.py(295):             for key, value in dictiter:\n",
      "copy.py(302):     return y\n",
      "copy.py(175):     if y is not x:\n",
      "copy.py(176):         memo[d] = y\n",
      "copy.py(177):         _keep_alive(x, memo) # Make sure x lives at least as long as d\n",
      " --- modulename: copy, funcname: _keep_alive\n",
      "copy.py(253):     try:\n",
      "copy.py(254):         memo[id(memo)].append(x)\n",
      "copy.py(255):     except KeyError:\n",
      "copy.py(257):         memo[id(memo)]=[x]\n",
      "copy.py(178):     return y\n",
      " --- modulename: copy, funcname: <genexpr>\n",
      "copy.py(264):         args = (deepcopy(arg, memo) for arg in args)\n",
      " --- modulename: copyreg, funcname: __newobj_ex__\n",
      "copyreg.py(107):     return cls.__new__(cls, *args, **kwargs)\n",
      "copy.py(266):     if deep:\n",
      "copy.py(267):         memo[id(x)] = y\n",
      "copy.py(269):     if state is not None:\n",
      "copy.py(270):         if deep:\n",
      "copy.py(271):             state = deepcopy(state, memo)\n",
      " --- modulename: copy, funcname: deepcopy\n",
      "copy.py(134):     if memo is None:\n",
      "copy.py(137):     d = id(x)\n",
      "copy.py(138):     y = memo.get(d, _nil)\n",
      "copy.py(139):     if y is not _nil:\n",
      "copy.py(142):     cls = type(x)\n",
      "copy.py(144):     copier = _deepcopy_dispatch.get(cls)\n",
      "copy.py(145):     if copier is not None:\n",
      "copy.py(146):         y = copier(x, memo)\n",
      " --- modulename: copy, funcname: _deepcopy_dict\n",
      "copy.py(228):     y = {}\n",
      "copy.py(229):     memo[id(x)] = y\n",
      "copy.py(230):     for key, value in x.items():\n",
      "copy.py(231):         y[deepcopy(key, memo)] = deepcopy(value, memo)\n",
      " --- modulename: copy, funcname: deepcopy\n",
      "copy.py(134):     if memo is None:\n",
      "copy.py(137):     d = id(x)\n",
      "copy.py(138):     y = memo.get(d, _nil)\n",
      "copy.py(139):     if y is not _nil:\n",
      "copy.py(140):         return y\n",
      " --- modulename: copy, funcname: deepcopy\n",
      "copy.py(134):     if memo is None:\n",
      "copy.py(137):     d = id(x)\n",
      "copy.py(138):     y = memo.get(d, _nil)\n",
      "copy.py(139):     if y is not _nil:\n",
      "copy.py(142):     cls = type(x)\n",
      "copy.py(144):     copier = _deepcopy_dispatch.get(cls)\n",
      "copy.py(145):     if copier is not None:\n",
      "copy.py(146):         y = copier(x, memo)\n",
      " --- modulename: copy, funcname: _deepcopy_atomic\n",
      "copy.py(183):     return x\n",
      "copy.py(175):     if y is not x:\n",
      "copy.py(178):     return y\n",
      "copy.py(230):     for key, value in x.items():\n",
      "copy.py(231):         y[deepcopy(key, memo)] = deepcopy(value, memo)\n",
      " --- modulename: copy, funcname: deepcopy\n",
      "copy.py(134):     if memo is None:\n",
      "copy.py(137):     d = id(x)\n",
      "copy.py(138):     y = memo.get(d, _nil)\n",
      "copy.py(139):     if y is not _nil:\n",
      "copy.py(142):     cls = type(x)\n",
      "copy.py(144):     copier = _deepcopy_dispatch.get(cls)\n",
      "copy.py(145):     if copier is not None:\n",
      "copy.py(146):         y = copier(x, memo)\n",
      " --- modulename: copy, funcname: _deepcopy_atomic\n",
      "copy.py(183):     return x\n",
      "copy.py(175):     if y is not x:\n",
      "copy.py(178):     return y\n",
      " --- modulename: copy, funcname: deepcopy\n",
      "copy.py(134):     if memo is None:\n",
      "copy.py(137):     d = id(x)\n",
      "copy.py(138):     y = memo.get(d, _nil)\n",
      "copy.py(139):     if y is not _nil:\n",
      "copy.py(142):     cls = type(x)\n",
      "copy.py(144):     copier = _deepcopy_dispatch.get(cls)\n",
      "copy.py(145):     if copier is not None:\n",
      "copy.py(146):         y = copier(x, memo)\n",
      " --- modulename: copy, funcname: _deepcopy_atomic\n",
      "copy.py(183):     return x\n",
      "copy.py(175):     if y is not x:\n",
      "copy.py(178):     return y\n",
      "copy.py(230):     for key, value in x.items():\n",
      "copy.py(232):     return y\n",
      "copy.py(175):     if y is not x:\n",
      "copy.py(176):         memo[d] = y\n",
      "copy.py(177):         _keep_alive(x, memo) # Make sure x lives at least as long as d\n",
      " --- modulename: copy, funcname: _keep_alive\n",
      "copy.py(253):     try:\n",
      "copy.py(254):         memo[id(memo)].append(x)\n",
      "copy.py(178):     return y\n",
      "copy.py(272):         if hasattr(y, '__setstate__'):\n",
      "copy.py(275):             if isinstance(state, tuple) and len(state) == 2:\n",
      "copy.py(278):                 slotstate = None\n",
      "copy.py(279):             if state is not None:\n",
      "copy.py(280):                 y.__dict__.update(state)\n",
      "copy.py(281):             if slotstate is not None:\n",
      "copy.py(285):     if listiter is not None:\n",
      "copy.py(293):     if dictiter is not None:\n",
      "copy.py(302):     return y\n",
      "copy.py(175):     if y is not x:\n",
      "copy.py(176):         memo[d] = y\n",
      "copy.py(177):         _keep_alive(x, memo) # Make sure x lives at least as long as d\n",
      " --- modulename: copy, funcname: _keep_alive\n",
      "copy.py(253):     try:\n",
      "copy.py(254):         memo[id(memo)].append(x)\n",
      "copy.py(178):     return y\n",
      "main.py(1066):             if not config.validate_all and not field.validate_always:\n",
      "main.py(1067):                 values[name] = value\n",
      "main.py(1068):                 continue\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: any_class_validator\n",
      "validators.py(568):     if isinstance(v, type):\n",
      "validators.py(569):         return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1082):     if check_extra:\n",
      "main.py(1096):     for skip_on_failure, validator in model.__post_root_validators__:\n",
      "main.py(1104):     if errors:\n",
      "main.py(1107):         return values, fields_set, None\n",
      "main.py(340):         if validation_error:\n",
      "main.py(342):         try:\n",
      "main.py(343):             object_setattr(__pydantic_self__, '__dict__', values)\n",
      "main.py(348):         object_setattr(__pydantic_self__, '__fields_set__', fields_set)\n",
      "main.py(349):         __pydantic_self__._init_private_attributes()\n",
      " --- modulename: main, funcname: _init_private_attributes\n",
      "main.py(422):         for name, private_attr in self.__private_attributes__.items():\n",
      "component.py(348):             all_args = {**nmargs, **kwargs, **model.__dict__}\n",
      "component.py(353):             if not getattr(self, \"__init_args__\", {}):\n",
      "component.py(354):                 self.__init_args__ = OrderedDict(\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(355):                     {\n",
      " --- modulename: component, funcname: <dictcomp>\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(354):                 self.__init_args__ = OrderedDict(\n",
      "component.py(361):                 self.__class__.__getnewargs_ex__ = validated_getnewargs_ex\n",
      "component.py(362):                 self.__class__.__repr__ = validated_repr\n",
      "component.py(364):             return init(self, **all_args)\n",
      " --- modulename: feature, funcname: __init__\n",
      "feature.py(244):         self.target_field = target_field\n",
      "feature.py(245):         self.output_field = output_field\n",
      "feature.py(246):         self.dtype = dtype\n",
      "feature.py(247):         self.imputation_method = imputation_method\n",
      " --- modulename: _base, funcname: __add__\n",
      "_base.py(40):         return self.chain(other)\n",
      " --- modulename: _base, funcname: chain\n",
      "_base.py(37):         return Chain([self, other])\n",
      " --- modulename: _base, funcname: __init__\n",
      "<string>(3): <string>(4):  --- modulename: _base, funcname: __post_init__\n",
      "_base.py(57):         transformations = []\n",
      "_base.py(59):         for transformation in self.transformations:\n",
      "_base.py(60):             if isinstance(transformation, Identity):\n",
      " --- modulename: abc, funcname: __instancecheck__\n",
      "abc.py(119):             return _abc_instancecheck(cls, instance)\n",
      "_base.py(62):             elif isinstance(transformation, Chain):\n",
      " --- modulename: abc, funcname: __instancecheck__\n",
      "abc.py(119):             return _abc_instancecheck(cls, instance)\n",
      "_base.py(65):                 assert isinstance(transformation, Transformation)\n",
      " --- modulename: abc, funcname: __instancecheck__\n",
      "abc.py(119):             return _abc_instancecheck(cls, instance)\n",
      "_base.py(66):                 transformations.append(transformation)\n",
      "_base.py(59):         for transformation in self.transformations:\n",
      "_base.py(60):             if isinstance(transformation, Identity):\n",
      " --- modulename: abc, funcname: __instancecheck__\n",
      "abc.py(119):             return _abc_instancecheck(cls, instance)\n",
      "_base.py(62):             elif isinstance(transformation, Chain):\n",
      " --- modulename: abc, funcname: __instancecheck__\n",
      "abc.py(119):             return _abc_instancecheck(cls, instance)\n",
      "_base.py(65):                 assert isinstance(transformation, Transformation)\n",
      " --- modulename: abc, funcname: __instancecheck__\n",
      "abc.py(119):             return _abc_instancecheck(cls, instance)\n",
      "_base.py(66):                 transformations.append(transformation)\n",
      "_base.py(59):         for transformation in self.transformations:\n",
      "_base.py(68):         self.transformations = transformations\n",
      "_base.py(69):         self.__init_passed_kwargs__ = {\"transformations\": transformations}\n",
      "forecast.py(1032):         if self.hparams.feat_dynamic_real_dim > 0:\n",
      " --- modulename: hparams_mixin, funcname: hparams\n",
      "hparams_mixin.py(161):         if not hasattr(self, \"_hparams\"):\n",
      "hparams_mixin.py(163):         return self._hparams\n",
      " --- modulename: data, funcname: __getattr__\n",
      "data.py(486):         try:\n",
      "data.py(487):             return self[key]\n",
      "forecast.py(1044):         if self.hparams.past_feat_dynamic_real_dim > 0:\n",
      " --- modulename: hparams_mixin, funcname: hparams\n",
      "hparams_mixin.py(161):         if not hasattr(self, \"_hparams\"):\n",
      "hparams_mixin.py(163):         return self._hparams\n",
      " --- modulename: data, funcname: __getattr__\n",
      "data.py(486):         try:\n",
      "data.py(487):             return self[key]\n",
      "forecast.py(1055):         return transform\n",
      " --- modulename: _base, funcname: __add__\n",
      "_base.py(40):         return self.chain(other)\n",
      " --- modulename: _base, funcname: chain\n",
      "_base.py(37):         return Chain([self, other])\n",
      " --- modulename: _base, funcname: __init__\n",
      "<string>(3): <string>(4):  --- modulename: _base, funcname: __post_init__\n",
      "_base.py(57):         transformations = []\n",
      "_base.py(59):         for transformation in self.transformations:\n",
      "_base.py(60):             if isinstance(transformation, Identity):\n",
      " --- modulename: abc, funcname: __instancecheck__\n",
      "abc.py(119):             return _abc_instancecheck(cls, instance)\n",
      "_base.py(62):             elif isinstance(transformation, Chain):\n",
      "_base.py(63):                 transformations.extend(transformation.transformations)\n",
      "_base.py(59):         for transformation in self.transformations:\n",
      "_base.py(60):             if isinstance(transformation, Identity):\n",
      " --- modulename: abc, funcname: __instancecheck__\n",
      "abc.py(119):             return _abc_instancecheck(cls, instance)\n",
      "_base.py(62):             elif isinstance(transformation, Chain):\n",
      " --- modulename: abc, funcname: __instancecheck__\n",
      "abc.py(119):             return _abc_instancecheck(cls, instance)\n",
      "_base.py(65):                 assert isinstance(transformation, Transformation)\n",
      " --- modulename: abc, funcname: __instancecheck__\n",
      "abc.py(119):             return _abc_instancecheck(cls, instance)\n",
      "_base.py(66):                 transformations.append(transformation)\n",
      "_base.py(59):         for transformation in self.transformations:\n",
      "_base.py(68):         self.transformations = transformations\n",
      "_base.py(69):         self.__init_passed_kwargs__ = {\"transformations\": transformations}\n",
      "forecast.py(150):             device=device,\n",
      "forecast.py(144):         return PyTorchPredictor(\n",
      " --- modulename: component, funcname: init_wrapper\n",
      "component.py(336):             self, *args = args\n",
      "component.py(338):             nmargs = {\n",
      "component.py(340):                 for (name, param), arg in zip(\n",
      "component.py(341):                     list(init_params.items()), [self] + args\n",
      "component.py(340):                 for (name, param), arg in zip(\n",
      "component.py(338):             nmargs = {\n",
      " --- modulename: component, funcname: <dictcomp>\n",
      "component.py(338):             nmargs = {\n",
      "component.py(340):                 for (name, param), arg in zip(\n",
      "component.py(343):                 if name != \"self\"\n",
      "component.py(338):             nmargs = {\n",
      "component.py(343):                 if name != \"self\"\n",
      "component.py(345):             model = PydanticModel(**{**nmargs, **kwargs})\n",
      " --- modulename: main, funcname: __init__\n",
      "main.py(339):         values, fields_set, validation_error = validate_model(__pydantic_self__.__class__, data)\n",
      " --- modulename: main, funcname: validate_model\n",
      "main.py(1036):     values = {}\n",
      "main.py(1037):     errors = []\n",
      "main.py(1039):     names_used = set()\n",
      "main.py(1041):     fields_set = set()\n",
      "main.py(1042):     config = model.__config__\n",
      "main.py(1043):     check_extra = config.extra is not Extra.ignore\n",
      "main.py(1044):     cls_ = cls or model\n",
      "main.py(1046):     for validator in model.__pre_root_validators__:\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(882):         elif self.shape in MAPPING_LIKE_SHAPES:\n",
      "fields.py(884):         elif self.shape == SHAPE_TUPLE:\n",
      "fields.py(886):         elif self.shape == SHAPE_ITERABLE:\n",
      "fields.py(888):         elif self.shape == SHAPE_GENERIC:\n",
      "fields.py(892):             v, errors = self._validate_sequence_like(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_sequence_like\n",
      "fields.py(906):         if not sequence_like(v):\n",
      " --- modulename: utils, funcname: sequence_like\n",
      "utils.py(158):     return isinstance(v, (list, tuple, set, frozenset, GeneratorType, deque))\n",
      "fields.py(920):         loc = loc if isinstance(loc, tuple) else (loc,)\n",
      "fields.py(921):         result = []\n",
      "fields.py(922):         errors: List[ErrorList] = []\n",
      "fields.py(923):         for i, v_ in enumerate(v):\n",
      "fields.py(924):             v_loc = *loc, i\n",
      "fields.py(925):             r, ee = self._validate_singleton(v_, values, v_loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1057):             if self.discriminator_key is not None:\n",
      "fields.py(1060):             errors = []\n",
      "fields.py(1062):             if self.model_config.smart_union and is_union(get_origin(self.type_)):\n",
      "fields.py(1090):             for field in self.sub_fields:\n",
      "fields.py(1091):                 value, error = field.validate(v, values, loc=loc, cls=cls)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: str_validator\n",
      "validators.py(60):     if isinstance(v, str):\n",
      "validators.py(61):         if isinstance(v, Enum):\n",
      "validators.py(64):             return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "fields.py(1092):                 if error:\n",
      "fields.py(1095):                     return value, None\n",
      "fields.py(926):             if ee:\n",
      "fields.py(929):                 result.append(r)\n",
      "fields.py(923):         for i, v_ in enumerate(v):\n",
      "fields.py(924):             v_loc = *loc, i\n",
      "fields.py(925):             r, ee = self._validate_singleton(v_, values, v_loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1057):             if self.discriminator_key is not None:\n",
      "fields.py(1060):             errors = []\n",
      "fields.py(1062):             if self.model_config.smart_union and is_union(get_origin(self.type_)):\n",
      "fields.py(1090):             for field in self.sub_fields:\n",
      "fields.py(1091):                 value, error = field.validate(v, values, loc=loc, cls=cls)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: str_validator\n",
      "validators.py(60):     if isinstance(v, str):\n",
      "validators.py(61):         if isinstance(v, Enum):\n",
      "validators.py(64):             return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "fields.py(1092):                 if error:\n",
      "fields.py(1095):                     return value, None\n",
      "fields.py(926):             if ee:\n",
      "fields.py(929):                 result.append(r)\n",
      "fields.py(923):         for i, v_ in enumerate(v):\n",
      "fields.py(924):             v_loc = *loc, i\n",
      "fields.py(925):             r, ee = self._validate_singleton(v_, values, v_loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1057):             if self.discriminator_key is not None:\n",
      "fields.py(1060):             errors = []\n",
      "fields.py(1062):             if self.model_config.smart_union and is_union(get_origin(self.type_)):\n",
      "fields.py(1090):             for field in self.sub_fields:\n",
      "fields.py(1091):                 value, error = field.validate(v, values, loc=loc, cls=cls)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: str_validator\n",
      "validators.py(60):     if isinstance(v, str):\n",
      "validators.py(61):         if isinstance(v, Enum):\n",
      "validators.py(64):             return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "fields.py(1092):                 if error:\n",
      "fields.py(1095):                     return value, None\n",
      "fields.py(926):             if ee:\n",
      "fields.py(929):                 result.append(r)\n",
      "fields.py(923):         for i, v_ in enumerate(v):\n",
      "fields.py(931):         if errors:\n",
      "fields.py(934):         converted: Union[List[Any], Set[Any], FrozenSet[Any], Tuple[Any, ...], Iterator[Any], Deque[Any]] = result\n",
      "fields.py(936):         if self.shape == SHAPE_SET:\n",
      "fields.py(938):         elif self.shape == SHAPE_FROZENSET:\n",
      "fields.py(940):         elif self.shape == SHAPE_TUPLE_ELLIPSIS:\n",
      "fields.py(942):         elif self.shape == SHAPE_DEQUE:\n",
      "fields.py(944):         elif self.shape == SHAPE_SEQUENCE:\n",
      "fields.py(953):         return converted, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: arbitrary_type_validator\n",
      "validators.py(551):         if isinstance(v, type_):\n",
      "validators.py(552):             return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: int_validator\n",
      "validators.py(128):     if isinstance(v, int) and not (v is True or v is False):\n",
      "validators.py(129):         return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: int_validator\n",
      "validators.py(128):     if isinstance(v, int) and not (v is True or v is False):\n",
      "validators.py(129):         return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: arbitrary_type_validator\n",
      "validators.py(551):         if isinstance(v, type_):\n",
      " --- modulename: abc, funcname: __instancecheck__\n",
      "abc.py(119):             return _abc_instancecheck(cls, instance)\n",
      "validators.py(552):             return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1060):             if field.required:\n",
      "main.py(1064):             value = field.get_default()\n",
      " --- modulename: fields, funcname: get_default\n",
      "fields.py(437):         return smart_deepcopy(self.default) if self.default_factory is None else self.default_factory()\n",
      " --- modulename: utils, funcname: smart_deepcopy\n",
      "utils.py(682):     obj_type = obj.__class__\n",
      "utils.py(683):     if obj_type in IMMUTABLE_NON_COLLECTIONS_TYPES:\n",
      "utils.py(685):     try:\n",
      "utils.py(686):         if not obj and obj_type in BUILTIN_COLLECTIONS:\n",
      "utils.py(693):     return deepcopy(obj)  # slowest way when we actually might need a deepcopy\n",
      " --- modulename: copy, funcname: deepcopy\n",
      "copy.py(134):     if memo is None:\n",
      "copy.py(135):         memo = {}\n",
      "copy.py(137):     d = id(x)\n",
      "copy.py(138):     y = memo.get(d, _nil)\n",
      "copy.py(139):     if y is not _nil:\n",
      "copy.py(142):     cls = type(x)\n",
      "copy.py(144):     copier = _deepcopy_dispatch.get(cls)\n",
      "copy.py(145):     if copier is not None:\n",
      "copy.py(148):         if issubclass(cls, type):\n",
      "copy.py(151):             copier = getattr(x, \"__deepcopy__\", None)\n",
      "copy.py(152):             if copier is not None:\n",
      "copy.py(155):                 reductor = dispatch_table.get(cls)\n",
      "copy.py(156):                 if reductor:\n",
      "copy.py(159):                     reductor = getattr(x, \"__reduce_ex__\", None)\n",
      "copy.py(160):                     if reductor is not None:\n",
      "copy.py(161):                         rv = reductor(4)\n",
      " --- modulename: component, funcname: validated_getnewargs_ex\n",
      "component.py(332):             return (), self.__init_args__\n",
      "copy.py(169):                 if isinstance(rv, str):\n",
      "copy.py(172):                     y = _reconstruct(x, memo, *rv)\n",
      " --- modulename: copy, funcname: _reconstruct\n",
      "copy.py(262):     deep = memo is not None\n",
      "copy.py(263):     if deep and args:\n",
      "copy.py(264):         args = (deepcopy(arg, memo) for arg in args)\n",
      "copy.py(265):     y = func(*args)\n",
      " --- modulename: copy, funcname: <genexpr>\n",
      "copy.py(264):         args = (deepcopy(arg, memo) for arg in args)\n",
      " --- modulename: copy, funcname: deepcopy\n",
      "copy.py(134):     if memo is None:\n",
      "copy.py(137):     d = id(x)\n",
      "copy.py(138):     y = memo.get(d, _nil)\n",
      "copy.py(139):     if y is not _nil:\n",
      "copy.py(142):     cls = type(x)\n",
      "copy.py(144):     copier = _deepcopy_dispatch.get(cls)\n",
      "copy.py(145):     if copier is not None:\n",
      "copy.py(146):         y = copier(x, memo)\n",
      " --- modulename: copy, funcname: _deepcopy_atomic\n",
      "copy.py(183):     return x\n",
      "copy.py(175):     if y is not x:\n",
      "copy.py(178):     return y\n",
      " --- modulename: copy, funcname: <genexpr>\n",
      "copy.py(264):         args = (deepcopy(arg, memo) for arg in args)\n",
      " --- modulename: copyreg, funcname: __newobj__\n",
      "copyreg.py(101):     return cls.__new__(cls, *args)\n",
      "copy.py(266):     if deep:\n",
      "copy.py(267):         memo[id(x)] = y\n",
      "copy.py(269):     if state is not None:\n",
      "copy.py(270):         if deep:\n",
      "copy.py(271):             state = deepcopy(state, memo)\n",
      " --- modulename: copy, funcname: deepcopy\n",
      "copy.py(134):     if memo is None:\n",
      "copy.py(137):     d = id(x)\n",
      "copy.py(138):     y = memo.get(d, _nil)\n",
      "copy.py(139):     if y is not _nil:\n",
      "copy.py(142):     cls = type(x)\n",
      "copy.py(144):     copier = _deepcopy_dispatch.get(cls)\n",
      "copy.py(145):     if copier is not None:\n",
      "copy.py(146):         y = copier(x, memo)\n",
      " --- modulename: copy, funcname: _deepcopy_dict\n",
      "copy.py(228):     y = {}\n",
      "copy.py(229):     memo[id(x)] = y\n",
      "copy.py(230):     for key, value in x.items():\n",
      "copy.py(231):         y[deepcopy(key, memo)] = deepcopy(value, memo)\n",
      " --- modulename: copy, funcname: deepcopy\n",
      "copy.py(134):     if memo is None:\n",
      "copy.py(137):     d = id(x)\n",
      "copy.py(138):     y = memo.get(d, _nil)\n",
      "copy.py(139):     if y is not _nil:\n",
      "copy.py(142):     cls = type(x)\n",
      "copy.py(144):     copier = _deepcopy_dispatch.get(cls)\n",
      "copy.py(145):     if copier is not None:\n",
      "copy.py(148):         if issubclass(cls, type):\n",
      "copy.py(151):             copier = getattr(x, \"__deepcopy__\", None)\n",
      "copy.py(152):             if copier is not None:\n",
      "copy.py(155):                 reductor = dispatch_table.get(cls)\n",
      "copy.py(156):                 if reductor:\n",
      "copy.py(159):                     reductor = getattr(x, \"__reduce_ex__\", None)\n",
      "copy.py(160):                     if reductor is not None:\n",
      "copy.py(161):                         rv = reductor(4)\n",
      "copy.py(169):                 if isinstance(rv, str):\n",
      "copy.py(172):                     y = _reconstruct(x, memo, *rv)\n",
      " --- modulename: copy, funcname: _reconstruct\n",
      "copy.py(262):     deep = memo is not None\n",
      "copy.py(263):     if deep and args:\n",
      "copy.py(265):     y = func(*args)\n",
      "copy.py(266):     if deep:\n",
      "copy.py(267):         memo[id(x)] = y\n",
      "copy.py(269):     if state is not None:\n",
      "copy.py(285):     if listiter is not None:\n",
      "copy.py(293):     if dictiter is not None:\n",
      "copy.py(294):         if deep:\n",
      "copy.py(295):             for key, value in dictiter:\n",
      "copy.py(302):     return y\n",
      "copy.py(175):     if y is not x:\n",
      "copy.py(176):         memo[d] = y\n",
      "copy.py(177):         _keep_alive(x, memo) # Make sure x lives at least as long as d\n",
      " --- modulename: copy, funcname: _keep_alive\n",
      "copy.py(253):     try:\n",
      "copy.py(254):         memo[id(memo)].append(x)\n",
      "copy.py(255):     except KeyError:\n",
      "copy.py(257):         memo[id(memo)]=[x]\n",
      "copy.py(178):     return y\n",
      " --- modulename: copy, funcname: deepcopy\n",
      "copy.py(134):     if memo is None:\n",
      "copy.py(137):     d = id(x)\n",
      "copy.py(138):     y = memo.get(d, _nil)\n",
      "copy.py(139):     if y is not _nil:\n",
      "copy.py(142):     cls = type(x)\n",
      "copy.py(144):     copier = _deepcopy_dispatch.get(cls)\n",
      "copy.py(145):     if copier is not None:\n",
      "copy.py(146):         y = copier(x, memo)\n",
      " --- modulename: copy, funcname: _deepcopy_atomic\n",
      "copy.py(183):     return x\n",
      "copy.py(175):     if y is not x:\n",
      "copy.py(178):     return y\n",
      "copy.py(230):     for key, value in x.items():\n",
      "copy.py(232):     return y\n",
      "copy.py(175):     if y is not x:\n",
      "copy.py(176):         memo[d] = y\n",
      "copy.py(177):         _keep_alive(x, memo) # Make sure x lives at least as long as d\n",
      " --- modulename: copy, funcname: _keep_alive\n",
      "copy.py(253):     try:\n",
      "copy.py(254):         memo[id(memo)].append(x)\n",
      "copy.py(178):     return y\n",
      "copy.py(272):         if hasattr(y, '__setstate__'):\n",
      "copy.py(275):             if isinstance(state, tuple) and len(state) == 2:\n",
      "copy.py(278):                 slotstate = None\n",
      "copy.py(279):             if state is not None:\n",
      "copy.py(280):                 y.__dict__.update(state)\n",
      "copy.py(281):             if slotstate is not None:\n",
      "copy.py(285):     if listiter is not None:\n",
      "copy.py(293):     if dictiter is not None:\n",
      "copy.py(302):     return y\n",
      "copy.py(175):     if y is not x:\n",
      "copy.py(176):         memo[d] = y\n",
      "copy.py(177):         _keep_alive(x, memo) # Make sure x lives at least as long as d\n",
      " --- modulename: copy, funcname: _keep_alive\n",
      "copy.py(253):     try:\n",
      "copy.py(254):         memo[id(memo)].append(x)\n",
      "copy.py(178):     return y\n",
      "main.py(1066):             if not config.validate_all and not field.validate_always:\n",
      "main.py(1067):                 values[name] = value\n",
      "main.py(1068):                 continue\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1060):             if field.required:\n",
      "main.py(1064):             value = field.get_default()\n",
      " --- modulename: fields, funcname: get_default\n",
      "fields.py(437):         return smart_deepcopy(self.default) if self.default_factory is None else self.default_factory()\n",
      " --- modulename: utils, funcname: smart_deepcopy\n",
      "utils.py(682):     obj_type = obj.__class__\n",
      "utils.py(683):     if obj_type in IMMUTABLE_NON_COLLECTIONS_TYPES:\n",
      "utils.py(684):         return obj  # fastest case: obj is immutable and not collection therefore will not be copied anyway\n",
      "main.py(1066):             if not config.validate_all and not field.validate_always:\n",
      "main.py(1067):                 values[name] = value\n",
      "main.py(1068):                 continue\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1060):             if field.required:\n",
      "main.py(1064):             value = field.get_default()\n",
      " --- modulename: fields, funcname: get_default\n",
      "fields.py(437):         return smart_deepcopy(self.default) if self.default_factory is None else self.default_factory()\n",
      " --- modulename: utils, funcname: smart_deepcopy\n",
      "utils.py(682):     obj_type = obj.__class__\n",
      "utils.py(683):     if obj_type in IMMUTABLE_NON_COLLECTIONS_TYPES:\n",
      "utils.py(684):         return obj  # fastest case: obj is immutable and not collection therefore will not be copied anyway\n",
      "main.py(1066):             if not config.validate_all and not field.validate_always:\n",
      "main.py(1067):                 values[name] = value\n",
      "main.py(1068):                 continue\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1053):         value = input_data.get(field.alias, _missing)\n",
      "main.py(1054):         using_name = False\n",
      "main.py(1055):         if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n",
      "main.py(1059):         if value is _missing:\n",
      "main.py(1070):             fields_set.add(name)\n",
      "main.py(1071):             if check_extra:\n",
      "main.py(1074):         v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1057):             if self.discriminator_key is not None:\n",
      "fields.py(1060):             errors = []\n",
      "fields.py(1062):             if self.model_config.smart_union and is_union(get_origin(self.type_)):\n",
      "fields.py(1090):             for field in self.sub_fields:\n",
      "fields.py(1091):                 value, error = field.validate(v, values, loc=loc, cls=cls)\n",
      " --- modulename: fields, funcname: validate\n",
      "fields.py(853):         assert self.type_.__class__ is not DeferredType\n",
      "fields.py(855):         if self.type_.__class__ is ForwardRef:\n",
      "fields.py(863):         if self.pre_validators:\n",
      "fields.py(868):         if v is None:\n",
      "fields.py(880):         if self.shape == SHAPE_SINGLETON:\n",
      "fields.py(881):             v, errors = self._validate_singleton(v, values, loc, cls)\n",
      " --- modulename: fields, funcname: _validate_singleton\n",
      "fields.py(1056):         if self.sub_fields:\n",
      "fields.py(1098):             return self._apply_validators(v, values, loc, cls, self.validators)\n",
      " --- modulename: fields, funcname: _apply_validators\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1153):             try:\n",
      "fields.py(1154):                 v = validator(cls, v, values, self, self.model_config)\n",
      " --- modulename: class_validators, funcname: <lambda>\n",
      "class_validators.py(337):         return lambda cls, v, values, field, config: validator(v)\n",
      " --- modulename: validators, funcname: str_validator\n",
      "validators.py(60):     if isinstance(v, str):\n",
      "validators.py(61):         if isinstance(v, Enum):\n",
      "validators.py(64):             return v\n",
      "fields.py(1152):         for validator in validators:\n",
      "fields.py(1157):         return v, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "fields.py(1092):                 if error:\n",
      "fields.py(1095):                     return value, None\n",
      "fields.py(894):         if not errors and self.post_validators:\n",
      "fields.py(896):         return v, errors\n",
      "main.py(1075):         if isinstance(errors_, ErrorWrapper):\n",
      "main.py(1077):         elif isinstance(errors_, list):\n",
      "main.py(1080):             values[name] = v_\n",
      "main.py(1052):     for name, field in model.__fields__.items():\n",
      "main.py(1082):     if check_extra:\n",
      "main.py(1096):     for skip_on_failure, validator in model.__post_root_validators__:\n",
      "main.py(1104):     if errors:\n",
      "main.py(1107):         return values, fields_set, None\n",
      "main.py(340):         if validation_error:\n",
      "main.py(342):         try:\n",
      "main.py(343):             object_setattr(__pydantic_self__, '__dict__', values)\n",
      "main.py(348):         object_setattr(__pydantic_self__, '__fields_set__', fields_set)\n",
      "main.py(349):         __pydantic_self__._init_private_attributes()\n",
      " --- modulename: main, funcname: _init_private_attributes\n",
      "main.py(422):         for name, private_attr in self.__private_attributes__.items():\n",
      "component.py(348):             all_args = {**nmargs, **kwargs, **model.__dict__}\n",
      "component.py(353):             if not getattr(self, \"__init_args__\", {}):\n",
      "component.py(354):                 self.__init_args__ = OrderedDict(\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(355):                     {\n",
      " --- modulename: component, funcname: <dictcomp>\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(357):                         for name, arg in sorted(all_args.items())\n",
      "component.py(358):                         if not skip_encoding(arg)\n",
      " --- modulename: functools, funcname: wrapper\n",
      "functools.py(885):         if not args:\n",
      "functools.py(889):         return dispatch(args[0].__class__)(*args, **kw)\n",
      " --- modulename: functools, funcname: dispatch\n",
      "functools.py(826):         if cache_token is not None:\n",
      "functools.py(831):         try:\n",
      "functools.py(832):             impl = dispatch_cache[cls]\n",
      " --- modulename: weakref, funcname: __getitem__\n",
      "weakref.py(416):         return self.data[ref(key)]\n",
      "functools.py(839):         return impl\n",
      " --- modulename: component, funcname: skip_encoding\n",
      "component.py(212):     return False\n",
      "component.py(355):                     {\n",
      "component.py(356):                         name: arg\n",
      "component.py(355):                     {\n",
      "component.py(355):                     {\n",
      "component.py(354):                 self.__init_args__ = OrderedDict(\n",
      "component.py(361):                 self.__class__.__getnewargs_ex__ = validated_getnewargs_ex\n",
      "component.py(362):                 self.__class__.__repr__ = validated_repr\n",
      "component.py(364):             return init(self, **all_args)\n",
      " --- modulename: predictor, funcname: __init__\n",
      "predictor.py(55):         super().__init__(prediction_length, lead_time=lead_time)\n",
      " --- modulename: predictor, funcname: __init__\n",
      "predictor.py(54):             prediction_length > 0\n",
      "predictor.py(56):         assert lead_time >= 0, \"The value of `lead_time` should be >= 0\"\n",
      "predictor.py(58):         self.prediction_length = prediction_length\n",
      "predictor.py(59):         self.lead_time = lead_time\n",
      "predictor.py(56):         self.input_names = input_names\n",
      "predictor.py(57):         self.batch_size = batch_size\n",
      "predictor.py(58):         self.input_transform = input_transform\n",
      "predictor.py(59):         self.forecast_generator = forecast_generator\n",
      "predictor.py(60):         self.output_transform = output_transform\n",
      "predictor.py(61):         self.device = resolve_device(device)\n",
      " --- modulename: util, funcname: resolve_device\n",
      "util.py(29):     if device == \"auto\":\n",
      "util.py(30):         if torch.cuda.is_available():\n",
      " --- modulename: __init__, funcname: is_available\n",
      "__init__.py(117):     if not _is_compiled():\n",
      " --- modulename: __init__, funcname: _is_compiled\n",
      "__init__.py(108):     return hasattr(torch._C, \"_cuda_getDeviceCount\")\n",
      "__init__.py(119):     if _nvml_based_avail():\n",
      " --- modulename: __init__, funcname: _nvml_based_avail\n",
      "__init__.py(112):     return os.getenv(\"PYTORCH_NVML_BASED_CUDA_CHECK\") == \"1\"\n",
      " --- modulename: os, funcname: getenv\n",
      "os.py(776):     return environ.get(key, default)\n",
      " --- modulename: _collections_abc, funcname: get\n",
      "_collections_abc.py(823):         try:\n",
      "_collections_abc.py(824):             return self[key]\n",
      " --- modulename: os, funcname: __getitem__\n",
      "os.py(676):         try:\n",
      "os.py(677):             value = self._data[self.encodekey(key)]\n",
      " --- modulename: os, funcname: encode\n",
      "os.py(756):             if not isinstance(value, str):\n",
      "os.py(758):             return value.encode(encoding, 'surrogateescape')\n",
      "os.py(681):         return self.decodevalue(value)\n",
      " --- modulename: os, funcname: decode\n",
      "os.py(760):             return value.decode(encoding, 'surrogateescape')\n",
      "__init__.py(123):         return device_count() > 0\n",
      " --- modulename: __init__, funcname: device_count\n",
      "__init__.py(837):     if not _is_compiled():\n",
      " --- modulename: __init__, funcname: _is_compiled\n",
      "__init__.py(108):     return hasattr(torch._C, \"_cuda_getDeviceCount\")\n",
      "__init__.py(839):     if _cached_device_count is not None:\n",
      "__init__.py(840):         return _cached_device_count\n",
      "util.py(31):             return \"cuda\"\n",
      "predictor.py(62):         self.prediction_net = prediction_net.to(self.device)\n",
      " --- modulename: device_dtype_mixin, funcname: to\n",
      "device_dtype_mixin.py(53):         device, dtype = torch._C._nn._parse_to(*args, **kwargs)[:2]\n",
      "device_dtype_mixin.py(54):         _update_properties(self, device=device, dtype=dtype)\n",
      " --- modulename: device_dtype_mixin, funcname: _update_properties\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2426):             memo = set()\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(117):         if device is not None:\n",
      "device_dtype_mixin.py(118):             module._device = device\n",
      " --- modulename: module, funcname: __setattr__\n",
      "module.py(1732):         def remove_from(*dicts_or_sets):\n",
      "module.py(1740):         params = self.__dict__.get('_parameters')\n",
      "module.py(1741):         if isinstance(value, Parameter):\n",
      " --- modulename: parameter, funcname: __instancecheck__\n",
      "parameter.py(9):         return super().__instancecheck__(instance) or (\n",
      "parameter.py(10):             isinstance(instance, torch.Tensor) and getattr(instance, '_is_param', False))\n",
      "parameter.py(9):         return super().__instancecheck__(instance) or (\n",
      "module.py(1747):         elif params is not None and name in params:\n",
      "module.py(1754):             modules = self.__dict__.get('_modules')\n",
      "module.py(1755):             if isinstance(value, Module):\n",
      "module.py(1765):             elif modules is not None and name in modules:\n",
      "module.py(1776):                 buffers = self.__dict__.get('_buffers')\n",
      "module.py(1777):                 if buffers is not None and name in buffers:\n",
      "module.py(1788):                     super().__setattr__(name, value)\n",
      "device_dtype_mixin.py(119):         if dtype is not None:\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2432):                 if module is None:\n",
      "module.py(2434):                 submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "module.py(2435):                 yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2425):         if memo is None:\n",
      "module.py(2427):         if self not in memo:\n",
      "module.py(2428):             if remove_duplicate:\n",
      "module.py(2429):                 memo.add(self)\n",
      "module.py(2430):             yield prefix, self\n",
      "module.py(2393):             yield module\n",
      "device_dtype_mixin.py(113):         if not isinstance(module, _DeviceDtypeModuleMixin):\n",
      "device_dtype_mixin.py(114):             continue\n",
      "device_dtype_mixin.py(112):     for module in root.modules():\n",
      " --- modulename: module, funcname: modules\n",
      "module.py(2392):         for _, module in self.named_modules():\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      " --- modulename: module, funcname: named_modules\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "module.py(2431):             for name, module in self._modules.items():\n",
      "device_dtype_mixin.py(55):         return super().to(*args, **kwargs)\n",
      " --- modulename: module, funcname: to\n",
      "module.py(1138):         device, dtype, non_blocking, convert_to_format = torch._C._nn._parse_to(*args, **kwargs)\n",
      "module.py(1140):         if dtype is not None:\n",
      "module.py(1151):         def convert(t):\n",
      "module.py(1174):         return self._apply(convert)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(800):                 continue\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(2364):             if module is not None and module not in memo:\n",
      "module.py(2365):                 memo.add(module)\n",
      "module.py(2366):                 yield name, module\n",
      "module.py(2346):             yield module\n",
      "module.py(780):                 module._apply(fn)\n",
      " --- modulename: module, funcname: _apply\n",
      "module.py(778):         if recurse:\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2362):         memo = set()\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(799):             if param is None:\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: _contextlib, funcname: __new__\n",
      "_contextlib.py(155):         if orig_func is None:\n",
      "_contextlib.py(156):             return super().__new__(cls)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(76):         if not torch._jit_internal.is_scripting():\n",
      " --- modulename: _jit_internal, funcname: is_scripting\n",
      "_jit_internal.py(1149):     return False\n",
      "grad_mode.py(77):             super().__init__()\n",
      "grad_mode.py(78):         self.prev = False\n",
      " --- modulename: grad_mode, funcname: __enter__\n",
      "grad_mode.py(81):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(82):         torch.set_grad_enabled(False)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(805):                 param_applied = fn(param)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(804):             with torch.no_grad():\n",
      " --- modulename: grad_mode, funcname: __exit__\n",
      "grad_mode.py(85):         torch.set_grad_enabled(self.prev)\n",
      " --- modulename: grad_mode, funcname: __init__\n",
      "grad_mode.py(185):         self.prev = torch.is_grad_enabled()\n",
      "grad_mode.py(186):         self.mode = mode\n",
      "grad_mode.py(187):         torch._C._set_grad_enabled(mode)\n",
      "module.py(806):             p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      " --- modulename: module, funcname: compute_should_use_set_data\n",
      "module.py(783):             if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "module.py(792):                 return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_overwrite_module_params_on_conversion\n",
      "__future__.py(32):     return _overwrite_module_params_on_conversion\n",
      "module.py(809):             p_should_use_swap_tensors = should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n",
      " --- modulename: _python_dispatch, funcname: is_traceable_wrapper_subclass\n",
      "_python_dispatch.py(298):     is_subclass = isinstance(t, torch.Tensor) and type(t) != torch.Tensor\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(301):         and hasattr(t, \"__tensor_flatten__\")\n",
      "_python_dispatch.py(300):         is_subclass\n",
      "_python_dispatch.py(299):     return (\n",
      "module.py(811):             param_grad = param.grad\n",
      "module.py(812):             if p_should_use_swap_tensors:\n",
      "module.py(825):             elif p_should_use_set_data:\n",
      "module.py(826):                 param.data = param_applied\n",
      "module.py(827):                 out_param = param\n",
      "module.py(834):             if param_grad is not None:\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(853):             if buf is not None:\n",
      "module.py(854):                 self._buffers[key] = fn(buf)\n",
      " --- modulename: module, funcname: convert\n",
      "module.py(1152):             try:\n",
      "module.py(1153):                 if convert_to_format is not None and t.dim() in (4, 5):\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(1161):                     device,\n",
      "module.py(1162):                     dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "module.py(1163):                     non_blocking,\n",
      "module.py(1160):                 return t.to(\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "module.py(779):             for module in self.children():\n",
      " --- modulename: module, funcname: children\n",
      "module.py(2345):         for name, module in self.named_children():\n",
      " --- modulename: module, funcname: named_children\n",
      "module.py(2363):         for name, module in self._modules.items():\n",
      "module.py(782):         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "module.py(796):         should_use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
      " --- modulename: __future__, funcname: get_swap_module_params_on_conversion\n",
      "__future__.py(75):     return _swap_module_params_on_conversion\n",
      "module.py(798):         for key, param in self._parameters.items():\n",
      "module.py(852):         for key, buf in self._buffers.items():\n",
      "module.py(856):         return self\n",
      "predictor.py(63):         self.required_fields = [\"forecast_start\", \"item_id\", \"info\"]\n",
      "Ending trace...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m predictor \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcreate_predictor(batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m forecasts \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m(target)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "predictor = model.create_predictor(batch_size = 32)\n",
    "forecasts = predictor.predict(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "57cc8af9-85fc-4f78-8316-30e25943c4e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3415321b-f17d-45f7-8a1b-054c9039b939",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
