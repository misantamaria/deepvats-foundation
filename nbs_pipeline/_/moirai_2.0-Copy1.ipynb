{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8d7374a-16c4-409e-961d-02c667ac53f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "verbose                       = None\n",
    "check_memory_usage            = None\n",
    "time_flag                     = None\n",
    "window_size_percentage        = None\n",
    "show_plots                    = None\n",
    "reset_kernel                  = None\n",
    "pre_configured_case           = None\n",
    "case_id                       = None\n",
    "frequency_factor              = None\n",
    "frequency_factor_change_alias = None\n",
    "check_parameters              = True\n",
    "cuda_device                   = None\n",
    "remove_lambdas_flag           = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5ade51-58bf-4cd2-aff6-7ad3be23a4e4",
   "metadata": {},
   "source": [
    "# Explained pre-trained module trial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e9ee0f-7af4-4f38-b5de-bffad59c31c1",
   "metadata": {},
   "source": [
    "## Instalation commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13ae19aa-e2b9-4353-916a-0a8f3297e0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! git clone https://github.com/SalesforceAIResearch/uni2ts.git\n",
    "#! cd uni2ts\n",
    "#! pip install -e '.[notebook]' --no-warn-script-location\n",
    "#! conda install anaconda::gluonts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e226f16-49d0-454e-9add-8c4752b08623",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c45abc3-18ca-4751-8d44-09ab90e0c547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from gluonts.dataset.pandas import PandasDataset\n",
    "from gluonts.dataset.split import split\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "from uni2ts.eval_util.plot import plot_single\n",
    "from uni2ts.model.moirai import MoiraiForecast, MoiraiModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "526d39ba-6822-4e44-a3cb-349cbe2c2bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib.IpcWriteOptions size changed, may indicate binary incompatibility. Expected 72 from C header, got 88 from PyObject\n",
      "<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib.IpcReadOptions size changed, may indicate binary incompatibility. Expected 96 from C header, got 104 from PyObject\n",
      "<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib.Tensor size changed, may indicate binary incompatibility. Expected 64 from C header, got 80 from PyObject\n",
      "<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib.NativeFile size changed, may indicate binary incompatibility. Expected 96 from C header, got 104 from PyObject\n",
      "<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib.BufferedInputStream size changed, may indicate binary incompatibility. Expected 96 from C header, got 104 from PyObject\n",
      "<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib.BufferedOutputStream size changed, may indicate binary incompatibility. Expected 96 from C header, got 104 from PyObject\n",
      "<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib.CompressedInputStream size changed, may indicate binary incompatibility. Expected 96 from C header, got 104 from PyObject\n",
      "<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib.CompressedOutputStream size changed, may indicate binary incompatibility. Expected 96 from C header, got 104 from PyObject\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2004l\n",
      "Octave is ready <oct2py.core.Oct2Py object at 0x7fa74bbc0340>\n",
      "\u001b[?2004l\n",
      "\u001b[?2004l\n",
      "\u001b[?2004l\n",
      "\u001b[?2004l\n",
      "\u001b[?2004l\n",
      "\u001b[?2004l\n",
      "\u001b[?2004l\n",
      "\u001b[?2004l\n",
      "\u001b[?2004l\n",
      "\u001b[?2004l\n",
      "\u001b[?2004l\n",
      "\u001b[?2004l\n",
      "\u001b[?2004l\n",
      "\u001b[?2004l\n",
      "\u001b[?2004l\n"
     ]
    }
   ],
   "source": [
    "from dvats.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ee47ab-44b4-421f-97ec-0d52606a668f",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e784320-0d41-4087-acc9-887d9b58f34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# This is only needed if the notebook is run in VSCode\n",
    "import sys\n",
    "import dvats.utils as ut\n",
    "if '--vscode' in sys.argv:\n",
    "    print(\"Executing inside vscode\")\n",
    "    ut.DisplayHandle.update = ut.update_patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f1aeccd-4a3a-4707-b110-0af5215aa0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "def remove_lambdas(verbose = 0):\n",
    "    path = \"./uni2ts/src/uni2ts/distribution/mixture.py\"\n",
    "    if verbose > 0: print(f\"remove_lambdas | read file {path}\")\n",
    "    # Read the file\n",
    "    with open(path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    # Check wether identity is defined or not\n",
    "    identity_defined = any(\"def identity(\" in line for line in lines)\n",
    "    if verbose > 0: print(f\"remove_lambdas | identity already defined? {identity_defined}\")\n",
    "    if not identity_defined: \n",
    "        if verbose > 0: print(\"remove_lambdas | Look for domain_map line\")\n",
    "        for i, line in enumerate(lines):\n",
    "            if \"@property\" in line and \"def domain_map\" in lines[i+1]:                \n",
    "                domain_map_property_index = i\n",
    "                break\n",
    "        if verbose > 0: print(f\"remove_lambdas | Domain map in line {i}\")\n",
    "        # Insert identity function\n",
    "        identity_code = \"\"\"\n",
    "    def identity(self, x): \n",
    "        return x\n",
    "\n",
    "\"\"\"\n",
    "        lines.insert(domain_map_property_index, identity_code)\n",
    "        # Modify weights_logits line \n",
    "        inside_domain_map = False\n",
    "        for i in range(domain_map_property_index, len(lines)):\n",
    "            if \"weights_logits\" in lines[i]:\n",
    "                # Reemplazar la l√≠nea para que use identity\n",
    "                lines[i] = \"            weights_logits = self.identity,\\n\"\n",
    "                break\n",
    "        # Write changes \n",
    "        with open(path, 'w') as file:\n",
    "            file.writelines(lines)\n",
    "        importlib.reload(uni2ts)\n",
    "    else:\n",
    "        if identity_defined:\n",
    "            print(\"Identity already defined\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfc741c-dbc7-4b46-bf88-8f233637bdab",
   "metadata": {},
   "source": [
    "### Ensure model is pickable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c481684-65d6-4874-ae70-1df191cd0d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_lambdas | read file ./uni2ts/src/uni2ts/distribution/mixture.py\n",
      "remove_lambdas | identity already defined? True\n",
      "Identity already defined\n"
     ]
    }
   ],
   "source": [
    "if remove_lambdas_flag: remove_lambdas(verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1b2f45-e08d-4255-81ef-a4221e7bcb9e",
   "metadata": {},
   "source": [
    "## Load dataset artifact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf546f4-be65-4642-837c-a122ca2b17cc",
   "metadata": {},
   "source": [
    "### Check input parameters & set up default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93180bf5-e123-48d7-ae6b-84b1852b4f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "verbose                       = True  if verbose is None else verbose\n",
    "check_memory_usage            = True  if check_memory_usage is None else check_memory_usage\n",
    "time_flag                     = True  if time_flag is None else time_flag\n",
    "window_size_percentage        = False if window_size_percentage is None else window_size_percentage\n",
    "show_plots                    = True if show_plots is None else show_plots\n",
    "reset_kernel                  = False  if reset_kernel is None else reset_kernel\n",
    "pre_configured_case           = True if pre_configured_case is None else pre_configured_case\n",
    "case_id                       = 7 if case_id is None else case_id\n",
    "frequency_factor              = 1 if frequency_factor is None else frequency_factor\n",
    "frequency_factor_change_alias = True if frequency_factor_change_alias is None else frequency_factor_change_alias\n",
    "cuda_device                   = 1 if  cuda_device is None else cuda_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c3f41eb-ee0a-4a85-8fdb-20ea22d30416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Check parameters ---\n",
      "verbose: True check_memory_usage True time_flag: True window_size_percentage: False show_plots: True reset_kernel: False pre_configured_case: True case_id: 7 frequency_factor: 1 frequency_factor_change_alias True cuda_device 1\n"
     ]
    }
   ],
   "source": [
    "if check_parameters:\n",
    "    print(\"--- Check parameters ---\")\n",
    "    print(\n",
    "        \"verbose:\", verbose,\n",
    "        \"check_memory_usage\", check_memory_usage,\n",
    "        \"time_flag:\", time_flag,\n",
    "        \"window_size_percentage:\" , window_size_percentage,\n",
    "        \"show_plots:\",show_plots,\n",
    "        \"reset_kernel:\",reset_kernel,\n",
    "        \"pre_configured_case:\",pre_configured_case,\n",
    "        \"case_id:\",case_id,\n",
    "        \"frequency_factor:\", frequency_factor, \n",
    "        \"frequency_factor_change_alias\", frequency_factor_change_alias,\n",
    "        \"cuda_device\", cuda_device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f64138b8-e2c0-4ed3-ba9c-3b9a0e48675d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import dvats.config as cfg_\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", module=\"umap\")\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from dvats.all import *\n",
    "from fastcore.all import *\n",
    "from tsai.basics import *\n",
    "from tsai.models.InceptionTimePlus import *\n",
    "from tsai.callback.MVP import *\n",
    "import matplotlib.colors as colors\n",
    "from fastai.callback.wandb import WandbCallback\n",
    "from fastai.callback.progress import ShowGraphCallback\n",
    "from fastai.callback.schedule import *\n",
    "from fastai.callback.tracker import EarlyStoppingCallback\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from gluonts.dataset.pandas import PandasDataset\n",
    "from gluonts.dataset.split import split\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "import uni2ts\n",
    "from uni2ts.eval_util.plot import plot_single\n",
    "from uni2ts.model.moirai import MoiraiForecast, MoiraiModule\n",
    "from uni2ts.eval_util.plot import plot_next_multi\n",
    "import pyarrow.feather as ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60ad4604-0887-4345-b075-c54e7b5a69d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find /home/macu/work/nbs_pipeline/moirai_trial.ipynb.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU | Used mem: 0\n",
      "GPU | Used mem: 24\n",
      "GPU | Memory Usage: [\u001b[90m--------------------\u001b[0m] \u001b[90m0%\u001b[0m\n",
      "wandb_group: None\u001b[0m\n",
      "analysis_mode: online\u001b[0m\n",
      "epochs: 100\u001b[0m\n",
      "\u001b[93m\u001b[1mfreq is missing in original dict | 1s \u001b[0m\n",
      "\u001b[94mtrain_artifact: mi-santamaria/deepvats/PulsusParadoxus-SP02:latest\u001b[0m -> mi-santamaria/deepvats/toy:latest\u001b[0m\n",
      "r: 0.71\u001b[0m\n",
      "\u001b[93m\u001b[1martifact_name is missing in original dict | toy \u001b[0m\n",
      "\u001b[93m\u001b[1mtime_col is missing in original dict | None \u001b[0m\n",
      "valid_size: 0.2\u001b[0m\n",
      "\u001b[93m\u001b[1mcsv_config is missing in original dict | {} \u001b[0m\n",
      "\u001b[94mbatch_size: 512\u001b[0m -> 32\u001b[0m\n",
      "\u001b[94mstride: 900\u001b[0m -> 1\u001b[0m\n",
      "mask_future: False\u001b[0m\n",
      "valid_artifact: None\u001b[0m\n",
      "norm_by_sample: False\u001b[0m\n",
      "norm_use_single_batch: False\u001b[0m\n",
      "\u001b[94mmvp_ws: (15, 100)\u001b[0m -> [10, 30]\u001b[0m\n",
      "\u001b[93m\u001b[1mdata_cols is missing in original dict | [] \u001b[0m\n",
      "\u001b[93m\u001b[1mdata_fpath is missing in original dict | ~/data/toy.csv \u001b[0m\n",
      "use_wandb: True\u001b[0m\n",
      "\u001b[94mw: 100\u001b[0m -> 30\u001b[0m\n",
      "mask_stateful: True\u001b[0m\n",
      "mask_sync: False\u001b[0m\n",
      "\u001b[94malias: PulsusParadoxus-SP02\u001b[0m -> toy\u001b[0m\n",
      "\u001b[93m\u001b[1mnorm_use_by_single_batch is missing in original dict | (False,) \u001b[0m\n",
      "runname: moirai_trial\n",
      "alias: toy\n",
      "analysis_mode: online\n",
      "batch_size: 32\n",
      "epochs: 100\n",
      "mask_future: False\n",
      "mask_stateful: True\n",
      "mask_sync: False\n",
      "mvp_ws: [10, 30]\n",
      "norm_by_sample: False\n",
      "norm_use_single_batch: False\n",
      "r: 0.71\n",
      "stride: 1\n",
      "train_artifact: mi-santamaria/deepvats/toy:latest\n",
      "valid_artifact: None\n",
      "use_wandb: True\n",
      "valid_size: 0.2\n",
      "w: 30\n",
      "wandb_group: None\n",
      "artifact_name: toy\n",
      "data_cols: []\n",
      "data_fpath: ~/data/toy.csv\n",
      "freq: 1s\n",
      "time_col: None\n",
      "csv_config: {}\n",
      "norm_use_by_single_batch: (False,)\n",
      "--> Wandb init\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmi-santamaria\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/macu/work/wandb/run-20241001_101951-0crcrlaz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mi-santamaria/deepvats/runs/0crcrlaz' target=\"_blank\">moirai_trial</a></strong> to <a href='https://wandb.ai/mi-santamaria/deepvats' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mi-santamaria/deepvats' target=\"_blank\">https://wandb.ai/mi-santamaria/deepvats</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mi-santamaria/deepvats/runs/0crcrlaz' target=\"_blank\">https://wandb.ai/mi-santamaria/deepvats/runs/0crcrlaz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wandb init -->\n",
      "alias: toy\n",
      "analysis_mode: online\n",
      "batch_size: 32\n",
      "epochs: 100\n",
      "mask_future: False\n",
      "mask_stateful: True\n",
      "mask_sync: False\n",
      "mvp_ws: [10, 30]\n",
      "norm_by_sample: False\n",
      "norm_use_single_batch: False\n",
      "r: 0.71\n",
      "stride: 1\n",
      "train_artifact: mi-santamaria/deepvats/toy:latest\n",
      "valid_artifact: None\n",
      "use_wandb: True\n",
      "valid_size: 0.2\n",
      "w: 30\n",
      "wandb_group: None\n",
      "artifact_name: toy\n",
      "data_cols: []\n",
      "data_fpath: ~/data/toy.csv\n",
      "freq: 1s\n",
      "time_col: None\n",
      "csv_config: {}\n",
      "norm_use_by_single_batch: [False]\n",
      "---> W&B Train Artifact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(550, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| export\n",
    "wandb_api = wandb.Api()\n",
    "device = torch.device(f'cuda:{cuda_device}' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device)\n",
    "if check_memory_usage:\n",
    "    gpu_device = torch.cuda.current_device()\n",
    "    gpu_memory_status(gpu_device)\n",
    "user, project, version, data, config, job_type = cfg_.get_artifact_config_MVP(False)\n",
    "if pre_configured_case: \n",
    "    cfg_.force_artifact_config_mvp(\n",
    "        config = config,\n",
    "        id = case_id,\n",
    "        verbose = verbose, \n",
    "        both = verbose > 0,\n",
    "        frequency_factor = frequency_factor,\n",
    "        frequency_factor_change_alias = frequency_factor_change_alias\n",
    "    )\n",
    "path = os.path.expanduser(\"~/work/nbs_pipeline/\")\n",
    "name=\"moirai_trial\"\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = path+name+\".ipynb\"\n",
    "runname=name\n",
    "if verbose > 0: print(\"runname: \"+runname)\n",
    "if verbose > 0: cfg_.show_attrdict(config)\n",
    "\n",
    "if verbose > 0: print(\"--> Wandb init\")\n",
    "run = wandb.init(\n",
    "    entity = user,\n",
    "    # work-nbs is a place to log draft runs\n",
    "    project=project,\n",
    "    group=config.wandb_group,\n",
    "    job_type=job_type,\n",
    "    allow_val_change=True,\n",
    "    mode=config.analysis_mode,\n",
    "    config=config,\n",
    "    # When use_wandb is false the run is not linked to a personal account\n",
    "    #NOTE: This is not working right now\n",
    "    anonymous = 'never' if config.use_wandb else 'must', \n",
    "    resume=False,\n",
    "    name = runname\n",
    ")\n",
    "if verbose > 0: print(\"Wandb init -->\")\n",
    "config = run.config  # Object for storing hyperparameters\n",
    "artifacts_gettr = run.use_artifact if config.use_wandb else wandb_api.artifact\n",
    "config = run.config  # Object for storing hyperparameters\n",
    "if verbose > 0: cfg_.show_attrdict(config)\n",
    "artifacts_gettr = run.use_artifact if config.use_wandb else wandb_api.artifact\n",
    "train_artifact = artifacts_gettr(config.train_artifact)\n",
    "if verbose > 0: print(\"---> W&B Train Artifact\")\n",
    "df_train = train_artifact.to_df()\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a74030b0-7661-42dd-868c-ad57cd98bed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T3</th>\n",
       "      <th>T2</th>\n",
       "      <th>T1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:00:00</th>\n",
       "      <td>0.741822</td>\n",
       "      <td>0.637180</td>\n",
       "      <td>0.565117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:00:01</th>\n",
       "      <td>0.739731</td>\n",
       "      <td>0.629415</td>\n",
       "      <td>0.493513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:00:02</th>\n",
       "      <td>0.718757</td>\n",
       "      <td>0.539220</td>\n",
       "      <td>0.469350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:00:03</th>\n",
       "      <td>0.730169</td>\n",
       "      <td>0.577670</td>\n",
       "      <td>0.444100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:00:04</th>\n",
       "      <td>0.752406</td>\n",
       "      <td>0.570180</td>\n",
       "      <td>0.373008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           T3        T2        T1\n",
       "1970-01-01 00:00:00  0.741822  0.637180  0.565117\n",
       "1970-01-01 00:00:01  0.739731  0.629415  0.493513\n",
       "1970-01-01 00:00:02  0.718757  0.539220  0.469350\n",
       "1970-01-01 00:00:03  0.730169  0.577670  0.444100\n",
       "1970-01-01 00:00:04  0.752406  0.570180  0.373008"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0f8b29-38a6-4658-81c8-b9ac4d8322a2",
   "metadata": {},
   "source": [
    "### Sliding windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94aadad1-8c27-4a6a-b398-6c070db00a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Sliding window |  30  |  1\n",
      " Sliding window |  30  |  1 ---> | df_train ~  (550, 3)\n",
      " sw_df_train |  30  |  1 --->\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "if verbose > 0: print(\"---> Sliding window | \", config.w,  \" | \", config.stride )\n",
    "sw = SlidingWindow(window_len=config.w, stride=config.stride, get_y=[])\n",
    "if verbose > 0: print(\" Sliding window | \", config.w,  \" | \", config.stride, \"---> | df_train ~ \", df_train.shape )\n",
    "X_train, _ = sw(df_train)\n",
    "if verbose > 0: print(\" sw_df_train | \", config.w,  \" | \", config.stride, \"--->\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e0bb9a4-a0f0-4896-9d1b-3b2fb04323bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(521, 3, 30)\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "if verbose > 0: \n",
    "    print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff7d914c-f9a8-4dc5-9707-f7b309615ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(X):  521\n",
      "--> Split 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x50 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split --> 417\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "assert config.analysis_mode in ['offline','online'], 'Invalid analysis mode'\n",
    "\n",
    "X = X_train\n",
    "if verbose > 0: print(\"len(X): \", len(X));\n",
    "if config.analysis_mode == 'online':\n",
    "    if verbose > 0: print(\"--> Split 1\")\n",
    "    splits = TimeSplitter(valid_size=0.2, show_plot=show_plots)(X)\n",
    "elif config.analysis_mode == 'offline':\n",
    "    if verbose > 0: print(\"--> Split 2\")\n",
    "    splits = get_splits(np.arange(len(X)), valid_size=config.valid_size, show_plot = show_plots)\n",
    "if verbose > 0: \n",
    "    print(\"Split -->\", len(splits[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "072b6522-bcaa-46ae-bf32-31f31dae4bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(521, 3, 30)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((#417) [0,1,2,3,4,5,6,7,8,9...],\n",
       " (#104) [417,418,419,420,421,422,423,424,425,426...])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| hide\n",
    "if verbose > 0: \n",
    "    print(X.shape)\n",
    "    display(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1183ff9a-fb49-4381-9125-dc08b36aa7ab",
   "metadata": {},
   "source": [
    "## Load & get embeddings from moirai module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1b6b2be-a055-4618-ab99-7d171b5834ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_variate_id(\n",
    "    batch_size, \n",
    "    seq_len, \n",
    "    num_variates\n",
    "):\n",
    "    # Crear un tensor de variate_id con tama√±o (batch_size, seq_len, num_variates)\n",
    "    variate_id = torch.arange(num_variates).unsqueeze(0).unsqueeze(0).repeat(batch_size, seq_len, 1)\n",
    "    return variate_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1176e835-f851-4e03-af55-73bdb2b9d211",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, num_variates, seq_len = X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39243d9a-d4e0-4d59-9423-d3ddf2eb46bc",
   "metadata": {},
   "source": [
    "Aqu√≠ tendremos que seleccionar cualquiera de los modelos preentrenados en Moirai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1231eef3-f10d-41ee-9c60-439bed9a92c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = 'large'\n",
    "#model_size = 'small'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bcbdbc-8459-43c6-8437-b0866e20f546",
   "metadata": {},
   "source": [
    "Nuestro X tiene tama√±o \n",
    "num_ventanas x num_variables x tama√±o_ventana\n",
    "\n",
    "El target que recibe Moirai tiene tama√±o \n",
    "num_ventanas x tama√±o_ventana x num_variables\n",
    "\n",
    "Permutamos para ajustar la entrada correctamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "042b0780-5775-4c84-9ee6-003c4b32f905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([521, 30, 3])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create target ~ ( batch , seq_length , max_patch )\n",
    "target = torch.randn(batch_size, seq_len, num_variates)\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952c780d-4e43-42ca-a09a-8507d641a145",
   "metadata": {},
   "source": [
    "Como todos los valores son observados, hacemos que el observed mask est√© a 1\n",
    "Como el tama√±o es el mismo que el del target, usamos torch.ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d2c7f90-6d22-4168-8ede-f6f7fc2aec07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(521, 3, 30)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97aec840-b37d-4a72-912d-45495376ea87",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_windows, num_variates, window_len = X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da37749e-bbbc-4b63-a6d6-155d2e3b03a0",
   "metadata": {},
   "source": [
    "---> Me hubiera gustado coger patch_size 8  para que tuviera sentido para toy que la ventana tiene tama√±o 30, pero parece ser que el modelo est√° pre-entrenado con tama√±o 128 y eso nos da problemas. Igual podemos paddear a 0 tambi√©n de manera individual cada token (a modo de √±apa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c95eb6a-a38c-4b77-8725-6405e3b25dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = num_windows\n",
    "seq_len = window_len * num_variates\n",
    "#patch_size = 8 \n",
    "patch_size = 128\n",
    "max_patch = patch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "964d2a92-6524-4749-9093-51743570675f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_pad(X, patch_size):\n",
    "    # X tiene forma (bs, n variables, longitud)\n",
    "    bs, n_vars, length = X.shape\n",
    "    \n",
    "    # Calculamos el n√∫mero de patches\n",
    "    n_patches = int(np.ceil(length / patch_size))\n",
    "    \n",
    "    # Creamos un array con ceros que tenga el tama√±o del nuevo array dividido en patches\n",
    "    padded_length = n_patches * patch_size\n",
    "    X_padded = np.zeros((bs, n_vars, padded_length))\n",
    "    \n",
    "    # Rellenamos X_padded con los valores originales de X\n",
    "    X_padded[:, :, :length] = X\n",
    "    \n",
    "    # Ahora dividimos X_padded en bloques de tama√±o patch_size\n",
    "    X_patches = X_padded.reshape(bs, n_vars, n_patches, patch_size)\n",
    "    \n",
    "    X_flat = X_patches.transpose(0, 2, 1, 3).reshape(bs, n_patches * n_vars, patch_size)\n",
    "\n",
    "    # Transformamos a un tensor de PyTorch\n",
    "    X_patches_torch = torch.tensor(X_flat)\n",
    "    \n",
    "    return X_patches_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54f7305c-afb0-43c5-9af3-4c4f23691131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_patches = np.ceil(window_len/patch_size)\n",
    "n_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3554c072-7cdd-41c5-9742-d1fbe1cdc010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ceil(window_len/patch_size)*num_variates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dfaad26c-8a6d-4401-8e00-87625f2a5afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = split_and_pad(X, patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "899e2d6d-f57b-4138-96ee-894d78380bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, seq_len, max_patch = target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3388d823-5f01-4b54-b8ef-51f083435295",
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_mask = torch.zeros_like(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b12a2fb2-86b3-4838-9f26-70972585088b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_id(X_patches_torch):\n",
    "    batch_size, seq_len, _ = X_patches_torch.shape\n",
    "    # Generamos un tensor que comienza en 0 y va incrementando de 1 en 1 hasta seq_len * batch_size - 1\n",
    "    return torch.arange(batch_size * seq_len).view(batch_size, seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3aeb7506-c95f-49cc-b4a7-ab8219ffff30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([521, 3])\n",
      "tensor([[   0,    1,    2],\n",
      "        [   3,    4,    5],\n",
      "        [   6,    7,    8],\n",
      "        ...,\n",
      "        [1554, 1555, 1556],\n",
      "        [1557, 1558, 1559],\n",
      "        [1560, 1561, 1562]])\n"
     ]
    }
   ],
   "source": [
    "sample_id = get_sample_id(target)\n",
    "print(sample_id.shape)\n",
    "print(sample_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57320b93-ae0e-4e1e-8e5d-d76a69dbe886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variate_id(batch_size, seq_len, n_vars, patches_per_var):\n",
    "    patches_per_var = int(patches_per_var)\n",
    "    # Creamos un tensor que repite cada ID de variable patches_per_var veces\n",
    "    var_ids = torch.arange(n_vars).repeat_interleave(patches_per_var)\n",
    "    # Repetimos este patr√≥n para cada muestra en el batch\n",
    "    return var_ids.unsqueeze(0).repeat(batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "468aca3d-dfb9-4e64-a461-395c81554006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 521, \n",
      " seq_len 3, \n",
      " n_vars 3, \n",
      "  patches_per_var 1.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"batch_size {batch_size}, \\n seq_len {seq_len}, \\n n_vars {num_variates}, \\n  patches_per_var {n_patches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b0cce1a3-06a2-4a99-91b8-e2bdf881a956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([521, 3])\n",
      "tensor([[0, 1, 2],\n",
      "        [0, 1, 2],\n",
      "        [0, 1, 2],\n",
      "        ...,\n",
      "        [0, 1, 2],\n",
      "        [0, 1, 2],\n",
      "        [0, 1, 2]])\n"
     ]
    }
   ],
   "source": [
    "variate_id = get_variate_id(\n",
    "    batch_size = batch_size, \n",
    "    seq_len = seq_len, \n",
    "    n_vars = num_variates, \n",
    "    patches_per_var = n_patches\n",
    ")\n",
    "print(variate_id.shape)\n",
    "print(variate_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3fa6ddfa-119f-4821-9f0d-061a3af91040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_id(batch_size, seq_len, patches_per_var):\n",
    "    patches_per_var = int (patches_per_var)\n",
    "    # Creamos un tensor que repite una secuencia de 0 a patches_per_var-1\n",
    "    time_ids = torch.arange(patches_per_var).repeat(seq_len // patches_per_var + 1)[:seq_len]\n",
    "    # Repetimos este patr√≥n para cada muestra en el batch\n",
    "    return time_ids.unsqueeze(0).repeat(batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "35424515-3594-49d9-b564-906cb9e8671a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([521, 3])\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "time_id = get_time_id(batch_size, seq_len, n_patches)\n",
    "print(time_id.shape)\n",
    "print(time_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a4a8da4f-ed4c-4249-98cd-6d028ee5ddc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19cd75b89a3f494d9b28016ee3a6b2b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f02dcf9003044bf1ab737245878c2c88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get pre-trained module\n",
    "module = MoiraiModule.from_pretrained(f\"Salesforce/moirai-1.0-R-{model_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1651022f-cf52-4926-9de6-05487dd58181",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_mask = torch.zeros((batch_size, seq_len), dtype=torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6691097d-206e-4aa2-883e-08604b297c25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([521, 128])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_size_tensor = torch.zeros((batch_size, patch_size))+patch_size\n",
    "patch_size_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8c48d2b8-d5e3-46ce-b6f4-901e95fd5462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       " \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjaxtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'*batch seq_len max_patch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mobserved_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjaxtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBool\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'*batch seq_len max_patch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msample_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjaxtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'*batch seq_len'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtime_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjaxtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'*batch seq_len'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mvariate_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjaxtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'*batch seq_len'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mprediction_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjaxtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBool\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'*batch seq_len'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjaxtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'*batch seq_len'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Defines the forward pass of MoiraiModule.\n",
       "This method expects processed inputs.\n",
       "\n",
       "1. Apply scaling to observations\n",
       "2. Project from observations to representations\n",
       "3. Replace prediction window with learnable mask\n",
       "4. Apply transformer layers\n",
       "5. Project from representations to distribution parameters\n",
       "6. Return distribution object\n",
       "\n",
       ":param target: input data\n",
       ":param observed_mask: binary mask for missing values, 1 if observed, 0 otherwise\n",
       ":param sample_id: indices indicating the sample index (for packing)\n",
       ":param time_id: indices indicating the time index\n",
       ":param variate_id: indices indicating the variate index\n",
       ":param prediction_mask: binary mask for prediction horizon, 1 if part of the horizon, 0 otherwise\n",
       ":param patch_size: patch size for each token\n",
       ":return: predictive distribution\n",
       "\u001b[0;31mFile:\u001b[0m      ~/work/nbs_pipeline/uni2ts/src/uni2ts/model/moirai/module.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "? module.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "205f1297-8c80-427a-b664-d4a2a6ec5a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target ~ torch.Size([521, 3, 128])\n",
      "observed_mask ~ torch.Size([521, 3, 128])\n",
      "sample_id ~ torch.Size([521, 3])\n",
      "time_id ~ torch.Size([521, 3])\n",
      "variate_id ~ torch.Size([521, 3])\n",
      "prediction_mask ~ torch.Size([521, 3])\n",
      "patch_size_tensor ~ torch.Size([521, 128])\n"
     ]
    }
   ],
   "source": [
    "print(f\"target ~ {target.shape}\")\n",
    "print(f\"observed_mask ~ {observed_mask.shape}\")\n",
    "print(f\"sample_id ~ {sample_id.shape}\")\n",
    "print(f\"time_id ~ {time_id.shape}\")\n",
    "print(f\"variate_id ~ {variate_id.shape}\")\n",
    "print(f\"prediction_mask ~ {prediction_mask.shape}\")\n",
    "print(f\"patch_size_tensor ~ {patch_size_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df31f1d-9d9b-428e-bb5b-68e68662c0dc",
   "metadata": {},
   "source": [
    "---> Aqu√≠ cuidado porque he tenido que poner parche en \n",
    "\n",
    "\n",
    "File ~/work/nbs_pipeline/uni2ts/src/uni2ts/module/ts_embed.py:100, in MultiInSizeLinear.forward(self, x, in_feat_size)\n",
    "     \n",
    "     96     weight = self.weight[idx] * self.mask[idx]\n",
    "     \n",
    "     97     bias = self.bias[idx] if self.bias is not None else 0\n",
    "     \n",
    "     98     out = out + (\n",
    "     99         torch.eq(in_feat_size, feat_size).unsqueeze(-1)\n",
    "     \n",
    "--> 100         * (einsum(weight, x, \"out inp, ... inp -> ... out\") + bias)\n",
    "    \n",
    "    101     )\n",
    "    \n",
    "    102 return out\n",
    "\n",
    "para convertir todo a double\n",
    "\n",
    "Modificado a\n",
    "\n",
    "def forward(\n",
    "        \n",
    "        self,\n",
    "        \n",
    "        x: Float[torch.Tensor, \"*batch max_feat\"],\n",
    "    \n",
    "        in_feat_size: Int[torch.Tensor, \"*batch\"],\n",
    "    \n",
    "    ) -> Float[torch.Tensor, \"*batch out_feat\"]:\n",
    "        \n",
    "        out = 0\n",
    "\n",
    "        for idx, feat_size in enumerate(self.in_features_ls):\n",
    "            \n",
    "            weight = self.weight[idx].float() * self.mask[idx].float()  # Convertimos a Float\n",
    "            \n",
    "            bias = self.bias[idx].float() if self.bias is not None else 0  # Convertimos a Float\n",
    "            \n",
    "            x = x.float()  # Aseg√∫rate de que x tambi√©n est√© en Float\n",
    "            \n",
    "            out = out + (\n",
    "                \n",
    "                torch.eq(in_feat_size, feat_size).unsqueeze(-1)\n",
    "                \n",
    "                * (torch.einsum(\"out inp, ... inp -> ... out\", weight, x) + bias)\n",
    "        \n",
    "            )\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a3f850f8-94b7-40e3-b92a-378e1ce4a31e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([521, 3, 128])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d5dfebc1-9178-47a7-95d3-7418bc89d3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object ` module.save_hyperparameters` not found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Error while calling W&B API: context deadline exceeded (<Response [500]>)\n",
      "wandb: ERROR Error while calling W&B API: context deadline exceeded (<Response [500]>)\n"
     ]
    }
   ],
   "source": [
    "? module.save_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a84fdf73-757d-4244-af96-e2387e219ea9",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (128) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mobserved_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msample_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtime_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariate_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvariate_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprediction_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpatch_size_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdouble\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/share/miniconda3/envs/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/share/miniconda3/envs/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/work/nbs_pipeline/uni2ts/src/uni2ts/model/moirai/module.py:168\u001b[0m, in \u001b[0;36mMoiraiModule.forward\u001b[0;34m(self, target, observed_mask, sample_id, time_id, variate_id, prediction_mask, patch_size)\u001b[0m\n\u001b[1;32m    161\u001b[0m loc, scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler(\n\u001b[1;32m    162\u001b[0m     target,\n\u001b[1;32m    163\u001b[0m     observed_mask \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m~\u001b[39mprediction_mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    164\u001b[0m     sample_id,\n\u001b[1;32m    165\u001b[0m     variate_id,\n\u001b[1;32m    166\u001b[0m )\n\u001b[1;32m    167\u001b[0m scaled_target \u001b[38;5;241m=\u001b[39m (target \u001b[38;5;241m-\u001b[39m loc) \u001b[38;5;241m/\u001b[39m scale\n\u001b[0;32m--> 168\u001b[0m reprs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscaled_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m masked_reprs \u001b[38;5;241m=\u001b[39m mask_fill(reprs, prediction_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_encoding\u001b[38;5;241m.\u001b[39mweight)\n\u001b[1;32m    170\u001b[0m reprs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m    171\u001b[0m     masked_reprs,\n\u001b[1;32m    172\u001b[0m     packed_attention_mask(sample_id),\n\u001b[1;32m    173\u001b[0m     time_id\u001b[38;5;241m=\u001b[39mtime_id,\n\u001b[1;32m    174\u001b[0m     var_id\u001b[38;5;241m=\u001b[39mvariate_id,\n\u001b[1;32m    175\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/share/miniconda3/envs/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/share/miniconda3/envs/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/work/nbs_pipeline/uni2ts/src/uni2ts/module/ts_embed.py:99\u001b[0m, in \u001b[0;36mMultiInSizeLinear.forward\u001b[0;34m(self, x, in_feat_size)\u001b[0m\n\u001b[1;32m     96\u001b[0m     weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight[idx] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask[idx]\n\u001b[1;32m     97\u001b[0m     bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias[idx] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     98\u001b[0m     out \u001b[38;5;241m=\u001b[39m out \u001b[38;5;241m+\u001b[39m (\n\u001b[0;32m---> 99\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meq\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_feat_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeat_size\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mout inp, ... inp -> ... out\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m     )\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (128) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "res = module(\n",
    "    target = target.float(),\n",
    "    observed_mask = observed_mask.float(),\n",
    "    sample_id = sample_id,\n",
    "    time_id = time_id,\n",
    "    variate_id = variate_id,\n",
    "    prediction_mask = prediction_mask,\n",
    "    patch_size = patch_size_tensor.double()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911e92f3-628e-45c1-9fcc-241311a4384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(patch_size_tensor.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
