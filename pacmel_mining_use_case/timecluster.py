# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/01_Timecluster_replication.ipynb (unless otherwise specified).

__all__ = ['createDCAE', 'slices2array', 'get_latent_features', 'train_surrogate_model']

# Cell
from fastcore import test
import pandas as pd
import seaborn as sns
import numpy as np
from tensorflow.keras.models import Sequential, Model, load_model
from tensorflow.keras.layers import Dense, Flatten, Conv1D, MaxPool1D, Reshape, UpSampling1D, InputLayer
import umap

# Cell
def createDCAE(w, d, delta, n_filters=[64,32,12], filter_sizes=[10,5,5], pool_sizes=[2,2,3], output_filter_size=10):
    "Create a Deep Convolutional Autoencoder for multivariate time series of `d` dimensions, \
    sliced with a window size of `w`. The parameter `delta` sets the number of latent features that will be \
    contained in the Dense layer of the network. The the number of features \
    maps (filters), the filter size and the pool size can also be adjusted."
    # Test that the parameterization of the model is correct
    # 1. n_filters, filter_sizes and pool_sizes have the same length
    assert test.all_equal([len(x) for x in [n_filters, filter_sizes, pool_sizes]], np.repeat(len(n_filters), 3))
    # 2. Test that the number of filters in the last convLayer is equal to the product of the pool sizes
    assert np.prod(pool_sizes) == n_filters[-1]
    # 3. Test that the product of pool sizes is a divisor of the window size
    assert w % np.prod(pool_sizes).all() == 0
    # Create the model
    model = Sequential()
    model.add(InputLayer(input_shape=(w,d)))
    for (i, x) in enumerate(n_filters):
        model.add(Conv1D(filters=n_filters[i], kernel_size=filter_sizes[i], activation='relu', padding='same'))
        model.add(MaxPool1D(pool_size=pool_sizes[i]))
    aux_shape = model.output_shape[1:]
    model.add(Flatten())
    model.add(Dense(units=np.prod(aux_shape), activation='linear', name='latent_features'))
    model.add(Reshape(target_shape=aux_shape))
    for i, x in reversed(list(enumerate(n_filters))):
        model.add(Conv1D(filters=n_filters[i], kernel_size=filter_sizes[i], activation='relu', padding='same'))
        model.add(UpSampling1D(size=pool_sizes[i]))
    model.add(Conv1D(filters=d, kernel_size=output_filter_size, activation='linear', padding='same'))
    return model

# Cell
def slices2array(slices):
    "`slices` is a list of dataframes, each of them containing an slice of a multivariate time series."
    return np.rollaxis(np.dstack([x.values for x in slices]), -1)

# Cell
def get_latent_features(dcae, input_data, bottleneck_ln='latent_features'):
    "Get the activations of the bottleneck layer within the fitted autoencoder `dcae` (a Keras model) \
    for the input data `input_data` (a tensor). The name of the bottleneck layer is given in `bottleneck_ln"
    layer_latent_output = dcae.get_layer(bottleneck_ln).output
    intermediate_model = Model(inputs=dcae.input, outputs=layer_latent_output)
    intermediate_prediction = intermediate_model.predict(input_data)
    return intermediate_prediction

# Cell
def train_surrogate_model(dcae, embeddings, lat_ln='latent_features'):
    "Train a surrogate model that learns the `embeddings` from the latent features contained in the layer \
    `lat_ln` of a previously trained Deep Convolutional AutoEncoder `dcae`"
    x = dcae.get_layer(lat_ln).output
    x = Dense(units=embeddings.shape[1], activation='linear')(x)
    surrogate_model = Model(dcae.input, x)
    l_nms = [layer.name for layer in surrogate_model.layers]
    layer_idx = l_nms.index(lat_ln)
    # The layers that are already trained from the autoencoder must be `frozen`
    for layer in surrogate_model.layers[:layer_idx]:
        layer.trainable = False
    return surrogate_model